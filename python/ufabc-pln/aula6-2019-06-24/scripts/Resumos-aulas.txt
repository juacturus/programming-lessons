

AI-Complete é um termo associado a problemas (ou sub-problemas) comunamentee
atribuidos a Inteligencia artificial que indica que a solução para aquele
problema é muito dificil. O nome foi cunhado por Fanya Montalvo e é uma
analogia a definição de problemas no estudos de algoritmos NP (NP-Hard, NP-
Complete, NP-Easy). Exemplos de problemas considerados AI-Complete são Visão
computacional, Processamento de linguagem natural, quebra de AntiCaptcha e
problema de Bongard.
A maioria dos problemas relacinados a esse termo são algoritmos que atuem
simulando ou melhorando ações humanas, como o ato de entender um texto, uma
imagem ou um video. Em um artigo de Dafna Shahaf e Eyal Amir definem AI-
Complete como "Problemas faceis para humanos resolverem porem muito dificil
para maquinas resolverem" em tradução livre.
Alguns artigos defendem uma forma de usar as definições acima comparando os
problemas a um Oraculo Humano, que seria algo semelhante a uma maquina de
turing que pode computar qualquer coisa que um humano médio possa resolver.
Segundo Shahaf et all. um problema C pode ser considerado AI-Complete se:
1.  C é um problema IA (um Oraculo Humano pode resolver)
2. Qualquer problema IA pode ser convertido para C através de algum algoritmo
polinomial
Nesse mesmo artigo é definido AI-Hard e AI-easy como:
Para AI-Hard:
1. Um problema H é AI-Hard se e somente se existe um problema IA-Complete C que
possa ser reduzido em tempo polinomial para H.
para AI-Easy:
1. São problemas que podem ser resolvidos de forma polinomail de forma
deterministca por uma maquina de turing para algum problema de IA.


O termo "Processamento de linguagem natural" (PLN) refere-se ao estudo da
linguagem humana utilizando mecanismos computacionais que permitam seu
processamento com algum nível de automatização. Esse estudo pode ser feito em
duas frentes: pode-se tentar compreender a linguagem humana através de
ferramentas de tecnologia de informação, para fins de pesquisa, ou elaborar
sistemas que interpretem os significados do discurso a fim de reduzir as
diferenças entre pessoas e sistemas. A ideia principal é permitir que sistemas
compreendam a linguagem humana e tenham a capacidade de produzi-la da mesma
maneira como pessoas o fazem.
Para isso, utiliza ferramentas de linguística, para possibilitar que se
compreenda a relação entre os componentes de um texto, e de inteligência
artificial, pois a construção dos modelos envolve, em geral, problemas de busca
e aprendizado de máquina.
O processamento de linguagens pode parecer uma área de estudo nova, mas
sistemas mais rudimentares, que estudam a frequência de palavras e efetuam
traduções simples, por exemplo, datam do início do estudo das ciências da
computação. Mesmo que não fosse possível executá-los por limitações
tecnológicas, ideias de máquinas inteligentes existem desde que sistemas de
informação foram criados  a ideia do teste de Turing nos permite ver isso: um
software que seja indistinguível de uma pessoa esteve na mente de
desenvolvedores desde o início. Os sistemas mais recentes permitem a
compreensão e a elaboração de linguagem natural a um nível no qual é possível
identificar ordens e requisições num texto complexo e interagir com o usuário
de maneira confortável  como fazem os assistentes pessoais e sistemas busca,
por exemplo. 
Linguagens são aprendidas intuitivamente, possuem nuances e imprecisões e seus
significados dependem muito do contexto, além de possuírem alfabeto e
construções gramaticais virtualmente infinitos. Essas características fazem com
que seu processamento seja considerado um problema AI-completo, ou seja,
resolvê-lo implicaria resolver todos os outros problemas de Inteligência
artificial, que se baseiam em encontrar um software que tenha a capacidade de
tomar decisões de maneira assertiva sem supervisão. Para conseguir um sistema
de PLN perfeito, precisaríamos que ele pensasse da mesma maneira como uma
pessoa pensa, o que significaria que qualquer problema que exigisse isso
estaria resolvido. 
Considerando isso, no curso de Processamento de Linguagem Natural estudaremos a
análise de texto e de fala, sua análise estrutural e semântica que permita a
sua interpretação. Efetuaremos a categorização, busca, resumo e tradução de
textos, e o conhecimento adquirido será avaliado através de resumos sobre cada
aula ministrada, de uma prova que avaliará todo o conteúdo o curso e de um
projeto final que será entregue em etapas ao longo do período letivo.  


Avaliação:
A avaliação será composta por resumos(40%), uma prova teórica única(40%) e um
projeto(20%) que possui relatórios e uma apresentação. Além disso, haverá uma
prova substitutiva(17/05) e uma prova de recuperação(Q2/2018), caso necessário.
Vale mencionar que para aprovar na disciplina é necessário aprovar no projeto.
As notas estão compostas da seguinte forma de A para F respectivamente:
9>=7,5>6>=5>=0. Os resumos serão feitos por aula, de 250 a 500 palavras,
enviados pelo Tidia e terão prazo de 48h após o término da aula. A prova
abordará os conceitos vistos em aula e o projeto será dividido em 3 relatórios
(60% do projeto) e uma apresentação de 10 minutos(40% do projeto).
Introdução:
Processamento de linguagem natural(PLN) envolve atividades que utilizem a
linguagem humana,como fala ou escrita e pode ser utilizada para realizar
tarefas que se aproximem do entendimento humano. Como exemplos temos, sistemas
de respostas, busca semântica, processador de linguagem e criar resumos.
A PLN pode ser considerada uma subárea de inteligência artificial e
linguística, estando relacionada com compiladores, aprendizado de máquina,
modelos probabilísticos, prova de teoremas e interação humano-máquina. Pode ser
tanto abordada como pesquisa, tentando entender como a linguagem humana
funciona, tanto falada quanto escrita, ou pelo desenvolvimento de sistemas que
tentam analisar/gerar linguagem ou facilitar a interação humano-máquina,
reconhecendo contexto, analisando morfologia, sintaxe e semântica.
Porém, a PLN apresenta um desafio, pois possui um amplo domínio, seu
vocabulário é compelxo, uma expressão possui diversos significados e a
construção gramatical possui diversas formas, o que destoa das linguagem
utilizadas em processamento da informação, que são mais simples, diretas e
precisas.  
IA-Complete:
Os problemas taxados de IA-completos são considerados difíceis
computacionalmente e não podem ser resolvidos com um algoritmo específico, como
exemplos: processamento de linguagem natural, visão computacional e navegação
ou planejamento. O termo é utilizado como uma analogia do NP-completo de teoria
da complexidade. Também pode ser entendido como problemas que necessitam da
criação de uma inteligência artificial genérica, com um desempenho similar ao
humano em diversos domínios.


Na disciplina teremos avaliações diversas, e.g., relatórios, resumos, provas e
apresentações. Discutiremos uma sequências de mecanismos artificiais para
extração e tomada de decisão baseando-se em linguagem falada/escrita por
humanos. Essa área encontra-se categorizada como AI-Hard. AI-Hard (conhecido
também como AI-Complete) representa o conjunto de problemas definidos como:
podem ser solucionáveis por humanos; podem ser convertidos para algoritmos com
complexidade de tempo polinomial.


A disciplina Processamento de Linguagem Natural (PLN), ministrada pelo
professor Jesús, possui como ementa e objetivo a introdução aos conceitos de
PLN, apresentar conceitos de processamento sintático, estudar sobre técnicas de
análise e gramáticas, aprender a como realizar interpretação semântica e, por
fim, apresentar conceitos sobre processamento de discurso e suas aplicações. Há
diversas biografias disponíveis, como Fundations of statical natural language
processing e Speech & language processing, além de artigos como Jumping NLP
Curves: A Review of Natural Language Processing Research, que traz um resumo
sobre as principais pesquisas feitas na área de PLN.
As formas de avaliação da disciplina estão divididas em três partes: resumos
por aula, que consistem 40% da nota total; Prova teórica única, no dia 03/05,
que consiste 40% da nota total; e, por fim, projeto, que inclui relatórios e
apresentação, constituindo o restante da nota total, isto é, 20%. Há tanto
prova substitutiva, no dia 17/05, quanto prova de recuperação, com data ainda
não definida, mas que será no segundo quadrimestre de 2018. Para ser aprovado
na disciplina, é necessário a aprovação no projeto. Os intervalos dos conceitos
se definem da seguinte forma: A caso a nota final seja maior ou igual a 9, B se
a nota final for entre 7,5 e menor que 9, C se a nota final for entre 6 e menor
que 7,5, D se a nota final for entre 5 e menor que 6, e F caso a nota final
seja menor do que 5.
Aprofundando as formas de avaliação, os resumos por aula devem ser apenas
textos, entre 250 a 500 palavras, sobre o que fora dado no dia da aula em
questão, para ser submetido no Tidia em até 48h após cada aula, sendo que os
resumos serão publicados na página da disciplina para tanto que alunos possam
ver suas evoluções ao longo da disciplina quanto uma forma de ranking, para que
os alunos possam ver como estão seus desenvolvimentos com relação à outros
alunos, além de que esses resumos poderão ser utilizados futuramente para uma
atividade. A prova teórica será composta de conceitos que foram vistos em aula.
O projeto, feito em dupla, será dividido em quatro partes: mini-relatório 1,
para o dia 03/04, contendo uma página e valendo 10%; mini-relatório 2, para o
dia 17/04, contendo três páginas e valendo 20%; mini-relatório 3, para o dia
08/05, contendo cinco páginas e valendo 30%; e por fim as apresentações de 10
minutos cada, valendo 40%, que poderão ser apresentados dia 08/05, 10/05 e 15/
05. Com o projeto dividido desta forma ele poderá ser desenvolvido
progressivamente conforme as aulas vão avançando e não contendo o risco de ser
deixado para ser feito de última hora. Além disso, há também testes, que não
valem nota, mas que serão realizados pelos alunos em quase todas as aulas no
final sobre a matéria que fora dada naquele dia, para ver a absorção do
conteúdo. Como linguagem de programação será utilizado Python (a parte prática
será codificada em sala), com o Jupyter Notebook e usando conceitos básicos da
linguagem, como listas, matrizes e dicionários.
Quando falamos de PLN estamos falando de relações com atividades que envolvam a
linguagem humana, isto é, quando falamos em linguagem artificial, falamos sobre
linguagens de programação como Java e Python, mas quando falamos em linguagem
natural, falamos sobre a linguagem falada/escrita por pessoas. O objetivo de
PLN é conseguir construir mecanismos artificiais que possam permitir a
compreensão da linguagem natural para a realização de tarefas ou aplicações
próximas ao entendimento humano. Sistemas de respostas são exemplos de
aplicações de PLN, mas nem todas as tarefas são fáceis: se formos fazer buscas
de termos ou palavras-chave, é algo relativamente fácil e clássico, mas se
formos fazer buscas de perguntas completas, torna-se uma tarefa complexa e
difícil. A PLN é uma subárea da área de Inteligência Artificial (IA) e
Linguística, definido como processamento (semi)automático da linguagem humana,
sendo associado com linguista computacional, tecnologia e engenharia da
linguagem, sendo usada em contraste com a fala e a tecnologia da fala e da
linguagem. Também está associada com áreas como compiladores e aprendizado de
máquina. Há duas principais visões para PLN, uma delas é a visão da pesquisa,
em que se quer entender como opera  a linguagem humana, e a visão do
desenvolvimento, em que se quer construir sistemas que analisem/gerem linguagem
e possam reduzir brechas entre homem-máquina. Para se entender um texto,
precisa reconhecer o seu contexto, analisar morfologicamente, analisar
sintaticamente e analisar semanticamente. 
Novamente, PLN não é uma tarefa fácil, sendo também atribuído à IA-complete.
Este termo é utilizado para descrever problemas em IA, indicando que a solução
pressupõe uma solução para um "problema forte de IA", ou a síntese de uma
inteligência de nível humano. Um problema considerado IA-complete é muito
difícil, como problemas de visão computacional. No contexto de PLN, o problema
consiste em criar um sistema que seja capaz de entender e falar um idioma
natural e humano. O teste de Turing, anticaptcha e prevenção à SPAM são
exemplos de problemas considerados IA-complete. Problemas são considerados IA-
complete se são um conjunto de problemas de IA e que qualquer problema de IA
possa ser convertido à este com algum algoritmo de tempo polinomial. Há ainda
problemas também considerados IA-hard, que são casos em que o problema IA-
complete que seja de tempo polinomial reduzível. 


Resumo:
 
Forma de avaliação
 
Resumos por aula: ? 40%
Prova de teoria (única): 03/05 ? 40%
Projeto (relatórios+apresentação):? 20%
Prova substitutiva: 17/05
Prova de recuperação: Q2/2018
 
Atribuição de conceitos:
A: nota ? 9
B: 7,5 ? nota < 9
C: 6 ? nota < 7,5
D: 5,0 ? nota < 6
F: nota < 5,0
 
Resumos por aula:
Redação de 250 a 500 palavras (apenas texto).
Envio pelo Tidia (prazo máx. 48h após cada aula).
Todos os resumos serão publicados na pág. da disciplina.
 
Prova de teoria (única): 03/05
Serão abordados os conceitos vistos em aula.
 
Projeto (relatórios+apresentação): Grupos de 2 pessoas.
Mini-relatório 1 (1 página  10%): 03/04
Mini-relatório 2 (3 páginas  20%): 17/04
Mini-relatório 3 (5 páginas  30%): 08/05
Apresentações (10min  40%): 08/05, 10/05 e 15/05
 
Aulas python - Jupyter Notebook
 
 
Introdução
 
Processamento de Linguagem Natural é um ramo da ciência cujo objetivo é tornar
possível o processamento e troca de informação intra dispositivos eletrônicos
ou entre dispositivos e seres humanos com uma interface natural para os humanos
e de forma autonoma, de forma a utilizar a linguagem falada ou escrita pelas
pessoas e seus diversos idiomas.
 
 
O que significa AI-Complete?
 
O termo AI-Complete, significa em tradução livre Inteligencia Artificial -
Completo. Problemas ou tarefas que são consideradas como AI-Complete tem como
característica principal serem altamente complexo. Isso implica que a
dificuldade desses problemas computacionais é equivalente à da resolução do
problema da inteligência artificial central - tornando os computadores tão
inteligentes quanto as pessoas, ou IA forte.
 
Atualmente, os problemas IA Completos não podem ser resolvidos com tecnologia
da computação somente. Esta propriedade pode ser útil, por exemplo, para testar
a presença de seres humanos como com CAPTCHAs e para a segurança do computador
para contornar ataques de força bruta.


A disciplina de Processamento de linguagem natural (PLN), PLN-Q1-2018,
ministrada por Jesús P. Mena-Chalco no primeiro quadrimestre de 2018 terá como
foco abordar as principais técnicas e ferramentas usadas na mineração de texto
em linguagem natural. O método de avaliação será diluído em 3 atividades: 40%
da nota em resumos semanais, 40% numa prova contendo todo o conteúdo e 20% em
um projeto que será apresentado no final da disciplina.
Processamento de linguagem natural é uma subárea da inteligência artificial e
linguística, em que seu objeto de estudo é o texto produzido por pessoas, e
como podemos extrair informações a partir deles. As informações obtidas podem
ser o significado (semântica) do que o escritor quis dizer com as palavras,
pode ser uma sumarização automática de todo o texto, classificação do texto em
categorias, traduções, detecção de plágio dentre outras funcionalidades.
Como qualquer outra área do conhecimento, PLN também possui algumas
dificuldades e desafios inerentes ao seu próprio objeto de estudo. As
linguagens naturais seguem uma lógica sintática bem definida, entretanto, a
sintaxe serve apenas para dar meios representativos à idéias subjetivas que são
extremamente abstratas e difíceis de mensurar de forma algorítmica e
padronizada. Entender como as diferentes figuras de linguagem funcionam e
interagem no universo da semântica é fundamental.
Estes problemas relacionados à entender o significado do texto pertencem ao
conjunto de problemas na inteligência artificial chamada de AI-Complete, ou AI-
Hard. Pertencer a esta classe de problemas significa que, com os computadores e
algoritmos atuais, demoramos um tempo polinomial para resolvê-los, configurando
uma baixa eficiência.


PLN (Processamento de Linguagem Natural) é uma subárea da computação, ou ainda
a interseção entre linguística e Inteligência Artificial, tem como principal
objetivo processar e interpretar uma linguagem natural através de dispositivos
artificiais (computadores). Do ponto de vista da pesquisa, seu objetivo é
entender como funciona a linguagem natural. Numa perspectiva de
desenvolvimento, o objetivo é fazer sistemas capazes de analisarem linguagem
natural e diminuir brechas entre humanos e computadores. Onde linguagem natural
é a linguagem falada/escritas por humanos, e a linguagem artificial é uma
linguagem criada por humanos para uma determinada função, por exemplo
linguagens de programação e código binário.
Um exemplo de PLN simples são algumas das funções Unix, como a wc, que conta
o número de palavras de um dado texto. Exemplos mais complexos são: sistemas de
buscas que interpretam uma pergunta completa e não apenas termos e palavras-
chaves.
Lexicalmente, a linguagem natural é muito mais complexa, pois possui
ambiguidade (o significado de uma palavra muda dependendo do contexto e o lugar
em uma frase) e várias formas de construção gramatical (declarativo,
interrogativo...). Além de que o significado semântico de uma frase não vem
apenas da análise das palavras soltas, e sim do contexto e da frase inteira.
Até hoje, processar uma frase e concluir que é um sarcasmo ou trocadilho é uma
tarefa difícil para o PLN. Por isso, o entendimento perfeito da linguagem
natural é conhecido como um problema IA-complete/IA-hard. IA-hard é um problema
considerado tão difícil quanto desenvolver uma IA tão inteligente e forte
quanto um humano. Essa nomenclatura faz uma referência à classe de problemas
NP-hard, problemas que não podem ser resolvidos em tempo polinomial.


O Processamento de Linguagem Natural (PLN) consiste em uma subárea da
inteligencia artificial e da linguística que tem como objetivo estudar a
capacidade de máquinas interpretarem a linguagem humana. Devido ao seu caráter
interdisicplinar, ao se estudar PLN frequentemente esbarramos em tópicos como
compiladores, prova de teoremas, modelos probabilísticos ,aprendizagem de
máquina, interação humano-computador e inteligencia artificial.
Embora PLN tenha sido extensivamente estudada, esta área da ciência
computacional ainda possui muitas dificuldades em termos práticos e teóricos.
Esta dificuldade se dá por conta das características intrínsecas à linguagem
humana. As linguagens naturais, aquelas faladas e escritas pelas pessoas, são
difíceis de serem aprendidas e sua compreensão tem caráter extremamente
intuitivo e cultural. É difícil fazer com que um programa entenda sarcasmo ou
trocadilhos, por exemplo. Desenvolver um programa capaz de interpretar um texto
tão bem quanto um humano, seria equivalente à desenvolver a própria
inteligência humana em linhas de código. Problemas com este tipo de solução, ou
seja,  que a resposta seria a criação de uma inteligência humana completa, são
chamados de AI-Complete.
Apesar das dificuldades desta área da ciência, muitos avanços foram feitos e
aplicações extremamente eficazes foram elaboradas. Alguns casos populares de
tecnologias que utilizam PLN atualmente são aquelas utilizadas para se
comunicar diretamente com usuários usando alguma linguagem natural. São
exemplos destas tecnologias o chatbot Tay e os assistentes pessoais Siri,
Cortana e Google Now.


Apresentação
O professor apresentou seu currículo acadêmico, assim como sua formação em
Engenharia da Computação, e Mestrado e Doutorado em Ciência da Computação no
IME-USP. Suas áreas de pesquisa são o reconhecimento de padrões e cientometria.
Sobre a Disciplina
A ementa foi apresentada. A bibliografia foi apresentada e o livro Introduction
to Algorithms foi recomendado aos estudantes que ainda não cursaram as
disciplinas Algoritmos e Estruturas de Dados I e II, assim como Análise de
Algoritmos.
Os alunos serão avaliados através de resumos por aula (como este) (40%), uma
prova única de teoria (40%) e um projeto (20%), composto de 3 mini-relatórios e
uma apresentação em maio. A atribuição de conceitos também foi apresentada.
Por fim, Python foi apresentada como a linguagem a ser usada no curso, com o
Jupyter Notebook como ambiente de desenvolvimento.

Introdução ao PLN
PLN se refere às atividades que envolvam a linguagem humana, e não artificiais,
como linguagens de programação. O objetivo do PLN é construir mecanismos
artificiais que permitam o entendimento de linguagem natural para que
computadores realizem tarefas próximas do entendimento humano. O professor
apresentou o programa wc, presente nas distribuições GNU/Linux, como um dos
primeiros processadores de linguagens naturais. O programa faz uma simples
contagem de bytes, newlines (\n) e palavras.
O Google também foi apresentado como um interpretador de linguagem humana,
destacando a informação buscada pelo usuário com precisão, como a data de
fundação da UFABC, mesmo buscado de diferentes formas. Entretanto, também
mostrou que perguntas mais elaboradas e complicadas ainda são complicadas
demais para que o sistema devolva uma resposta específica, empregando então a
busca normal. Ou seja: a busca com palavras chave é fácil, mas com perguntas
completas a necessidade de resolução de inferências, síntese e contexto tornam
a tarefa muito mais difícil. Alguns exemplos no WolframAlpha também foram
exibidos.
O PLN é uma interseção das áreas de Inteligência Artificial e Linguística. Ele
é definido como processamento automático ou semi-automático da linguagem
humana.
Algumas variações dos termos que referenciam o assunto foram apresentadas, mas
o termo PLN será usado na disciplina de forma genérica.
Ele destaca duas visões de PLN:
- Pesquisa: Entender como opera a linguagem humana
- Engenharia: construção de sistemas que analisem/gerem linguagem.
PLN é difícil pois a linguagem humana é difícil de entender, e é aprendida
intuitivamente pelos humanos, o que é difícil de "explicar" para os
computadores. Como por ex, trocadilhos e sarcasmo podem ser obstáculos.
O PLN é apresentado como um problema IA-completo, o que significa que a
dificuldade deste problema computacional é equivalente a de resolver o problema
central à inteligência artificial - fazer computadores tão inteligentes quanto
pessoas, ou "IA forte". Um problema que não é resolvido por um simples
algoritmo específico é chamado de IA-completo.
O professor apresentou as fases da análise de linguagem, como tokenização/
análise fonológica e as análises morfológica, sintática e semântica, além do
processamento do discurso.
Após isso fomos brevemente orientados quanto ao resumo e tivemos um rápido
exercício de Python, onde nos perguntou sobre complexidade temporal e espacial.


A avaliação é dividida em três partes: resumo de cada aula, contendo de 250 à
500 palavras, valendo total de 40% da nota final; uma prova teórica em maio/
5 valendo 40% da nota final e um projeto, dividido em três relatórios e
apresentação, valendo 20% da nota final.
 
Introdução. Processamento de linguagem natural (NLP) busca possibilitar que
computadores realizem tarefas envolvendo linguagem natural. Por exemplo, a
busca semântica. A NLP é uma subárea entre a interseção da linguística e
inteligência artificial. Possui dois principais objetivos: objetivo de pesquisa
- entender como opera a linguagem humana e objetivo de desenvolvimento -
construir mecanismos que analisem/gerem linguagem e reduzir a brecha humano-
computador. Existem 5 níveis de NLP, sendo o primeiro para texto e fala, o
segundo para análise morfológica, o terceiro para análise sintáctica, o quarto
para interpretação semântica e o última para processamento de discurso.
 
Entender linguagens naturais é difícil. Para entender um texto é necessário
entender o seu contexto, a sua gramática e a sua semântica. Assim, alguns
problemas surgem: a linguagem humana é naturalmente difícil e intuitiva e
contém elementos difíceis até para humanos, como sarcasmo e trocadilhos. De
modo mais específico, a NLP tem que lidar com um domínio amplo, vocabulário
complexo, construções gramaticais variadas e com muitos significados possíveis
para uma única expressão. Também existem implicações sociais e cuidados devem
ser tomados para que um sistema de NLP não aprenda discursos de ódio, como
aconteceu com o twitter chatbot  da Microsoft. Assim, a a NLP é classificada
como um problema AI-complete. AI-complete, ou AI-hard, são problemas ou
subproblemas em AI em que a solução requer nível de inteligência próxima ao
nível humano. Outro problema classificado como AI-complete é visão
computacional.


O curso de Processamento de Linguagem Natural (PLN) tem como objetivo tratar de
problemas que envolvam reconhecimento e criação de linguagem natural, utilizada
por humanos. Busca-se criar computadores e programação que possam realizar
tarefas ou aplicações aproximadas ao entendimento humano. Durante a disciplina,
haverá algumas formas de avaliação diferentes, sendo que o envio de resumos têm
peso de 40%, a prova de teoria tem peso de 40% e o projeto final peso de 20%.
Para aprovação na disciplina, é necessário ter aprovação no projeto final, que
consiste na entrega de três mini-relatórios e apresentação de dez minutos.
A PLN é uma subárea entre a inteligência artificial e a linguística e tem duas
perspectivas diferentes, de pesquisa e desenvolvimento. Considerando a
pesquisa, a PLN busca compreender a linguagem humana, tanto escrita quanto
falada. Já os objetivos de desenvolvimento e engenharia estão relacionados com
a criação de dispositivos que interpretem e gerem linguagem, de forma a reduzir
a distância entre homem e máquina. Apesar de haver vários estudos na área, a
PLN ainda é um desafio pelas características subjetivas da linguagem natural,
que tem uma construção rica em vocabulário e significado, que contrastam com a
objetividade do processamento de máquina. Por exemplo, trocadilhos e sarcasmo
são elementos das estruturas semânticas da linguagem natural, que apesar de
serem fáceis de ser aprendidos intuitivamente por uma criança, são difíceis de
serem decodificados pela linguagem computacional.
 O que significa IA-Complete? AI-complete ou AI-hard são problemas de
inteligência artificial que requerem um modo de resolução mais próximo ao nível
humano de inteligência. O desenvolvimento de inteligências artificiais que
solucionem problemas relacionados com habilidades humanas básicas
(entendimento de linguagem falada e escrita, compreensão de imagens complexas,
lidar com situações inesperadas)  atualmente é insuficiente, de forma que a
interferência humana ainda seja necessária em um problema AI-complete.
Algoritmos convencionais permitem a resolução de problemas específicos,
considerados AI-soft, porém ainda têm aplicações limitadas em questões que uma
pessoa teria facilidade de lidar.


Apresentação da disciplina: Vimos que a avaliação da disciplina será dada por
três pesos: Uma prova escrita terá quarenta por cento do peso; os resumos das
aulas submetidos pelo Tidia terão mais quarenta por cento e por fim, o projeto
final, que é composto de 3 relatórios e uma apresentação oral de dez minutos
tem peso de vinte porcento.Podemos dizer que o Processamento de Linguagem
Natural (PLN) seria uma intersecção entre a inteligencia artificial e a
linguagem, associado também a linguística computacional, tecnologia da
linguagem etc.
Nessa aula introdutória, vimos que o PLN tem relação com as atividades que
envolvem a linguagem humana, como fala e escrita. No caso, usaremos o
computador (softwares) como um elemento intermediário, para fins de construir
mecanismos computacionais que permitam o entendimento da linguagem e assim
realizar tarefas dessa mesma linguagem.
Agora, sobre o termo "IA-Complete", pesquisei que é um termo usado para
problemas (ou subproblemas) de Inteligência artificial de nível tão elevado,
que apenas um humano conseguiria resolvê-lo. Ou seja, são inteligências
artificiais que podem fazer tarefas humanas. Como por exemplo, o "The Natural
Language Problem", um sistema capaz de entender e falar uma linguagem natural
assim como humanos. Ou seja, tal problema não poderia ser resolvido por um
simples algoritmo.


O professor iniciou a aula nos mostrando a ementa da disciplina que terá como
temas: introdução ao processamento de linguagem natural; processamento
sintatico; técnicas de análise (ou parsing); gramáticas; interpretação
semantica e processamento de discurso, além da bibliografia básica da matéria.
 
Houve uma tarefa envolvendo complexidade de algoritmos e python. 
 
O objetivo da PLN é construir mecanismos artificias que permitam o entendimento
de linguagem natural para realizar tarefas ou aplicações próximas ao
entendimento humano. Esse objetivo pode ser ainda mais segmentado: para
aplicações com objetivo de pesquisa/academico, normalmente PLN é usada para
entender como opera a linguagem humana; quando o objetivo é desenvolvimento
estamos falando de usar a PLN para criar sistemas que analisem/gerem linguagem
e/ou diminuam a brecha humano máquina.
 
O professor mostrou as paginas do google e do wolfram como exemplos de
aplicações que utilizam PLN e falou sobre os desafios encontrados hoje em dia
pelos sistemas, um deles é a dificuldade que os programas tem de entender
sarcasmos ou ironias em uma fala. Humanos conseguem aprender a linguagem
intuitivamente, máquinas não. A microsoft lançou um chatbot no twitter que
deveria manter uma conversa natural com outros usuários, o projeto teve de ser
cancelado pois a ferramenta, apelidada de TayandYou, aprendeu discursos
racistas, homofóbicos e de ódio.
 
Como tarefa, deveriamos pesquisar o significado do termo "Problema AI-
Completo". Problemas AI-Completo possuem no seu escopo um paralelo entre: visão
computacional, entendimento de linguagem natural e a capacidade de lidar com
qualquer circunstancia inesperada que apareça no mundo real. Atualmente,
problemas AI completos nao podem ser resolvido apenas com tecnologia
computacional e precisam da intereção humana.


                  
Introdução
 
Nessa aula foi apresentado os tópicos que serão trabalhados na disciplina, o
sistema de avaliação do aluno e as datas das entregas dos trabalhos e das
provas. A constituição da nota do aluno é: 40% os resumos das aulas, 40% uma
prova ( 03/05, a recuperação 17/05 ) e 20% em um trabalho que terá entrega de
mini-relatórios ( 03/04, 17/04 e 08/05 ) junto a uma apresentação ( 08/05 ou
10/05 ou 15/05 ), a recuperação está a ser definida. O conceitos que serão
atribuídos em relação a nota são os seguintes: F para uma nota menor que 5, D
para uma nota maior ou igual a 5 e menor que 6, C uma nota maior ou igual a 6 e
menor que 7,5, B para uma nota maior ou igual a 7,5 e menor que 9 e A para uma
nota maior ou igual a 9. A disciplina conta com o apoio do tidia (PLN-Q1-2018:
PLN 2018 ) e um site (http://professor.ufabc.edu.br/~jesus.mena/courses/pln-1q-
2018/ ). Os códigos da disciplina serão na linguagem python e a IDE usada será
a plataforma jupyter notebook (http://jupyter.org/install ), os conceitos de
computação necessários são de compiladores, listas, matrizes e dicionários.

 
Conceitos da aula
 
Processamento de linguagem natural ou PLN é considerado um ramo da inteligência
artificial e trabalha com linguagens, que não são artificiais ( linguagens de
programação, binária, ASCII, etc... ), mas sim com as naturais humanas como:
escrita e fala. Hoje em dia a PLN já está bem desenvolvida para palavras,
chaves e termos conseguido compreender e responder essa perguntas, porém quando
vamos para uma pergunta mais elaborada como frases longas e complexas a
ferramenta não se demonstra muito eficiênte, por causa da grande complexidade
das linguagens naturais humanas, por esse motivo desenvolver um boa ferramenta
de PLN é difícil e um desafio.
 
O que é IA-Complete?
 
IA-Complete é a junção de visão computacional e PLN na tentativa de resolver
qualquer problema do mundo real, porém hoje não temos tecnologias suficiente
para resolver esse problema, pois lidar com situações inesperada é algo muito
complexo. Hoje os métodos que temos resolvem algumas situações, porém está
longe de resolver todos os problemas.

 


A primeira aula iniciou com a apresentação da ementa do curso e do professor.
Um mapa de termos recorrentes ao assunto de Processamento de Linguagem Natural
(PLN) foi mostrado, assim como a ementa do curso, que consiste em introdução ao
processamento de linguagem natural, processamento sintático, técnicas de
análise (parsing), gramáticas, interpretação semântica, processamento de
discurso e aplicações. As formas de avaliação também foram comentadas, e
consistem em uma prova valendo 40% da nota total, três projetos cuja média
valerá 20%, e resumos que devem ser entregues a cada aula, que somados valerão
também 40%. Os programas desenvolvidos durante as aulas utilizarão a linguagem
Python e serão compartilhados utilizando o Jupyter Notebook. Um teste de quatro
questões sobre Python foi aplicado a todos os alunos presentes.
Após isso, conceitos básicos da disciplina foram introduzidos, como a motivação
de sua existência - construir mecanismos artificiais que permitam a um
computador executar atividades que, para humanos, demandariam compreensão sobre
uma de suas línguas. Aplicações foram mostradas (mecanismo de pesquisa do
Google) e vimos que, computacionalmente, buscas utilizando perguntas como input
são mais difíceis de serem processadas do que as que usam palavras-chave, já
que as primeiras demandam que o algoritmo infira e sintetise informações de
fontes variadas.
Processamento de Linguagem Natural foi apresentado como um campo de estudo
englobado pela Inteligência Artificial e pela Linguística, logo, vale-se de
conceitos das duas áreas, mas neste quadrimestre apenas os conceitos clássicos
utilizados no processamento automático da linguagem humana serão cobertos.
PLN possui outros nomes, como Linguístrica Computacional ou Tecnologia da
Linguagem, assim como Linguística é frequentemente referido como Fala ou
Tecnologia da Fala e da Linguagem na literatura acadêmica. É uma área
intimamente relacionada com compiladores, modelos probabilísticos, prova de
teoremas, aprendizado de máquina, interação humano-computador etc. Consiste em
um desafio muito maior que linguagens de máquina pois necessita trabalhar com
vocabulários muito maiores e construções gramaticais muito mais complexas, além
de expressões ambíguas (aplicações de sarcarmos e trocadilhos, por exemplo),
que para os humanos são intuitivas, mas não para computadores.
O caso da inteligência artificial da Microsoft, de nome Tay, foi apresentado na
conclusão da aula. Este projeto foi fechado em 24h após seu lançamento por
aprender discursos racistas e homofóbicos com seus seguidores no Twitter e os
reproduzir, se tornando um exemplo de quão experimental e inovador o campo de
estudos é e o quando ainda se pode avançar nele.
Por fim, foi solicitado que os alunos expliquem o conceito de AI-complete. Um
problema AI-complete consiste em um problema muito difícil de ser resolvido,
que até o momento de sua caracterização, exigiria que o problema central da
inteligência artificial - ou seja, tornar computadores tão inteligentes quanto
pessoas - fosse resolvido anteriormente. Em termos práticos, chamar um problema
de AI-complete significa que, até o presente momento, não existem formas
conhecidas de resolvê-lo utilizando apenas algoritmos.


A complexidade de tempo e de memória é diferente para um algoritmo. A
complexidade de tempo está relacionada à quantidade de execuções de instruções
e a complexidade de memória está relacionada à quantidade de memória usada
durante o processamento.
PLN é uma área mais abrangente do que PI pois abrange interpretação semântica
de linguagem natural (humana), e esta pode ter múltiplos significados. PLN é
muito mais complexa e difícil também pois deve contemplar sarcasmo e
trocadilhos. PLN é uma subárea de AI.
PLN é utilizado por empresas com objetivo comercial pois interpretando textos é
possível oferecer melhores ofertas a um público específico e também calcular a
aprovação ou desaprovação do produto ou serviço oferecido. É muito utilizado
atualmente pelo Google mostrando o resultado de uma busca no formato de uma
resposta objetiva.
Uma área da PLN é a geração de resumos, ou seja versões resumidas de textos.
Entretanto, algo simples como um contador de palavras em um texto já pode ser
considerado um rudimento de PLN, como por exemplo, o wc do Linux.
O que significa AI-complete (analogia com NP-complete)?
Um problema NP-complete é um problema resolvido em tempo polinomial não
determinístico, e uma solução  em tempo polinomial determinístico de qualquer
problema dessa categoria serve também como solução para qualquer problema NP-
complete.
Analogamente, um problema AI-complete é um problema que requer a solução do
problema central da inteligência artificial, ou seja, a inteligência artificial
geral, uma inteligência artificial equivalente à inteligência humana. PLN é
considerado um problema AI-complete.


Vimos na aula 1 introdutória a PLN que ela tem relação com atividades humanas,
esse se torna um dos motivos de sua complexibilidade. Uma das primeiras
ferramentas para processamento de PLN é a função wc do bash, que conta o número
de caracteres, palavras ou linhas de um texto. PLN vê questo?s mais complexas,
como resposta a uma pergunta específica terá orquestra em Santo André neste
domingo?, já uma busca é muito mais simples, como hora atual.
Usaremos o termo PLN para nos referirmos a linguagem natural, dentre outros
termos que existem.
Existe uma grande diferença entre PI e PLN, enquanto PI trata apenas o que se
pode programar, como, por exemplo, linguagens de programação e outros tipos de
linguagens que são interpretadas por um computador, PLN inclui qualquer
linguagem humana, com todos os seus termos nacionais, regionais, e infinitas
peculiaridades, não existe nenhuma exatidão em uma linguagem humana, mesmo com
uma grafia correta. Um exemplo, é a dificuldade de um computador poder
processar e interpretar um trocadilho, por exemplo, a frase "na vida tudo muda
até bermuda", até uma pessoa é possível não entenda este trocadilho
rapidamente.
Problemas de PLN são comparáveis a problemas NP-completos ou IA-completos.
Os problemas mais difíceis, em Inteligência Artificial, são informalmente
conhecidos como AI-completos ou AI-difíceis, a dificuldade desses problemas
computacionais é equivalente à da resolução do problema da inteligência
artificial central, tornando os computadores tão inteligentes quanto as
pessoas. Os problemas AI-completos não podem ser resolvidos com a computação
atual, exigindo, computação humana. Esta propriedade pode ser útil, por
exemplo, para testar a presença de seres humanos como com CAPTCHAs e para a
segurança do computador, contornando ataques de força bruta.


O Processamento da Linguagem Natural (PLN) tem por objetivo o desenvolvimento
de mecanismos artificiais que possibilitem o entendimento da linguagem natural,
escrita e/ou falada por humanos, co o intuito de realizar ações próximas ao
comportamento humano.
O PLN engloba duas sub-áreas das ciências a inteligência artificial e a
linguística.
Algumas áreas de estudos que se relaciona com PLN são: Compiladores
(autômatos), Prova de teoremas, Modelos probabilísticos, Aprendizado de
máquina, Interação humano-computador e Inteligência artificial.
Teve inicio em meados dos anos 50. Ainda possuem desafios devido a alta
complexidade  da própria linguagem natural, por exemplo como fazer a computador
entender o sarcasmo sendo que até os humanos tem dificuldade na sua
interpretação.
As técnicas de PLN se não forem bem aplicadas podem produzir erros inesperados
como o caso do Chatbot criado pela Microsoft, denominado de @TayandYou,
utilizou as mensagens do Twitter como base para o aprendizado da linguagem
natural, em menos de 24 horas o Chatbot apresentou tendências racistas.
Alguns exemplos de PLN simples presentes no nosso cotidiano são os utilizados
pelos sistemas de busca como o Google. O principal mecanismo  é a busca de
palavras-chaves em meio a uma base. Já um nível mais um pouco avançado que
esses sistemas são capazes de realizar é interpretar uma pergunta e inferir uma
resposta.
Dentro do campo da inteligência artificial existe a denominação AI-complete ou
AI-hard que seria a inteligência artificial tão inteligente quanto uma pessoa.
Sendo a PLN um subcampo da AI existe também a denominação NP-complete ou NP-
hard que seria um mecanismo artificial capaz de compreender a linguem natural
tão bem quanto uma pessoa.


Nessa primeira aula do curso foram apresentadas as formas de avaliação, que são
elas: resumos (40%) que serão entregues pelo Tidia e devem ter entre 250 e 500
palavras, posteriormente serão divulgados na página da disciplina para
visualização; prova única (40%) e teórica no final do quadrimestre; e projeto
final (20%), sendo este constituído de relatórios e uma apresentação, podendo
ser realizado em grupo de até 2 pessoas. Essa composição embora não muito comum
parece interessante, com clara ênfase no estudo/absorção do conteúdo pelo
aluno. Outro detalhe é que para a aprovação na disciplina é necessária
aprovação no projeto final.
Foram mostrados conceitos introdutórios de Processamento de Linguagem Natural
(PLN), que é uma subárea da Inteligência Artificial e Linguística que trata da
linguagem humana, falada e/ou escrita. Se relaciona com outros ramos da
computação e matemática também, como aprendizado de máquina, probabilidade,
autômatos e outros.
Um dos objetivos em PLN é possibilitar que computadores possam interpretar a
linguagem escrita ou falada, de uma ou mais línguas naturais, de forma
semântica, sintática, morfológica e seus demais aspectos. Diante de tal
propósito comumente formam-se equipes de trabalho interdisciplinares formadas
por cientistas da computação, matemático, linguistas, lógicos e especialistas
de assuntos que possam contribuir com as soluções dessas questões.
Para a interpretação dessa linguagem (texto ou som) é feito um pré-
processamento que mantém apenas a estrutura importante da linguagem. Ainda hoje
encontram-se diversos desafios no processamento da linguagem humana devido a
sua variação, subjetividade, significados intuitivos, expressões não literais e
outras figuras de linguagem como sarcasmo, até mesmo mudanças de humor.
Podemos caracterizar o PLN dentro da IA-complete que é uma classe que problemas
difíceis da computação que não podem ser resolvidos com algoritmos simples e já
conhecidos e que geralmente envolvem visão computacional e compreensão da
linguagem.
 

 


"Se eu aprendi, porque o computador não consegue?". Essa é provavelmente a
pergunta que motiva os programadores de IA, mas no final do dia eles se deparam
com um problema ainda maior: falta de avanço tecnológico. AI-complete é a
classe de problemas considerados muito avançados para uma inteligencia
artificial resolver sozinha, pois ela precisaria pensar como um humano. Por
enquanto, não temos como criar isto, mas existem algumas liberdades que a IA já
possui. Uma delas é a habilidade de processar linguagem natural. Essa área toma
conta das atividades que envolvem a linguagem humana e serve para criar
mecanismos artificiais que entendam a linguagem o mais humanamente possível.
Parece fácil, porém quanto mais complexa a estrutura frasal mais o computador
tem que levar em consideração e assumir corretamente para poder responder. O
objetivo do curso é estudarmos as técnicas clássicas
para processar automaticamente a linguagem humana. Se o objetivo for pesquisa
será uma apresentação de como opera a linguagem humana, se for engenharia, a
ideia é construir sistemas que analisem ou gerem linguagem reduzindo a brecha
homem-máquina. Para entender um texto é necessário reconhecer seu contexto,
então fazer a análise morfológica, sintática e semântica. Esses são os núcleos
do curso que tentaram ajudar a driblar os problemas de como lidar com
trocadilhos e sarcasmos por exemplo. E por ser uma matéria de faculdade não
poderiam as formas de avaliação, constituídas por uma prova única (40%), os
resumos de cada aula (40% para até 48h depois) e um projeto final em grupo 
(20%) que consiste de 3 relatórios e uma apresentação. Durante o curso usaremos
preferencialmente os conceitos básicos da linguagem. E lembre-se: Baixe o
Jupyter no seu Notebook.


Quando se estuda Inteligência artificial é comum nos depararmos com diversos
ramos, um deles pode ser denominado como IA-Completo ou IA-hard que são
responsáveis por representar os difíceis desse ramo. Isso implica tentar mesmo
que dada tamanha dificuldade solucionar problemas centrais da inteligência
artificial tornando máquinas tão capazes quanto seres humanos. Classificar uma
problema como IA-completo significa que este não pode ser resolvido por um
algoritmo específico simples. De forma hipotética podemos dizer que dentro
desses problemas está o processamento da linguagem natural, computação
supervisionada e quaisquer que tratam de circunstâncias inesperadas enquanto
tenta-se resolver problemas do mundo real. É sabido que nos dias de hoje
problemas IA-Completo ainda requerem um vetor humano para que sejam
solucionados, um exemplo disso é a utilização de CAPTCHA onde teoricamente é
necessário a interação humano-máquina para que seja realizado. A expressão IA-
complete, criada com referência aos termos NP-completo e Turing-completo,
designa um problema cuja resolução é considerada equivalente à criação de uma
inteligência artificial realista. O entendimento abrangente da linguagem
natural geralmente é considerado um problema de IA-completo porque a
compreensão dos textos requer uma compreensão dos conceitos associados a eles.


   A matéria apresenta um sistema de avaliação que garante que o aluno ou aluna
aprenda algo, e não somente decore passagens ou um método que está escrito num
livro, e ainda aplique algo que aprendeu no quadrimestre (ou não, ele(a) já
sabia) no final da matéria, isso também alivia um certo peso de todo seu
aprendizado estar sendo testado por dois pedaços de folhas que estarão sendo
julgados imparcialmente (ou não).
    O objetivo dessa matéria que é um campo de estudo em inteligência articial
é trazer a máquina mais perto do ser humano, isto é, produzir um computador
capaz de interpretar a linguagem humana sem depender de textos com perguntas
pré-definidos e dar respostas pré-prontas.
    Empresas grandes, principalmente as que lidam muito com usuários, como
Google, Microsoft e Apple, já criaram seus próprios serviços em que os usuários
conversam com a máquina, ainda não é um sistema perfeito, muitas vezes apenas
usado para fazer pesquisas rápidas em vez de ficar digitando "google.com" e as
palavras chaves ou uma pergunta, mas algumas pessoas tentam criar conversas com
esses serviços e sucedem, mas até onde isso foi previsto em um script? Será que
o dia em que teremos um computador capaz de pensar sozinho e ter emoções está
muito longe?
    Pode ter sido extremamente ambicioso e incompleto, mas, na Arábia Saudita
um robo, chamado de Sophia, recebeu cidadânia, baseado em conversas e
entrevistas pode se dizer que é um "chat bot" com corpo, mas seria esse um
passo na direção certa de criar uma solução para problemas no nível de "IA-
complete"? 
    IA-complete se refere a um problema de tanta dificuldade que é comparável a
fazer uma IA tão inteligente quanto um humano, chamar um problema com este nome
pode se inquirir que o problema não é resolvido por algoritmos simples. 


Introdução ao Processamento de Linguagem Natural
1. O que é Processamento de Linguagem  Natural ?
O PLN é uma subárea da ciência da computação voltada a interação entre
computador e a linguagem natural, isto é, o PLN é uma manipulação automática,
via software, da linguagem humana, como, por exemplo, um discurso ou um texto.
O estudo do PLN data do início dos anos 50, porém outros trabalhos correlatos
de períodos anteriores podem ser encontrados. O PLN tornou-se muito popular com
o avanço nas áreas de linguística e inteligência artificial.
2. Objetivo do PLN
O termo "linguagem natural" refere-se ao modo como nós, humanos,  nos
comunicamos uns com o outros. Existem diversas formas de comunicação como,
linguagem corporal - expressões e movimentos do corpo -, por exemplo. Contudo,
o PLN foca principalmente em discurso e texto. Constantemente, entramos em uma
conversa e discutimos sobres os assuntos do dia a dia ou mandamos um e-mail, um
sms, lemos um livro ou algum artigo na internet. Todas essas atividades mostram
o quão importante é este tipo de dado, logo isso gera a demanda para que
possamos entender e automatizar a linguagem natural.
Portanto, podemos concluir este tópico dizendo que o objetivo do PLN é criar
meios artificiais capazes de compreender a linguagem natural de maneira
inteligente e útil para ajudar na realização de tarefas próximas ao
entendimento humano.
3. Desafios
Ao utilizar o PLN um desenvolvedor pode realizar tarefas como tradução, análise
de sentimento, reconhecimento de discurso, resumo de textos, chatbot, correção
gramatical, converter um discurso em texto e entre outras. Apesar das inúmeras
aplicações ainda existem diversos desafios a serem superados pelo PLN.
A linguagem humana é pouco precisa e está em constante mudança, logo entender
apenas as palavras não é suficiente, é preciso entender os conceitos, as
referências, às expressões, o contexto e como tudo isso junto dá o significado
ao texto.
Portanto, apesar dos humanos serem bons aprendizes da linguagem natural,
descrever as características que governam o significado de um texto ou discurso
ainda é um problema difícil, mesmo para os humanos. Logo, essa ambiguidade é o
que torna o PLN um desafio.
 4.  O que significa AI-Complete ?
Em inteligência artificial o termo AI-Complete é nome dado aos problemas que
apresentam a maior dificuldade para serem solucionados. Os problemas de AI-
Complete tentam sintetizar o nível de inteligência humana, por exemplo: o
problema da visão computacional é construir um sistemas que permite uma máquina
ver tão bem quanto um humano ou PLN que tenta desenvolver um sistema que o
permita entender a linguagem natural tão bem quanto um humano.
5. Referências
https://blog.algorithmia.com/introduction-natural-language-processing-nlp/
https://machinelearningmastery.com/natural-language-processing/
https://www.researchgate.net/publication/251422306_AI-Complete_AI-Hard_or_AI-
Easy_Classification_of_Problems_in_Artificial_Intelligence
https://en.wikipedia.org/wiki/AI-complete
https://www.techopedia.com/definition/17520/ai-complete


Aula inicial da disciplina.
- Nessa disciplina os conceitos de PLN que serão dados serão de processamento
automático da linguagem humana classico.
- Processamento de Linguagem Natural é um campo de estudo tanto da IA quanto da
Linguistica, portanto ele abrange termos e conhecimentos das duas categorias.
Pode também ser chamada de  Linguístrica Computacional ou Tecnologia da
Linguagem.
- PLN se intervenciona com modelos probabilísticos, interação humano-
computador, machine learning, entre outros
- Em buscas, o argumento varia sua dificuldade. Busca com palavra chaves são
mais fáceis do que com pergunta, pois para perguntas o algoritmo tem que
sintetizar as respostas de fontes varias.
- Linguagem humana é um desafio maior que a linguagem de maquina devido as
ilimitados de vocabulários, construções gramáticas, sintáticas e semânticas.
-  Tay da Microsoft era uma IA para Twitter que foi encerradas 24h depois do
inicio da sua interação com o publico divulgar opniões racistas e homofobicas.
Que mostra como o que você alimenta para no aprendizado de maquina definira os
resultados finais.
- Sobre AI-complete: esse termo se refere a problemas que a AI por si so não
consegue resolver, que alem de algoritmos o problema existe uma complementação
humana.
 


A aula inaugural do curso de Processamento de Linguagem Natural introduziu o
conceito básico de linguagem natural, que é a língua falada e escrita por
humanos. O PLN é o campo que tem como objetivo utilizar o estudo da Linguística
e Inteligência Artificial para criar mecanismos capazes de compreender e
realizar atividades referentes à semântica. Existem diversas aplicações do PLN
sendo que, nesta aula falamos mais sobre o de respostas à perguntas e alguns
mecanismos muito inteligentes foram citados, como o Wolfram Alpha, que por
enquanto só responde perguntas em Inglês.
Os grandes desafios do processamento automático da linguagem estão relacionados
à dificuldade dos programas entenderem e extraírem significado de ironias e
trocadilhos. Os mecanismos que interagem com humanos para aprender com cada
interação estão suscetíveis à erros éticos e  por não serem capazes de
compreender significados que às vezes estão relacionados à subjetividade humana
e suas relações, acabam reproduzindo comportamentos inadequados por não
possuírem inteligência social que só interações humanas são capazes de
proporcionar. É desafiador a criação de mecanismos que consigam interagir e
interpretar de forma tão humana alguns contextos. No PLN, criar mecanismos com
um nível de inteligência próximo ou superior ao humano, capazes de entender e
gerar novas linguagens humanas, ainda é inconcebível. O termo AI-hard engloba
essas questões e atualmente uma provocação ao AI-hard são os captchas. É muito
difícil conseguir criar um programa que consiga resolvê-los assim como humanos
podem.
Apesar disso, é essencial que tais impasses existam, pois só tentando
solucioná-los é que se conseguirá progresso no Processamento de Linguagem
Natural.


Resumo da aula 1
 
A avaliação para a disciplina será composta pelas notas dos resumos de cada
aula, que devem ser entregues pelo tidia em até quarenta e oito horas contendo
de duzentos e cinquenta a quinhentas palavras, que valem quarenta por cento da
nota final, mais a nota de uma prova teórica abordando os conceitos vistos em
sala que vale quarenta por cento também mais a nota do projeto composta por
três entregas de mini-relatórios de uma, três e cinco pa?inas e que valem dez,
vinte e trinta por cento da nota do projeto, respectivamente, e pela
apresentação final de até dez minutos que vale quarenta por cento da dessa
nota, o projeto como um todo vale vinte por cento da nota final.
A disciplina irá apresentar os conceitos fundamentais da área de Processamento
de Linguagem Natural que tem como principal objetivo criar mecanismos
artificiais que permitam o entendimento da linguagem natural para realização de
tarefas ou aplicações que exijam entendimento próximo ao humano, como por
exemplo busca envolvendo perguntas completas.
O processamento de Linguagem Natural é uma subárea da Inteligência Artificial e
da Linguística e é definida como o processamento automático ou semi-automático
da linguagem humana, está relacionada com a Linguística Computacional, com a
Tecnologia e a Engenharia de Linguagem. Dentro da Ciência da Computação se
relaciona com as disciplinas de Compiladores, Inteligência Artificial, Modelos
Probabilísticos, Aprendizado de Máquina, entre outras. Podemos dividir a
disciplina em duas visões principais: a pesquisa, que tenta entender como opera
a linguagem humana escrita e falada, e a engenharia, que tenta construir
sistemas que analisem/gerem linguagem natural reduzindo a brecha entre o homem
e a máquina.
O Processamento de Linguagem Natural ainda é um desafio pois lida com diversos
aspectos linguagem humana ainda muito difíceis de entender do ponto de vista
computacional como o grande poder de expressão, o léxico complexo, os muitos
tipos de formas gramaticais e a riqueza semântica das expressões. Assim como a
área de Visão Computacional seu entendimento completo é um problema conhecido
como IA-complete. IA-complete é um termo informal para classificar um problema
dentro da área de Inteligência Artificial que faz analogia a classificação NP-
complete, significando que a resolução de um problema desse tipo é equivalente,
ou tão difícil quanto, a resolução do problema central da IA, ou seja, tornar
as máquinas tão inteligentes quanto os seres humanos.


Expressões regulares são cadeias de textos especiais que podem ser processadas
em meio a um texto. De forma a encontrar esses padrões. Muito utilizados em
extração de textos.
A expressões regulares contemplam diversos operadores que podem ser utilizados
para criar um padrão especifico para a cadeia de texto que você bem preferir.
entre elas estão disjunções, como operadores OU.
Exemplo:
[cC]apivara|[cC]arpincho: captura uma ou outra cadeia (a ou b com o operador I
em "a|b"), com ou sem letra maiuscula.
Ainda, temos caracteres que permitem reconhecer uma repetição de padrões, ou
apenas um elemento daquela repetição. Como:
?: Caractere predecessor opcional
*: Caractere predecessor: 0 ou mais vezes
+: Caractere predecessor: 1 ou mais vezes
.: Placeholder para qualquer caractere apenas uma vez
Alem disso, temos a funcionalidade de ancorar, como ^(inicio da linha) e $
(final da linha)
 
Como muitos Regex podem ser complexos, podemos usar o Regex101 para entender
como cada padrão esta sendo computado e testar realtime sem necessidade de
codificar diretamente.
Com o regex, (e extendendo para qualquer problema de reconhecimento), podemos
analisar os resultados Falsos ou Verdadeiros e Positivos ou Negativos. Sendo
assim devemos minimizar os Falsos Positivos (resultados que retornam da regex
mas não são o que queremos procurar) e os Falsos Negativos (resultados que
queriamos retornar mas não retornaram).
Uma regex é entendida como um Automato finito deterministico. Basicamente uma
maquina de estado com um numero finito de estados.
plus da aula: como o regex se comporta como um automato finito deterministico,
ele não salva seus estados anteriores, mas as principais implementações de
regex implementam formas de armazenar o ultimo padrão capturado, dessa forma
você pode referenciar palavras encontradas dentro de um regex, usando "\x"
sendo x a posição do padrão numa especie de lista de padrões.
 
 


Expressões regulares são sequencias de caracretes construídas segundo uma
linguagem formal que permitem identificar padrões textuais. Na prática, elas
servem para executar buscas em textos, e por essa razão elas usualmente
consistem na primeira etapa do processamento de textos. Os padrões encontrados
por elas são utilizados em algoritmos de classificação e de aprendizado de
máquina.
Uma expressão regular pode ser representada por um autômato finito
determinístico (AFD), que é uma representação abstrata de uma máquina
com estados e transições entre eles, que são acessadas á medida que instruções
são lidas. 
ERs por padrão são algoritmos gulosos, ou seja, sempre procuram pela cadeia de
caracteres mais longa que atende aos critérios especificados. As linguagens de
ERs permitem o uso de identificadores para informar que essa estratégia não
deve ser utilizada. 
Expressões regulares podem ser utilizadas para a construção de formas
rudimentares de chatbots; ELIZA é um exemplo delas. ELIZA era um sistema que
possibilitava a manutenção de uma conversa através da imitação do que o usuário
diz.
As ERs por si só permitem que encontremos palavras ou expressões mais
utilizadas em um texto, permitindo que tenhamos umanoção de seu coniteúdo sem
precisar le-lo.
Durante a aula de processamento de linguagem natural na qual foram abordados
esses tópicos, fomos apresentados à sintaxe das ERs, parcialmente indicada a
seguir:
Colchetes ([]) indicam disjunções de caracteres; "[0-9]" indica a busca por um
único caractere numérico entre 1 e 9;
Acentos circunflexos (^) no início de uma cadeia indicam a negação da busca
indicada a seguir; "[^5-6]" indica que devem ser procuradas as cadeias que não
possuem 5 ou 6;
Barras verticais (|) executam a disjunção entre palavras; "tu|você" indica que
devem ser buscadas as ocorrências de "tu"  ou de "você";
Interrocações (?) indicam que o caractere antecessor é opcional; "você?s"
indica que tanto "você" quanto "vocês" serão encontrados;
Ponto final (.) indica que qualquer caractere pode estar naquela posição;
"nes.a" identificará "nessa", "nesta" e "nesca", por exemplo.
Asterisco (*) indica que o caractere antecessor pode aparecer zero ou mais
vezes; "soco*rro" identificará "socrro", "socorro" e "socoooooorro", por
exemplo.
Mais (+) indica que o caractere antecessor pode aparecer uma ou mais vezes;
"soco+rro" identificará "socorro" e "socoooooorro", por exemplo, mas não
"socrro";
Os caracteres âncora indicam o início (^) ou o final ($) de uma linha; "^UFABC"
e "UFABC$" encontrarão a string "UFABC" no inicio ou fim de uma linha,
respectivamente.
Efetuamos uma avaliação sobre o uso de expressões regulares, na qual testamos
os conhcecimentos adquiridos e vimos um exemplo de busca que não podia ser
realizada com as ferramentas vistas. 


Expressões regulares:
Também conhecidas como Regex ou regexp, as expressões regulares, cadeias de
texto especiais,  podem ser utilizadas na segmentação frases,
radicalização,divisão de tokens e normalização de texto. Um exemplo dado é o
efeito de disjunção dos [] na busca de expressões regulares, [1234567890] ou
[0-9] para qualquer 1 dígito, [^A-Z] não um caractere maiúsculo, eu|voce para
procurar por eu ou voce, porém, vale notar que os [] são utilizados apenas para
um caractere. Outros exemplos de caracteres especiais são: '?' para caractere
predecessor opcional ,'*' caractere predecessor 0 ou mais vezes ,'+' para
caractere predecessor 1 ou mais vezes e '.' para qualquer 1 caractere que
substitua. para representar o início de linha, o caractere '^' é utilizado na
forma ^[A-Z], já o final de linha é representado por $, como padrão, '\' é
utilizado como escape, indicando que o próximo caractere não é especial,
exemplo: \^ ou \. .
Pode-se utilizar o site recomendado regex101.com para testar o uso  das
expressões regulares. Existem alguns atalhos para facilitar as buscas por
expressões regulares, como no caso do \W  que é equivalente a [^a-zA-Z0-9], ou
\d para qualquer dígito.  
EM PLN é importante minimizar os falsos positivos e falsos negativos, assim
aumenta-se a precisão e a cobertura, pois obtém-se somente o que é desejado.
Vale lembrar que uma expressão regular geralmente pode ser representado por um
autômato finito determinístico, podendo-se construir um para verificar a
validade da ER.
 
Teste de avaliação:
Ao todo foram seis questões sobre expressões regulares, sendo que duas já
possuíam respostas. Para testar as respostas, foi utilizado o site mencionado
anteriormente. A primeira questão já estava respondida, a segunda era
relativamente fácil, sendo necessário lembrar dos limites da palavra, já a
terceira foi um pouco mais complexa, utilizando também os conceitos de começo
de linha,  limite de palavra e fim de linha. A quarta também já estava
respondida e ofereceu o conceito de agrupamento, a quinta desenvolveu o uso de
agrupamento em conjunto com repetição  {}.
A última questão abordou um conceito ainda não visto, que se refere a
capacidade de memorização, pois para detectar duas palavras repetidas
consecutivas, é necessário algum tipo de memória.Tal comando é '\1'.


Expressões regulares (ER) podem ser utilizadas para extrair trechos a partir de
um texto, com aplicações como normalização de um texto, divisão de tokens e
radicalização. Em outras palavras, ER são cadeias de texto especiais para a
extração de trechos de texto. Um exemplo dado foi caso deseje procurar sobre o
animal capivara, neste caso pode ser procurado como [cC]apivara, que significa
que será procurado tanto capivara como Capivara. Há características de ER:
disjunções, em que a cadeia de caracteres que esteja dentro de colchetes
especifica uma disjunção de caracteres a ser buscada; negação de disjunções,
que é caracterizada pelo símbolo acento circunflexo; disjunção com barra
vertical, que pode ter o mesmo significado de busca "ou"; outras opções, como ?
(caractere predecessor opcional), * (caractere predecessor 0 ou mais vezes), +
(caractere predecessor 1 ou mais vezes, . (caractere em questão repsentado pelo
ponto poderá ser qualquer um); caracteres âncoras, início de linha (^) e final
de linha ($). 
Em PLN há dois tipos de erros que devem ser lidados, falso positivo, que
identifica cadeias que não deveriam ser identificadas, e falso negativo, em que
não identifica cadeias que deveriam ser identificadas. Para minimizar falsos
positivos aumenta-se a precisão, e para minimizar falsos negativos aumenta-se a
cobertura.
As ER são utilizadas em classificadores, podendo ser descrito como um autômato
finito determinístico. Para a verificação se uma ER está bem construída é
descrito a instrução utilizando o autômato finito determinístico. Também são
utilizadas estratégias gulosas. Como forma de anexar o conteúdo, fora dado um
teste de avaliação em sala, com perguntas como selecionar todas as cadeias que
terminam com a letra m e selecionar todas as cadeias alfabéticas que tenham as
palavras 'gato gato' ou 'cachorro'. O exercício mais complexo deste teste fora
o que pedia para que fosse selecionado todas as cadeias com duas palavras
repetidas consecutivas.


Expressões Regulares
 
Primeiro passo para extrair texto ou procurar determinadas palavras ou frases
em textos já coletados
 
Podem ser usadas para:
 
Normalizaçao de texto (e.g., padronizar o texto convenientemente)
Divisão em tokens (e.g., divisão em palavras usando os espaços?)
Radicalização (e.g., lemmatization, stemming)
Segmentação de frases (e.g., divisão em frases usando a pontuação)
 
Outros nomes: Regex, Regexp
 
 
Utilização:
 
[cC]apivara  = capivara ou Capivara
[1234567890] = apenas um digito numeral 0 ou 1 ou 2 ...
[0-­9]  = Apenas 1 digito
[a­-z]  = Um caractere em minúscula
[A­Z]  = Um caractere em maiúscula
[5­-8]  = 5 ou 6 ou 7 ou 8
^  = A negação válida como primeiro caractere
[^A-­Z]  =  Não um caractere em maiúscula
[^Ss]  = Nem 'S' nem 's'
[^a^]  = Nem a nem ^
[cC]apivara|[cC]arpincho = capivara Capivara carpincho Carpincho 
pa?ra  = para pra
aa*h!  = ah! aah! aaah! aaaaah!
o+h!  = Oh! Ooh! Oooh!
ca.a  = casa caza caça
[ab]*  = aaaaaaa ababababa aaaabbbbb baaaaaaaa bbbbbbbbbbbbb
[0­9][0­9]*  = Um número de pelo menos um digito.
a$  = Casa
\.$  = Casa.
^A UFABC\.$  = Uma linha contendo exatamente A UFABC.
\  = \ serve para indicar que o caractere sucessor não é especial.
 
 
Dois tipos de erros:
 
Falso positivo: Identificar cadeias que não deveriam ser identificadas
(humanidade, idade, cadeira)
 
Falso negativo: Não identificar cadeias que deveriam ser identificadas (De)
 
 
Teste de avaliação
 
1 = [a­zA­Z]+
2 = [a­z]*m\b
3 = ^[0­9]+\b.*\b[a­zA­Z]+$
4 = (gato)|(cachorro) 
5 = (gato\s){2}|(cachorro) 
6 = \b(\w+)\W+\1
 
Expressões são bem parecidas com a representação de gramaticas de LFA, procurar
mais contato com RE para entender melhor 


O objetivo da aula 2 de PLN foi mostrar uma ferramenta para procurar fragmentos
de texto específicos num texto maior. Tendo um texto de referência com muitas
páginas, como podemos procurar por um termo, frase ou conjunto de palavras de
forma automática e com flexibilidade para encontrar algumas variações
definidas? Dentre as opções disponíveis, expressões regulares é uma boa
escolha; com ela podemos fazer pesquisas bem complexas com poucos caracteres de
comando.
    Para fazer uma pesquisa, é muito simples, basta escrever a palavra ou frase
desejada, no buscador. Caso seja preciso, existem alguns modificadores que
aumentam a flexibilidade da pesquisa, por procurarem várias combinações
diferentes de uma só vez. O modificador de disjunção, por exemplo, -
representado por []- faz com que cada elemento no seu interior se combine com
o resto da expressão na busca de forma individual. A pesquisa [aA]bacate é
similar a extrair ambas as palavras Abacate e abacate escrevendo apenas um
termo. O operador disjunção aceita outros parâmetros,, tais como, 0-9, a-z, A-
Z, ou a combinação simultânea de todos os anteriores, a-zA-Z0-9. Também podemos
usar o operador ^ para negar algum caractere dentro da disjunção; a pesquisa
abacate[^ ] retornaria todas as palavras abacate em que não possuem um
espaço logo em seguida, pois a busca aceitaria qualquer caracter exceto o
espaço. Saídas válidas para o exemplo anterior poderiam sem abacate,,
abacate., por exemplo.
    Caso queiramos identificar uma expressão que possui um conjunto de palavras
anteriores bem definido podemos extrair essas palavras sem, necessariamente,
saber cada caractere. Um exemplo simples é procurar numa lista, em que cada
linha é uma matéria diferente, todas as matérias que começam com
Processamento mas também incluir seu código e as outras palavras que também
fazem parte do nome da matéria: [A-Z0-9- ]+Processamento.*\b. Neste exemplo o
primeiro modificador [A-Z0-9- ]+ começa com a disjunção que seleciona
qualquer caractere alfanumérico somente de letras maiúsculas (já que sabemos
que o código da disciplina não possui minúsculas), números, espaço em branco e
o caracter de subtração. O sinal de + após o colchetes indica que temos um ou
mais caracteres dentro do intervalo da disjunção. Ou seja podemos ter somente
uma letra ou quantas forem necessárias desde o início da linha até chegar na
palavra Processamento que a extração incluirá todas elas. Depois temos a
palavra Processamento, já que ela é o nosso termo de pesquisa principal, e
logo em seguida, temos um elemento .* que indica qualquer string de
caracteres, e no final temos o \b, que quebra a extração na última palavra,
caso tenha um espaço adicional.
Existem inúmeras outras ferramentas dentro das expressões regulares que são
extremamente úteis no cotidiano de um programador. Como aplicação dessas
técnicas,  temos o ELIZA, um software do tipo chat criado na década de 60 que
conversa com um humano. O programa tenta encontrar palavras chave para
responder de acordo com o assunto, geralmente uma pergunta, fazendo um trabalho
rudimentar de psicanalista. E conseguimos reproduzir a funcionalidade do ELIZA
nos dias de hoje usando um banco de dados que assimila cada palavra chave com
uma possível resposta para aquele termo. Esta pesquisa das palavras chave fica
fácil usando as técnicas vistas anteriormente.


Uma expressão regular (ou regexp) é uma cadeia de texto, escrito em uma
linguagem formal (normalmente padrão em todas as linguagens) cuja finalidade é
buscar cadeias de textos específicas em um texto. Pode ser usado para
normalizar um texto (deixar tudo maiúsculo ou tirar acentos, por exemplo),
reduzir palavras para a sua versão mais simples (are vira be) e dividir uma
frase baseada na sua pontuação.
Existem dois tipos de erros possíveis quando usamos uma expressão regular.
Quando um falso positivo ocorre, uma cadeia de caracteres é selecionada quando
não deveria. Já um falso negativo ocorre quando uma cadeia atinge os critérios
de seleção, porém não é selecionada. Para minimizar falsos positivos, é
necessário aumentar a precisão, ou seja, restringir mais a sua ER, mesmo que
isso gere alguns falsos negativos, a chance de ocorrer um falso positivo é
menor. Para minimizar falsos negativos, o contrário deve ser feito, aumentar a
cobertura. O tipo de solução deve ser escolhido com base no problema. Existem
duas métricas para avaliar uma ER. A medida Precision é dada pela divisão do
número de cadeias selecionadas corretamente pelo número total de cadeias
selecionadas, é uma taxa de acerto. Já a medida Recall é dada pela divisão
entre número de cadeias selecionadas corretamente pelo total de cadeias
corretas (incluindo os falsos negativos). Uma ER também pode ser expressada
como um autômato finito determinista.
Todas os caracteres especiais que desempenham uma função podem ser encontrados
num arquivo no tidia.
A primeira questão do teste pedia para selecionar todas as cadeias alfabéticas.
Isso pode ser feiro com: [a­-zA-­Z]+ onde ocorre uma ou mais ocorrências de
uma letra maiúscula ou minúscula.  Na segunda questão era necessário Selecionar
todas as cadeias alfabéticas (em minúscula) que terminam com a letra m, para
isso: [a­-z]*m\b, note que a ausência do \b implica em pegar subcadeias que
terminam com m, por exemplo academ seria selecionado de academia, mesmo não
terminando a palavra. A terceira questão pedia para selecionar todas as cadeias
que começam no início da linha com um número inteiro e que terminam no final da
linha com uma palavra. A resposta é: ^\d+\b.*\b\w+$.  Na última questão foi
introduzido o \1, que repete a cadeia especificada anteriormente, servindo
como memória. \b(\w+)\W+\1 é capaz de identificar a ocorrência de palavras
repetidas (menos menos), pois \b faz com que a palavra não seja subpalavra de
uma outra palavra (word boundary), (\w+) é uma cadeia alfanumérica de qualquer
tamanho, \W um espaço ou separação e \1 serve para indicar a segunda ocorrência
da palavra especificada anteriormente.


Devido à complexidade dos alfabetos, criou-se um método formal para retirar
trechos de textos chamado expressões regulares (Ers ou Regex). ERs, portanto,
são concatenações de caracteres em uma linguagem formal que tem como objetivo
buscar e extrair padrões em um texto.
Supondo que exista um texto sobre capivaras e nós queremos saber quantas vezes
o termo capivara é citado. Se simplesmente procurarmos pelo termo "capivara"
nós estaremos negligenciando a palavra "Capivara" com "C" maiúsculo. Em
expressões regulares, ao em vez de procurar pelos dois termos, simplesmente
buscamo por: \[cC]apivara\. Este método é uma expressão regular e [cC] é
chamado disjunção. Através de expressões regulares ainda podemos procurar por
outras variações da palavra capivara, como por exemplo "capivaras".
Durante a aula, nos foi dado uma tabela com diversos comando utilizados em
expressões regulares para usarmos como referência durante a aula.
Algumas palavras possuem um grande número de flexões e, portanto, seu trabalho
de busca pode ocasionar erros. Um erro possível é identificar cadeias de letras
que não deviam ter sido identificadas, erro chamado de falso positivo. Ou
ainda, deixar de identificar cadeias de letras que deveriam ter sido
identificadas, o chamado falso negativo. É possível avaliar a qualidade de uma
busca através de duas medidas:
Precisão: verdadeiro positivo/(verdadeiro positivo + falso positivo). Aumentar
a precisão significa minimizar os falsos positivos.
Recall: verdadeiro positivo/(verdadeiro positivo + falso negativo). Aumentar o
recall significa diminuir o número de falsos negativos aumentando a cobertura.
Expressões regulares nada mais são que um tipo de máquina de estados finita, ou
ainda, autômato finito determinístico. Toda expressão regular pode ser
desenhada como um autômato, esta é uma forma de avaliar se a ER está bem
construída.
Após a exposição do tema, nos foi dirigido um teste com cinco perguntas para
serem respondidas utilizando expressões regulares. Duas questões merecem
destaque:
1) Selecionar todas as palavras terminadas em m: um erro comum foi utilizar a
ER \ [a-z]*m \ que retorna também subpalavras como amor. O correto seria \ [a-
z]*m\b \
2) Selecionar cadeias de palavras com duas repetições)
A resposta para esta questão consistia em uma técnica que ainda não havíamos
aprendido. basicamente existe o operador () que é capaz de "memorizar" o termo
escrito nele. Para capturar este termo novamente basta utilizar o comando \1.
Portanto a resposta é: \b(\w+)\W+\1
No final da aula o professor nos mostrou um chatbox simples que foi um dos
primeiros sistemas de PLN chamado Eliza. Ele foi programado para manter as
conversas limitadas. Em resumo este programa procura padrões textuais na
conversa e a resposta de Eliza seria apenas uma substituição dos padrões,
criando a ilusão de entendimento.
 


Aula 2 - Expressões Regulares
Expressões regulares são formas de expressar cadeias textuais alfanuméricas que
podem ser criadas a partir de autômatos finitos determinísticos. São comumente
utilizadas pra normaliza?ão de texto, divisão em tokens, radicaliza?ão e
segmenta?ão de frases. Também são referenciadas por Regex ou Regexp.
Diretivas:
Colchetes ([]) -> Disjun?ão de caracteres para busca, também usado com hífens
para determinar intervalos
[cC]apivara, [0-9]
Circunflexo em disjun?ões (^) -> Nega?ão em disjun?ões.
[^A-Z], [^Ss]
Barra vertical (|) -> Alternativa.
[cC](apivara|arpincho), eu|voce
Interroga?ão (?) -> Caractere predecessor opcional
pa?ra
Asterisco (*) -> Caractere predecessor 0 ou mais vezes (fecho de Klein)
aa*h!
Cruz de soma (+) -> Caractere predecessor 1 ou mais vezes
o+h!
Ponto (.) -> Qualquer caractere
ca.a
Circunflexo (^) -> Início de linha
^[A-Z]
Cifra ($) -> Final de linha
\.$ (\ indica caractere não especial)
Dois tipos de erro:
Falso positivo, onde se identificam cadeias que não deveriam ser identificadas
Falso negativo, onde não se identificam cadeias que deveriam ser identificadas
Minimizar falsos positivos: aumentar precisão
Minimizar falsos negativos: aumentar cobertura
Geralmente são o primeiro passo de qualquer processamento de texto.
O resultado de ERs é utilizado em classificarores de técnicas de aprendizado de
máquina.
É possível verificar a qualidade da constru?ão de uma ER através da montagem do
AFD correspondente à expressão regular.
Existem, além dos relatados, alguns outros operadores que abreviam ERs comuns,
como \w, que se refere a qualquer caractere alfanumérico e underline, comum
para e-mails, por exemplo.
Na busca das cadeias é utilizada estratégia gulosa, devido à natureza dos AFDs.
Eliza, um chatterbot feito com ERs, simula uma conversa com o interlocutor.
Hoje em dia, inúmeros BOTs existem para atendimento a clientes.
ERs também são usadas para identificar pessoas importantes em artigos
científicos.


Expressões regulares (RE) são cadeias de texto especiais, em uma linguagem
formal, para busca/extração de trechos de texto. RE são usadas para
normalização de texto, divisão em tokens, radicalização e segmentação de
frases. Podem ser representadas por um autômato finito determinístico (DFA).
São compostas por tokens, que podem ser consultados em <https://regex101.com>.
Este website também pode ser utilizado para praticar RE. A bibliografia
recomendada para este tópico é Speech and language processing dos autores
Daniel Jurafsky & James H. Martin. O teste de avaliação foi composto por 6
questões para praticar o básico de RE. Observou-se que o padrão em RE é usar
estratégias gulosas e que uma RE representa mais do que um DFA consegue, como
exemplo, RE possui memória. Quando construindo uma RE, deve se tomar cuidado
com alguns tipos erros de classificação. Falso positivos, quando identifica-se
cadeias que não deveriam ser identificadas. Falso negativo, quando não
identifica-se cadeias que deveriam ser identificadas. Um exemplo prático de RE
é o chatbot Eliza, um dos primeiros sistemas NLP. Eliza procura imitar
respostas dos usuários através de padrões textuais, mantendo uma conversa
limitada e superficial. Atualmente existem chatbot mais sofisticados.
 


Aula sobre expressões regulares, incluindo suas funções e a forma de
reconhecimento de determinados padrões de caracteres relacionados com a escrita
de textos. Dentre outras funções, expressões regulares são utilizadas para
extrair trechos a partir de um texto. Normalmente, são o primeiro passo para
qualquer processamento de texto e permitem melhorias em tecnologias de
aprendizado de máquina. A definição formal de expressões regulares refere-se a
cadeias de texto especiais, para identificação de cadeias de texto, e elas
também são denominadas Regex e Regexp.
Para extração de trechos padrões em textos, é necessário regularizar a
identificação de sinônimos e palavras que podem ser escritas de várias
maneiras. Alguns dos símbolos utilizados para esse tipo de processamento serão
descritos na sequência:
- Colchetes (exemplo: [cC]) - especificam uma disjunção de caracteres,
na busca. Dentre os caracteres dentro dos colchetes, qualquer um é aceito como
opção válida. Permite a implementação de faixas de caracteres (exemplo: [0-9]
- Circunflexo dentro dos colchetes (exemplo: [^a-z]) - negação dos caracteres.
-  Barra vertical [exemplo: nunca|jamais) - símbolo de OU.
- Interrogação - caractere predecessor opcional.
- Asterisco - caractere predecessor zero ou mais vezes.
- Mais (+) - caractere predecessor uma ou mais vezes.
- Ponto - Qualquer caractere, exceto vazio.
- Barra (\) - caracter sucessor não é especial.
- Circunflexo fora dos colchetes - âncora de início de linha.
- Cifrão fora dos colchetes - âncora de final de linha.
Informativo sobre o uso do site regex101, que permite a testagem de expressões
regulares, conforme esse modelo de processamento. Há várias maneiras de fazer a
busca da expressão regular, sendo algumas mais precisas do que as outras. É
muito comum a ocorrência de erros, que podem ser de dois tipos: falsos
positivos (identificação de cadeias irregulares) e falsos negativos (não
identificação de cadeias corretas, que incluem a expressão regular). Na busca
de minimizar os erros, é necessário aumentar a precisão, no caso dos falsos
positivos, e aumentar a cobertura, no caso dos falsos negativos.
Foi realizado um pequeno teste (sem atribuição de nota) para demonstrar o uso
do conteúdo aprendido sobre expressões regulares. Na avaliação, exigiu-se a
combinação dos símbolos estudados em sala de aula. Além disso, um dos
exercícios exigia símbolos ainda não estudados, para dar um direcionamento para
a continuidade do estudo.


Na aula do dia 22/02, aprendemos Expressões Regulares, de forma teórica e
depois prática.
De início, fomos apresentados às motivações e a importância das Expressões
Regulares, que são formas de fazer uma busca bem precisa em algum texto,
extraindo trechos. E depois, seguimos para suas classificações: As disjunções
são representadas entre colchetes, que dá como resultado apenas um caractere.
Exemplo: [aA]ula: busca tanto "aula" quanto "Aula", ou com números: [3-7], que
busca os algarismos de 3 a 7. Também vimos como negar uma disjunção: [^a-z], dá
como resultado qualquer caractere que não seja do alfabeto minúsculo. Vimos
também as funções dos caracteres ?, *, +, .. O ? é usado depois de um caractere
que pode ser opcional no texto. exemplo: tex?to, dá como resultado tanto
"texto" quanto teto. O caractere * indica que o caractere antecessor pode
aparecer 0 ou mais vezes. Exemplo: pod*e, dá como busca "poe", "pode", podde",
"poddde" etc. Semelhante ao *, quando usamos o caractere +, significa que
o caractere antecessor pode aparecer 1 ou mais vezes. Por fim, o "." é usado
para substituir qualquer caractere. Exemplo: ca.a dá como resultado "cama",
"capa", "casa" etc.
Caracteres âncoras ^ e $: São usados para fixar que o trecho buscado deve
aparecer no começo e/ou no final de uma linha, respectivamente.  Em seguida,
vimos em aula várias aplicações e exemplos das Expressões Regulares aprendidas.
Sobre o teste dado em sala: consistia em 4 testes, onde precisávamos escrever
as Expressões regulares que estavam sendo pedidas e exemplificadas. 


 
Uma expressão regular é uma sequencia de caracteres que define um padrão de
busca em texto e também pode ser vista como uma forma algébrica que descreve um
conjunto de strings, é comumente usada em diversas tarefas de processamento de
texto e busca de informação.
Dado um padrão e um corpus de texto, uma busca de expressão regular vai buscar
em todo corpus e retornar todas as linhas, documentos, etc  quais  possuam
ocorrências que corresponda ao padrão dado ou simplesmente retornar o padrão
encontrado.
Colchetes são usados para descrever uma disjunção de caracteres a ser
combinado.
\[Dd]igito combina com "digito" e "Digito"
Contadores, descrevem quantas vezes o padrão ou caractere precedente deve
ocorrer.
"\*"   zero ou mais vezes.
"+" uma ou mais vezes.
"?"  0 ou 1 vezes.
"{n}" exatamente n vezes.
"{n,m}" entre n e m vezes.
"{n,}" n ou mais vezes.
Ancoras:
Ancoras são operadores que acorram uma expressão regular em uma precisa
localização no texto.
^  combina com uma expressão regular apenas se ela ocorre no começo de uma
linha.
$ combina com uma expressão regular apenas se ela ocorre no final de uma
linha.
Operador de disjunção "|":
"|" é o operador que descreve a disjunção entre duas expressões regulares.
"\[a]{2}|ok" combina tanto com  "aa" como com "ok"
Hierarquia de precedência de operadores:
Parêntesis                   ()
Contadores                   + \* ? {}
Sequencias e ancoras         cenoura   $ok    ^linha
Operador de Disjunção        |
Padrões podem ser ambíguos de diversas formais, por exemplo, o padrão
"[a-zA-Z]" contra o texto bolo de cenoura combina tanto com b como com
bo, bol, bolo, por padrão o os motores de expressões regulares combinam
com a maior sequencia possível.
Memória:
É possível fazer referencia a padrões encontrados durante a busca, essa
funcionalidade é chamada de memória e é considerada uma extensão das expressões
regulares e pode não estar presente em todos os motores, para usar é necessário
colocar parênteses em uma expressão e referenciar a n-ésima expressão que
tenha parênteses por \n, exemplo:
(\d)a\1 combina com  2a2 e 3a3 mas não com 1a2
                       
 


A aula de hoje foi sobre expressões regulares. O professor iniciou falando um
pouco sobre o livro, base que atualmente esta na segunda edicao e indo para a
terceiro, e seus autores.
Expressão regular é uma técnica utilizada para realizar busca e extração de
trechos em um texto. Com ela conseguimos normalizar o texto; realizar divisao
de tokens; reduzir palavras ao seu radical e segmentar frases. O resultado das
expressões regulares podem ser utilizados como atributos de um classificador de
aprendizado de máquina.
Entre as operações possíveis, em aula vimos a disjunção que pode ser chamada
com colchetes ou [ ] ou barra vertical |. Por exemplo, [ 0-9 ] procura palavras
que contenham ao menos um dígito entre 0 e 9. [ a-zA-Z ] procura palavras que
contenham um caractere maiusculo ou minusculo. Com o acento circunflexo
realizamos a operação de negação ^. Analisamos tambem operações com ? * + . $ ^
(em um contexto que indica inicio da linha) e como elas interagem em conjunto.
A expressão regular [0-9][0-9]* procura por palavras que contenham pelo menos
um digito. A ordem de precedencia entre os operadores é: parenteses,
contadores, ancoras para sequencias e por fim disjunções. O professor utilizou
o site https://regex101.com/ para mostrar como as expressões regulares
funcionavam na prática. 
Falamos sobre dois tipo de erro, os falsos positivos (identificar cadeias que
nao deveriam ser identificadas) e falsos negativos (nao identificar cadeias que
deverias ser identificadas). Aumentando a precisão diminuimos os falsos
positivos e aumentando a cobertura minimizamos os falsos negativos.
Toda expressão regular pode ser transcrita como um automato finito
deterministico.
Houve um teste de avaliação em aula, deveriamos escrever as expressoes
regulares para os problemas apresentados.


Introdução
    Na aula foi explicado o conceito de expressões regulares, mostrando algumas
opções de descrição de seleção de texto, uma demonstração de uma breve
aplicação, algumas ferramentas para ajudar no aprendizado e um teste.

 
Conceitos
 
    Expressões regulares ou ER é utilizada para a normalização de texto,
divisão em tokens, radicalização e segmentação de uma frase, para isso ela
utiliza cadeia de texto especiais em uma linguagem formal. Um exemplo para
utilização é a representação da palavra Theo tanto maiúsculo( Theo ) como
minúsculo ( theo ), no caso utilizamos [ Tt ]heo. Porém para representar e
selecionar textos essa ferramenta tem mais alguns atributos que serão descritos
abaixo.
    Faixas de caracteres [-] : Tem como finalidade representar vários
caracteres em um único espaço, [a-z] representa a, b, c,  ,  z, já [A-
Z1-9] representa A, B,  , Z, 1 , 2, , 9.
    Disjunções [^]: É o contrário do que foi representado, [^Dd] é tudo menos
D e d.
    Ou |: Quando queremos mostrar que pode ser mais de uma expressão regular,
ER | RE no seria selecionado tanto ER e RE.
    Caractere [?]: caractere antecessor opcional, Th?eo seleciona tanto Theo
como Teo.
    Caractere [*]: caractere antecessor selecionado de 0 ou mais vezes, UF*ABC
representa UABC, UFABC, UFFABC, UFFFABC, UFFFFABC, etc.
    Caractere [+]: Caractere antecessor selecionado de 1 ou mais vezes, UF+ABC
representa, UFABC, UFFABC, UFFFABC, UFFFFABC, etc.
    Caractere [.]: Representa qualquer coisa do meio, UF.ABC, representa
UFAABC, UF!ABC, UFGABC, UFIABC, etc.
    Caractere [^]: Representa que a palavra vem no início da linha.
    Caractere [$]: Representa que a palavra vem no final da linha.
    Precedência (): Serve para mostrar precedência e guardar memória.
    Memória /número: Representa a memória salva na precedência.
 
Ferramenta
    A ferramenta mostrada foi o sitehttps://regex101.com/, onde é possível
colocar um texto e uma expressão regular, assim ele seleciona e mostra os
trechos de texto de maneira equivalente a ER.
 
Aplicação
    Uma aplicação são os chats bots como o Eliza, onde essa tem como finalidade
conversar com o usuário. Uma outra aplicação é em pesquisa científica, onde as
pessoas mineram texto através de expressões regulares.
 
Teste
    O teste foi constituído de 6 exercícios onde queria treinar e avaliar nosso
conhecimento sobre ER, onde os próprios alunos que atribuiam a sua própria
nota.   
 


Expressão regular (ou regex) é uma ferramenta computacional utilizada para
normalização de textos, divisão de texto em tokens, radicalização e segmentação
de frases. Uma regex consiste em uma cadeia de texto (string), estruturada em
uma linguagem formal própria, que é utilizada para buscar trechos de texto que
"combinam" com as regras definidas por ela.
Uma disjusção de um caractere dentro da expressão regular é definida dentro de
colchetes. Por exemplo, se necessário buscar por qualquer caractere numérico no
texto, podemos usar a expressão [0123456789], mas existem formas de
simplificação. 
Se a sequência de caracteres que deve ser compreendida pela busca for contígua
na tabela ASCII, basta separarmos o primeiro caractere do último por traço. No
exemplo anterior, teríamos [0-9].
O acento circunflexo é utilizado para negar a disjunção. Ou seja, a busca se
torna por qualquer caractere que não sejam os definidos entre colchetes. Para
buscar todos os caracteres não minúsculos, incluindo números e caracteres
especiais, pode-se usar a expressão [^a-z].
O pipe é utilizado como operador OU lógico. Por exemplo, se for preciso buscar
as palavras "eu" ou "voce", pode-se utilizar a expressão eu | voce.
Outras opções de operadores são o ponto de interrogação (?), o asterisco (*), o
sinal de adição (+) e o ponto final (.). Eles definem na busca,
respectivamente, um caractere predecessor opcional, um caractere predecessor
que ocorra zero ou mais vezes, um caractere predecessor que ocorra pelo menos
uma vez e a ocorrência de qualquer caractere.
O acento circunflexo, quando utilizado fora do contexto da disjunção de um
caractere, serve para delimitar o início de uma linha, enquanto o cifrão ($)
define o fim. Ambos são chamados de caracteres âncoras. Se fosse preciso buscar
linhas compostas por apenas uma letra maiúsculas no texto, poderia-se utilizar
a expressão ^[A-Z]$, por exemplo.
Após a definição destes elementos de sintaxe das expressões regulares, exemplos
foram apresentados e os dois tipos de erros possíveis foram definidos. São
eles: falsos positivos (quando uma string é identificada pela regex no texto,
mas não deveria) e falsos negativos (quando uma string não é identificada pela
regex no texto, mas deveria). Minimizar estes erros aumentam a precisão e a
cobertura da busca, respectivamente.
O teste aplicado nesta aula consistia em criar expressões regulares que
buscassem por padrões específicos, como todas as cadeias alfabéticas que fossem
terminadas com a letra "m", ou linhas que fossem iniciadas por números inteiros
e terminadas por uma palavra. O destaque foi a última questão pois envolvia o
conceito de armazenamento na memória e referencia a um padrão encontrado
dinamicamente pela regex. Isto é feito utilizando parênteses e a instrução \1.
As expressões regulares são exemplos de Autômatos Finitos Determinísticos.
Representá-las como tal pode ajudar a definir se ela está bem escrita. Por
padrão, as os motores de expressões regulares utilizam estratégias gulosas em
suas execuções.
A aula foi concluída com a demonstração de um dos primeiros sistemas de
Processamento de Linguagem Natural, chamado de Eliza. Ele consistia em um
programa que buscava e imitava alguns padrões de texto inseridos pelo usuário
para formular uma resposta simples.


Uma expressão regular (regex) serve para extrair trechos de texto, normalizar,
dividir em tokens, radicalizar, segmentar frases. Uma regex é formada a partir
de uma linguagem formal.

Alguns símbolos servem para formar regras e não são interpretados literalmente.
Os colchetes '[' e ']' especificam uma disjunção (ou exclusivo), exemplo [aB]
significa 'a' minúsculo ou 'B' maiúsculo. O acento circunflexo dentro dos
colchetes especifica a negação, exemplo [^aB] significa qualquer caractere
menos 'a' minúsculo ou 'B' maiúsculo. A barra vertical | também significa
disjunção. O ponto de interrogação '?' significa que o caractere anterior (ou
disjunção) pode aparecer zero ou uma vez, ou seja, é opcional. O asterisco '*'
significa que o caractere anterior (ou disjunção) pode aparecer zero ou mais
vezes, ou seja, pode ser repetido. O sinal de adição '+' é similar mas o
caracter deve aparecer pelo menos uma vez. A quantidade exata de caracteres a
ser identifica pode ser especificada entre chaves, exemplo: {n} para n
ocorrências, {n,m} para quantidade entre n e m, {n,} para no mínimo n
ocorrências. O ponto '.' significa qualquer caractere, ou seja, funciona como
um coringa. O acento circunflexo fora dos colchetes significa que o caractere
seguinte (ou disjunção) deve ser inicial na linha, e o cifrão '$' significa que
o caractere anterior deve finalizar a linha. Para que esses caracteres
especiais sejam interpretados normalmente devem ser precedidos por uma barra
'\'.

Podem haver dois tipos de erros: a não identificação de cadeias corretas
(falsos negativos) e a identificação de cadeias incorretas (falsos positivos).
Esses erros são prevenidos aumentando-se a cobertura ou a precisão,
respectivamente.

Uma regex é equivalente a uma máquina de estados finita ou a um autômato finito
determinístico pois gera linguagens regulares.
 


Assim como autômatos finitos são usados ??para reconhecer padrões de strings,
expressões regulares são usadas para gerar padrões de strings. Uma expressão
regular é uma fórmula algébrica cujo valor é um padrão que consiste em um
conjunto de strings, chamado de linguagem da expressão.
Operadores de uma expressão regular podem ser caracteres do alfabeto sobre os
quais a expressão regular é definida, variáveis ??cujos valores são qualquer
padrão definido por uma expressão regular, epsilon que denota a string vazia
sem caracteres, nulo que denota o conjunto vazio de Strings.
Expressões regulares fornecem um idioma expressivo para descrever padrões. Eles
são particularmente úteis porque, como os padrões de Strings simples, eles
podem ser procurados de forma eficiente em uma String maior. Acontece que, para
qualquer expressão regular, pode ser construído um autômato finito
determinístico (DFA) que reconhece qualquer string que a expressão regular
descreva no tempo linear no comprimento da String! Um DFA, também conhecido
como uma máquina de estados finitos, é um grafo finito em que os vértices são
os estados do autômato. As arestas do gráfico são rotuladas com caracteres e
existe um estado de início distinto e alguns estados de aceitação. Em um
autômato finito determinístico, há no máximo uma aresta de saída de qualquer
dado nó marcado com um carácter particular. O autômato inicia no estado inicial
e varre o texto da esquerda para a direita. Para cada caractere de entrada,
segue um único vértice de saída do estado atual rotulado com esse caractere.
Por padrão, um quantificador diz ao motor que combine todas as instâncias da
sua expressão quanto possível. Esse comportamento é chamado guloso.
Por exemplo, pegue o quantificador +. Permite que o motor combine uma ou mais
expressões: \d+ pode, portanto, combinar um ou mais dígitos. Por padrão, os
quantificadores são gulosos,  \d+ combina o máximo de dígitos possível.


Expressões regulares (ER) são geralmente utilizadas em buscas textuais. Alguns
sinônimos utilizados são regex e regexp. É uma técnica quase inteligente.
Comumente usada para: normalização de texto, divisão em tokens, radicalização
(e.g., lemmatization, stemming) e segmentação  de frases.
Uma definição mais formal para ER: cadeias de caracteres especiais, em uma
linguagem forma, para busca/extração de trechos em um texto.
Algumas ER:
1 - Disjunção : cadeia de caracteres entre colchetes [ ] irá identificar o
primeira ocorrência de qualquer caractere entre os colchetes. Semelhante ao
operador "ou" da lógica booleana.
Ex.: [Cc]apivara irá procurar por capivara ou Capivara.
Para representar uma faixa de caracter utiliza-se o hífen (sempre representado
em ordem crescente).
Ex. [0-9] busca a primeira ocorrência que contenha qualquer digito entre 0 e 9.
2- Negação em disjunção. ^
Nega a disjunção, sendo válida para o primeiro caractere.
Ex.: [^A-A] Ciência da computação.
identificara o caractere i.
3 - Outra disjunção |
outra forma de disjunção é o uso da barra vertical |.
Ex.: eu|voce identificará "eu " ou "voce"
4 - Para uso em caracteres predecessores ? * + .
? caractere predecessor opcional
* caractere predecessor: 0 ou mais vezes
+ caractere predecessor:1 ou mais vezes
. caractere coringa
5 - Caracteres âncoras ^$
^inicio de linha
$ final de linha
6 - Caractere de escape \
indica que o caractere seguinte não és especial
Os tipos de erros comuns no uso das ER.
Falso positivo, identificar cadeias que não deveria ser identificado.
Falso negativo, não identificar cadeias que deveriam ser identificadas.
Formas de minimizar os Falsos Positivos: aumentando a precisão.
Forma de minimizar os falsos negativos: aumentando a cobertura.
Geralmente as ERs são o primeiro passo para qualquer técnica que
envolva processamento de texto.
ER pode ser representado por uma máquina de estados finitos.
ELIZA foi um dos primeiros programas de PLN. Desenvolvido pelo MIT entre os
anos de 1964 e 1966.
Era uma espécie de chatbot bem limitado e trabalha realizando buscas textuais
na interação do usuário com o computador.


Expressões regulares oferecem um método poderoso, flexível e eficiente de
processamento de texto. A extensiva notação de correspondência de padrões de
regex, como também são conhecidas, permite que você analise grandes quantidades
de texto para encontrar padrões de caracteres específicos; para validar o texto
e garantir que ele corresponda a um padrão predefinido (como um número de cpf);
para extrair, editar, substituir ou excluir subcadeias de caracteres de texto.
Também utilizadas para normalização de textos, que seria a sua padronização de
acordo com normas e regras convenientes; tokenização, divisão das palavras
separadas por espaços; radicalização, encontrar a raiz das palavras;
segmentação de frases, divisão em frases usando a pontuação.
Para fazer uso de regex é necessário entender como sua linguagem funciona, para
formar as expressões responsáveis pela busca dos padrões em textos são usados
caracteres que representam funções específicas, como por exemplo os colchetes,
que representação disjunção entre caracteres, ou a barra vertical (pipe) que
representa a disjunção de caracteres, palavras ou outras expressões. Há também
símbolos âncoras, coringas e outros que assumem o papel de expressões muito
usadas a fim de facilitar a leitura do código.
Um cuidado a ser tomado são com os erros, que entre os frequentes estão: falso
positivo: Identificar resultados que não deveriam ser identificados; falso
negativo: não encontrar trechos que deveriam ser identificados.
Em PLN, e qualquer outro problema de reconhecimento de padrões, devemos lidar
com esses 2 tipos de erros.
Minimizar os Falsos Positivos: Aumentando a precisão.
Minimizar os Falsos Negativos: Aumentando a cobertura.
    Na avaliação realizada em sala um problema chamou a atenção: selecionar
todas as cadeias com duas palavras repetidas consecutivas, será possível  uma
expressão regular ter memória? que no caso seria necessária para lembrar a
cadeia encontrada para comparar com a seguinte. A solução compartilhada pelo
professor foi a do uso combinado dos parênteses com \1 que indica a expressão
contida num parênteses, referenciando o mesmo pela ordem em que aparece na
expressão da esquerda para a direita.
 
 
 


O conceito de Expressão Regular visto em aula é como uma String que é usada,
para descrever um conjunto das mesmas utilizando-se de certas regras
sintáticas. Suas aplicações são principalmente no desenvolvimento de Logicial e
pode ser implementada em diversas linguagens de programação, um exemplo é a
funcionalidade Localizar e Substituir que muitos editores de texto já comportam
nativamente. As expressões regulares podem ser usadas como critérios de filtro
na busca de texto, combinando o texto com o padrão de expressão regular.  Esse
processo também é chamado de correspondência de padrões .  Por exemplo, é
possível encontrar todas as palavras em uma lista de palavras que começam com S
e terminam em D , sem ter que especificar explicitamente as letras ou o número
deles.O termo expressão regular é essencialmente baseado no matemático Stephen
Kleene .  Ele usou um nome quase idêntico, que ele chamou de quantidades
regulares. As bases teóricas podem ser vistas em Linguagens formais e autômato
onde podemos nos utilizar e sintaxe e semântica. Como pode ser implementada em
diversas linguagens, há muitos exemplos onde o recurso é utilizado como por
exemplo no filtro grep em sistemas Linux ou até mesmo em um dos mais primitivos
sistemas de AI com interação básica chamado Eliza. No contexto de PLN elas são
muitos importantes pois podem desempenhar grande papel como por exemplo na
Bioinformática.  Elas são usadas ??em bancos de dados de proteína para
descrever motivos protéicos.  A expressão regular Wx(9,11)-[VFY]-[FYW]-x(6,7)-
[GSTNE]-[GSTQCR]-[FYW]-RSAP por exemplo pode ser utilizada para descrever um
domínio de proteína no banco PROSITE.
O teste 2 aplicado na aula consistia em 6 questões das quais 2 estavam
respondidas e era necessário implementar Expressões Regulares para filtrar
determinadas semânticas pedidas nos enunciados, apesar de aparentemente não ser
complexo, dependendo da experiência algo que pode ser feito de forma simples
torna-se muito grande. Embora estivesse confiante não consegui acertar tudo
mas, o aprendizado foi importante.


Expressões regulares podem ser usadas para identificar palavras específicas,
caracteres e cadeias de strings, ou até validar e identificar padrões, tudo
isso de uma forma flexível e com certa precisão, por causa da acentuação
presente na língua portuguesa, textos analisados devem ser convertidos sem seus
acentos, ou podem sofrer na identificação das letras com acentos, porque os
textos são analisados usando a tabela ASCII (American Standard Code for
Information Interchange) e nesta tabela originalmente não existiam vogais com
acentos por quase não serem usados na língua inglesa (foram introduzidas
posteriormente em uma tabela estendida), e, atalhos e partes de códigos
normalmente usados nas expressões regulares não abrangem tais letras.
Como todo sistema de reconhecimento de padrões criar expressões regulares está
vulnerável a erros como falso positivos ou falso negativos, mas ambos podem ser
corrigidos aumentando sua precisão (considerando apenas o que realmente importa
do item) e sua cobertura (considerando quantos itens relevantes são
detectados), respectivamente. Um exemplo de expressão regular para validação de
e-mail (vulnerável a erros como no slide da aula):
/ [\w\.\+-]+ @ [\w\.-]+ \. [a-zA-Z]{2,4} /
Teste de Avaliação em Aula
1. Apenas para exemplificar uma expressão regular
[a-zA-Z]+
2. Um pequeno teste de atenção aos limites de quando se procura por uma palavra
com um padrão específico em seu final
[a-Z]*m\b
3.Uma expressão regular um pouco mais "elaborada" e como o uso de atalhos podem
diminuir a linha de código
^[0-9]+\b.*\b[a-zA-Z]+$
^  \d   +\b.*\b    \w     +$
4. exemplo de como se utiliza alternância( ... | ... ) e agrupamento ( ( ... )
)
(gato)|(cachorro)
5. Uma expressão regular com alternância e agrupamento e com quantificação
usando ( {n} )
(gato\s){2}|(cachorro)
6. Uma "pegadinha" onde para ser resolvida exigia um pequeno detalhe, o uso de
"memória" presente no agrupamento ( ( ... ) \1 )
\b(\w+)\W+ \1
Expressões regulares podem ser testadas no site regex101.com


Expressões Regulares
1. O que são Expressões Regulares (Regex ou Regexp)
Expressão Regular é uma representação, ou seja, um conjunto de símbolos, que
fornecem uma maneira de identificar e extrair padrões em um texto. Cada símbolo
retrata algum tipo de informação.
2. Representações de Expressões Regulares
    * Os [ ] colchetes reconhecem qualquer caractere listadado dentro deles
    * [...-...] reconhece cada caractere listado no alcance
    * [^...] reconhece cada caracter não listado
    * [^...-...] reconhece cada caracter não listado no alcance
    * (...) agrupamento por precedência
    * * zero ou mais do antencessor
    * + um ou mais do antecessor
    * ? exatamente um ou zero do antecessor
    * \d qualquer digito de 0 à 9
    * \w qualquer digito alfanumerico/underline
    * \s espaços em branco (espaço, tab) 
    * ...|... reconhece qualquer um dos vizinhos, por exemplo (...) ou (...)
    * Veja mais sobre os simbolos que representam as expressões regulares aqui:
       https://tidia4.ufabc.edu.br/access/content/group/ff5ded65-3d10-4c36-
      af0e-33f9a65fdae8/regular-expression-syntax.pdf
3. Dois tipos de erros
Durante a busca por padrões é possível encontrar dois erros possíveis. São
eles:
   1. Falso Positivo: identificar padrões que não deveriam ser identificados.
      Para lidar com esses erros podemos aumentar a precisão. 
   2. Falso Negativo: não identificar padrões que deveriam ser identificados.
      Para lidar com esses erros podemos aumentar a cobertura.
4. Entendendo as Expressões Regulares e suas utilidades
As expressões regulares são o primeiro o passo para um processamento de um
texto e podem ser descritas como uma máquina de estado finita (MEF). As
espressões regulares são utilizadas por:
    * editores de texto
    * linguagens de programação (extrair texto baseado em padões)
    * validação (validar um e-mail, por exemplo)
    * chatbots (encontra padrões no texto e define uma resposta beaseada nesses
      modelos) 
    * Aprendizado de Máquina
    * Etc.
5. Bibliografia
http://professor.ufabc.edu.br/~jesus.mena/courses/pln-1q-2018/PLN-aula02.pdf
https://tidia4.ufabc.edu.br/access/content/group/ff5ded65-3d10-4c36-af0e-
33f9a65fdae8/regular-expression-syntax.pdf
https://tableless.com.br/o-basico-sobre-expressoes-regulares/
https://pt.wikipedia.org/wiki/Express%C3%A3o_regular#Uso
 


A segunda aula da disciplina introduziu o conceito de expressões regulares. As
ERs - Expressões Regulares, desenvolvidas pelo matemático Stephen Kleene -, são
cadeias de caracteres alfanuméricos utilizadas para procurar padrões textuais.
Podemos especificar o que será buscado em um texto através do uso desses
operadores.
 A busca por palavras mais simples pode ser feita utilizando o operador /. Por
exemplo, se quisermos encontrar a palavra "capivara" em um texto, podemos
utilizar a seguinte expressão: /capivara/. É importante enfatizar que essa
busca diferencia letras maiúsculas e minúsculas, portanto para que a palavra
"capivara" seja encontrada podemos utilizar a seguinte expressão /[cC]apivara/.
Sempre que colocarmos um ou mais caracteres entre colchetes, estaremos
indicando uma disjunção, no exemplo será buscada a palavra com c maiúsculo ou
minúsculo. Também podemos indicar uma faixa de caracteres entre os colchetes,
por exemplo, a seguinte expressão [A-Z] indica que um caractere maiúsculo de A
à Z deve ser buscado. Além disso, caso queiramos negar essas disjunções podemos
utilizar o operador circunflexo "^" logo no início da expressão, assim [^Aa]
significa nem 'A' ou 'a'. Além da disjunção apresentada, também podemos
utilizar a barra vertical entre duas expressões, por exemplo, podemos buscar
[mM]andioca | [mM]acaxeira, e os resultados serão: mandioca, Mandioca,
macaxeira e Macaxeira. Além dessas expressões, outras foram apresentadas como
os caracteres âncoras, que indicam o inicio "^" e o fim "$" de uma linha. A
estratégia das ERs são denotadas como gulosas pois dado uma busca, ela irá
parar somente quando chegar ao final da frase, sendo necessário modificá-la com
através dos operadores, caso isso não seja parte do objetivo. Para aprofundar
os conhecimentos desses operadores fizemos um teste em sala no qual deveríamos
criar as expressões regulares para encontrar as frases e palavras propostas
pelo professor.
Afim de compreender melhor como as ERs funcionam, podemos analisar a chatterbot
Eliza. Ela foi o primeiro software a realizar o processamento da linguagem
natural para conversar com um humano e utiliza a busca por padrões textuais
para responder o interlocutor.
Apesar de parecer óbvio, precisamos ter cuidado com a escrita das expressões
regulares afim de reduzir erros de falso positivo e falso negativo, para isso
precisamos aumentar a cobertura e a precisão, respectivamente. Portanto o
estudo das ERs, de sua escrita elegante e precisa é essencial.


Para o o estudo de linguagem natural, é importante conhecer as palavras que
estão presentes na língua, sua frequência e a maneira como elas se relacionam a
outras palavras. Esses processos são efetuados a partir do uso de expressões
regulares, sumarização e corpora.
Corpora são conjuntos de textos com anotações sobre sua estrutura - referências
as classes de palavras, por exemplo - que são utilizados para aprendizado e
validação de sistemas de processamento de linguagem natural.
Considerando a contagem de palavras, há duas métricas principais: a quantidade
de tokens e o tamanho do vocabulário. O primeiro refere-se à quantidade total
de palavras no texto, e o segundo à quantidade de palavras distintas.
Analisando sob esse aspecto, uma pergunta surge: há relação entre essas duas
medidas? Empiricamente, foi descoberto um padrão: a quantidade de vocábulos
aproxima-se do produto de uma constante pela quantidade de palavras elevado a
um valor entre zero e um. Tanto a constante quanto o exponencial varia de
acordo com o idioma; estima-se que na língua inglesa a constante varie entre 10
e 100 e o exponencial entre 0.4 e 0.6.
O gráfico dessa equação aproxima-se à de uma função de radiciação, com um
crescimento íngreme nos valores de x próximos a zero e mais suave nos valores
mais distantes. Isso faz sentido: um texto pequeno usa poucas palavras, mas
conforme cresce precisa de muitas palavras para que informações adicionais
sejam transmitidas e a redundância seja evitada. Porém, quando um texto é muito
longo, mais próximo da quantidade de palavras da língua seu vocabulário se
torna e a repetição de palavras torna-se inevitável, levando a quantidade de
palavras diferentes a se tornar estável.
Visto que essas medidas dizem respeito à quantidades de palavras, é válido
questionar a maneira como as palavras são contadas. Usualmente são utilizados
espaços e sinais de pontuação para separar as palavras. Entretanto, em alguns
contextos isso pode não ser adequado, como quando tratamos uma palavra
hifenizada, um endereço web ou textos com estruturas gramaticais mais
complexas. Por essa razão, as expressões regulares devem ser escritas para um
contexto específico.
A extração dos tokens de um texto pode ser utilizada para descobrir a
frequência de cada um de seus vocábulos. Entretanto, uma análise de frequência
mostra que as palavras mais comuns consistem, principalmente, de preposições,
conjunções, artigos e demais palavras que são mais importantes para a
manutenção da estrutura de um texto que seu significado. Considerando isso,
define-se as stopwords, que são palavras que poluem um texto e têm pouco
significado, para que a análise seja mais assertiva. Tal qual as expressões
regulares, a lista de stopwords deve ser feita para um propósito específico. 
A etapa seguinte à extração das palavras de um documento consiste em unificar
palavras de significado e grafia semelhantes  por exemplo, substituir
"outras", "outro" e "outrem" por algo que as represente  com o objetivo de
facilitar seu processamento. Esse processo pode ser feito de duas formas:
através do stemming, que transforma as palavras efetuando cortes (as palavras
citadas anteriormente seriam substituídas por "outr", por exemplo), e através
da lemmatization, através da qual se transforma as palavras na sua forma sem
flexão, encontrada no dicionário (nela as palavras seriam substituídas por
"outro"). O stemming é muito utilizado em algoritmos de busca, enquanto a
lemmatization é eficiente para sistemas de tradução automática.


Grupos: É a sub-expressão de uma expressão regular (er) entre parênteses.
Tokenização lexical: É o processo de separar cada palavra como um token, isto
é, uma unidade, mesmo que junta a pontuações.
Maximum Matching: É um algoritmo guloso de segmentação de palavras, para obter
a maior palavra no dicionário que corresponde aos caracteres que começam a
sentença. Pode funcionar bem em algumas línguas, mas existem modelos
estatísticos mais sofisticados.
Stopwords: A remoção de stopwords consiste em retirar palavras muito
frequentes, como artigos, que não adicionam informações relevantes ao
documento.
Corpus textual: Conjunto de documentos que serve de base para aprendizado
(análise) e validação(verificação).
Normalização de palavras: Como as palavras possuem diferentes formas, como seu
gênero, número e grau, é necessário uma maneira de reduzi-las ou simplificá-las
para uma forma padronizada ou única. Tais técnicas são muito importantes na
extração de informação, sumarização e classificação de textos. Nesses casos, o
conceito utilizado no texto é mais relevante do que o item lexical que vincula
a ideia. Assim, a normalização morfológica pode ser vista como o princípio que
relaciona itens que vinculam o mesmo conceito de significado.
Stemming: Redução de palavras flexionadas ou derivadas, a sua base ou raiz,
geralmente utilizando alguma heurística para cortar as extremidades, removendo
afixos derivacionais ou utilizando um conjunto de regras. Geralmente é
suficiente que as palavras sejam relacionadas no mesmo grupo para possuírem uma
raíz válida. Assim, é possível que um motor de busca utilize palavras com a
mesma origem como sinônimos para realizar uma consulta expandida, combinando
mais termos e realizando assim uma busca mais profunda, aumentando a taxa de
positivos verdadeiros, pois são relacionados ao tema procurado, porém, reduz a
precisão.
Lemmatization: São técnicas utilizadas para reduzir a palavra ao seu lema ou
sua forma de dicionário (canônica), deflexionando as palavras, dependendo da
língua utilizada, no caso do português retira-se a conjugação verbal e adequa-
se os substantivos e adjetivos para a forma singular masculina. Além disso,
lemmatization precisa identificar corretamente o significado da palavra na
sentença, como parte do contexto no documento como um todo ,se tornando um
problema extremamente complexo. Ao traduzir uma palavra de outra língua, é
importante entender a interpretação dada e o contexto, não somente a palavra de
origem, que pode não significar muito. Como por exemplo, em português ao
utilizar o termo "pé-frio" que é sem sorte em português, o termo "cold feet" em
inglês está mais relacionado a uma pessoa medrosa ou perda de confiança.


Iniciamos essa aula resgatando os conceitos da classe passada. Do teste de
avaliação em sala, em específico na sexta questão foi nos proposto elaborar uma
expressão regular que segmente duas palavras repetidas de forma seguida. Para
tanto, observamos a necessidade de um operador de memória que registre e
aplique o valor encontrado anteriormente. Temos que esse armazenamento consiste
em delimitar a expressão com os parênteses () e resgatar o valor com o comando
\i (dependendo do interpretador pode ser convencionado como $i), sendo i um
índice numérico pertencente aos números naturais maiores que 0, e.g., {1, 2, 3,
4}. Indo além, podemos misturar essa memória da expressão regular nos
parênteses de forma recursiva; para tanto, temos somente que nos atentar que
ao contrário da prioridade estabelecida na matemática, aqui indicamos o
primeiro grupo encontrado, ou seja, o grupo mais externo como primeiro. O
índice correspondente aos demais segue a numeração da esquerda para direita. De
forma didática, suponha uma lista de e-mail que queremos realizar uma busca com
a expressão ((\w+)\.(\w+))@\i, se estamos buscando o e-mail
jesus.mena@jesus.mena, logo i deve ser igual à 1, se estamos buscando
jesus.mena@jesus (i = 2); ou se estamos buscando jesus.mena@mena, i deverá ser
3.
Seguindo, convencionamos um intervalo na tabela ASCII que devemos buscar e
considerar para compor uma palavra. Isso ocorre pois ao contrário do inglês
temos acentos que compõem nossa linguagem. Analisando a tabela estendida
chegamos à expressão regular que melhor segmenta as palavras em um texto
corrido para nosso idioma ([-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+).
Com essa ferramenta chegamos, em uma das etapas fundamentais do processamento
de linguagem natural, na extração de palavras (tokenização). Definimos a
quantidade de palavras no texto corrido com tokens, e também estabelecendo um
vocabulário, um conjunto de type, V, que de forma grosseira simboliza quantas
palavras diferentes há no texto. Essas meta-informações acerca do texto
estabelecem métricas que possibilitam extrair padrões relativos aos textos.
Durante muito tempo tentou-se estabelecer uma relação entre a variedade de
palavras e a quantidade de palavras, uma heurística conhecida pertence à Lei de
Herdan (1960) e Lei de Heaps (1978): |V|=kN^?, sendo N o número de tokens, k
uma constantes positiva, ? entre [0, 1] e dependendo do tipo e do gênero do
texto. Um caso de estudo conhecido envolve o manuscrito de Voynich, um texto
que possui sistema de escrita não identificada e uma linguagem ininteligível.
Esse texto, apesar de não ter uma possível interpretação, possui um forte
indicativo de que há uma lógica por trás da linguagem pelos atributos
encontrados V e N (LANDINI, G., 2009).
Com as palavras em mãos, chegamos na normalização das palavras, o processo de
normalização levanta uma pergunta, o que é realmente uma palavra? Geralmente,
na própria língua não uma há uma definição generalista capaz definir o que
constitui uma palavra.
Podemos considerar a pontuação presente na escrita como uma palavra? Devemos
contar como palavras distintas quando uma palavra apresenta em CAIXA ALTA e
outra em caixa baixa (ATENÇÃO ou atenção)? Há sentido contabilizar conjugações
diferentes (estou e estamos)?
Essa pergunta complicada tem que levar em conta o contexto da análise, se
estamos processamento sentimentos há uma diferença quando uma palavra aparece
em maiúscula e minúscula. Aqui, passamos a entender a necessidade de um claro
objetivo em mente antes da análise, pois é esse objeto que norteará as escolhas
metodológicas necessárias durante o processamento da linguagem natural. Aqui
também, começamos a realizar considerações sobre a seleção do conjunto de
palavras, o como devemos traçar recortes para verificar o quão bom os
algoritmos estão sendo otimizados para critérios específicos. Na literatura
temos conjuntos especializados para isso denotados como corpus.  O corpus são
um conjunto de texto rotulados que possibilitam a validação e teste dos
resultados obtidos.
Convencionando o que buscamos como palavra, entramos em Lemmatization, isto é o
processo de extração de lemas. Definimos como lema o conjunto lexical tendo
palavras que representam a mesma ideia. Por exemplo, gatos e gato possuem o
mesmo lema: gato. A grosso modo, através do lema temos uma simplificação de
palavras que possuem formatos distintos. Essa simplificação à uma palavra raiz
possibilita remover repetições e inflexões relacionadas à gramática. Para
entendermos melhor, devemos considerar a morfologia como a pequena unidade
significativa que compõe as palavras, nesse campo há uma ramificação das
palavras considerando sua variação, e.g., inflexões, conjunções, entre outros.
Comumente, a simplificação à um core possui um custo computacional alto, sendo
necessários em contexto aplicados à tradução.
Com as definições morfológicas e validação empírica, também observamos que
comumente há palavras que não possuem um significado semântico importante. Em
outras palavras, não contribuem para a macro ou micro análise desejada.
Novamente, devemos nos atentar à aplicação. Em aplicações em que não buscamos
extração somente por repetição e derivados, palavras como: a, um, para, que
podem ser excluídas através de um stopword (também conhecida como stoplist).
Stopword consiste de uma lista de palavras que são excluídas na tabulação dos
dados. Devemos no atentar também sobre a extensão dessa lista, pois, se longa a
mesma pode tirar informações importantes e necessárias na análise de dados.
Discutimos em sala, o artigo de Dolimic, L. e Savoy, J. (2009) que concluíram
que o número de palavras em um stopword deve ser próximo de 9 para uma melhor
análise. Essa heurística traça um norte com relação à esse tipo de busca, no
entanto, nos relembra da limitação em relação à seu uso. Podemos analisar que o
uso de uma única stopword em diferentes aplicações pode gerar distorções em
nossos dados. Assim como o uso de um único parâmetro de um algoritmos otimizado
à uma base de dados específica. Se esse parâmetro for usado de forma irrestrita
podemos reduzir significativamente métricas como acurácia e precisão.
Encerramos o assunto sobre stopwords que em aplicações com maior rigor técnico
devem construir as stopword manualmente.
Finalizando a aula, discutimos algumas considerações não triviais. Até o
momento, levamos em conta algumas premissas importantes para realizar todo o
processo descrito, como por exemplo, a separação clara na língua de um espaço
entre as palavras. Isso não é possível se generalizar em múltiplos idiomas, em
específico em línguas aglutinantes e/ou línguas do tronco linguístico sino-
tibetano (chinês, japonês, coreano, entre outros). Um algoritmo muito conhecido
e consagrado para word tokenization em chinês é o Maximum Matching (também
conhecido como algoritmo de Greedy) consiste de uma busca gulosa aplicada na
separação de palavras. Esse algoritmo funciona da seguinte maneira: 1º passo,
inicializamos o algoritmo com uma lista de palavras em chinês e uma string
(frase a ser separada); 2º passo, começamos marcando um ponteiro para o começo
da string; 3º passo, buscamos a maior palavra no dicionário que casa com o
começo da string; 4º passo, movemos o ponteiro ao final do casamento de
palavras; 5º passo, repetimos o passo 3. Essa técnica, infelizmente não possui
bons resultados em inglês ou idiomas com construção semelhante. Esse método,
apesar de ser um baseline conhecido, possui sucessores que modelam de forma
probabilística essa separação.
Finalizamos a aula dialogando brevemente um outro processo de normalização, que
não busca uma palavra raiz e sim um recorte da palavra, conhecido como
Stemming. Houve também, durante a aula, uma exposição prática na linguagem de
programação Python, com inúmeros comandos e boas práticas para extração de
palavras, com e sem stopwords.
 
Referências usadas:
DOLAMIC, Ljiljana; SAVOY, Jacques. When stopword lists make the difference.
Journal of the Association for Information Science and Technology, v. 61, n. 1,
p. 200-203, 2010.
HEAPS, Harold Stanley. Information retrieval, computational and theoretical
aspects. Academic Press, 1978.
HERDAN, Gustav. Type-token mathematics. Mouton, 1960.
LANDINI, Gabriel. Evidence of linguistic structure in the Voynich manuscript
using spectral analysis. Cryptologia, v. 25, n. 4, p. 275-295, 2001.


É comum tratarmos sobre palavras e stopwords quando falamos de normalização de
texto. Revisando a aula de expressões regulares (ER), quando possuímos uma sub-
expressão entre parênteses em uma ER, podem ser de dois grupos: grupo 0 em que
há o casamento inteiro e grupo 1 em que há o primeiro casamento entre
parênteses, da esquerda para a direita. Às vezes em ER não é possível pegar
palavras que contenham símbolos como acentos, mas que seriam de suma
importância. Nestes casos há a necessidade da utilização da tabela ASCII
estendida, com exceção de alguns símbolos que não são utilizados na escrita.
Quando lidamos com palavras em PLN falamos de um termo chamado tokenização, que
consiste na tarefa de identificar tokens, isto é, palavras em um dado documento
ou texto. Também é utilizado o termo corpus, uma coletânea de documentos ou
sentenças sobre um determinado assunto, utilizados para aprendizado (análise) e
validação. Há evidências de que quanto maior o vocabulário maior é o número de
tokens, representando um crescimento de um vocabulário. Vimos a parte prática
em python sobre identificação de palavras e a quantidade de vezes que aparecem
em um documento. Nem sempre a tokenização é uma tarefa fácil, e muitas vezes
pode ser bem complexa, como é o caso em do teste realizado em aula que dava
frases e pedia a quantidade de palavras. Além disso, em idiomas que possuem
símbolos incorporados, como kanji (ideogramas) em linguagens como japonês e
chinês, é necessário a utilização de outro tipo de algoritmo para a contagem de
frequências. O mais comum é o maximum mathing que realiza uma busca gulosa na
frase para a identificação da separação das palavras, sendo necessário um
dicionário, mas não funcionando muito bem para línguas como o inglês.
Há a necessidade de pré-processamento de textos em relação à palavras que
possuem poucos sentido semântico (resposta para a segunda questão do teste),
também conhecidas como stopword/stoplist. Uma stopword é uma palavra que pode
ser considerada irrelevante para a análise do contexto em que ela está
inserida, geralmente são compostas por artigos definidos e indefinidos,
preposições e conjunções. Na parte prática vimos como são feitas as leituras
das stopwords e como podemos retirá-las dos textos, mas muitas vezes é
compensável que o desenvolvedor crie a sua própria stoplist com suas stopwords,
já que quais palavras que podem ser consideradas irrelevantes dependem do
contexto em que estão inseridas. Ademais, seu uso deve ser avaliado para cada
aplicação, já que algumas palavras que poderiam ser consideradas stopwords
possuem grande relevância para a análise de um texto, podendo inclusive
influenciar e reduzir a efetividade de uma busca, tendo muitas aplicações que
não as utilizam.
Podem ser realizadas também a normalização das palavras. Em línguas como o
português muitas palavras são flexionadas em, por exemplo, gênero, além de
inúmeros tempos distintos. Para o tratamento destas ocasiões há a normalização
de palavras, isto é, a redução ou a simplificação de palavras (que fora o
objetivo da terceira questão do teste). As duas técnicas mais importantes de
normalização de palavras são stemming e lemmatization. A técnica de stemming
consiste em reduzir uma palavra ao seu radical sem levar em conta a sua classe
gramatical, sendo um processo de heurística que corta as extremidades das
palavras removendo os afixos derivacionais, com um conjunto de regras
dependendo da linguagem. Há diversos algoritmos de stemming, como lovins
stemmer, porter stemmer e paice stemmer. A técnica de lemmatisation consiste em
deflexionar palavras, retirando a conjugação verbal, alterando substantivos e
adjetivos para o singular masculino, de maneira a reduzir uma palavra ao que é
encontrado em um dicionário. Essas técnicas podem ser aplicadas em recuperação
de informação, em que um vocabulário menor possa levar a um melhor desempenho
na busca, sendo que se o objetivo é a busca é utilizado stemming, e se o
objetivo é a tradução é utilizado o lemmatisation.


Resumo
 
Foi realizada uma revisão da aula anterior abordando tópicos de expressões
regulares. Foram mostrados comandos para selecionar duas palavras iguais
consecutivas através do comando
\b(\w+)\W+\1
Sendo que:
Cada sub-expressão entre parênteses em uma ER é um grupo:
\W+ é uma expressão regulara para filtrar palavras
A tabela ASCII estendida foi abordada para caracteres especiais. Na sequência
foi apresentando o conceito de token e "tokenização" para palavras, abrangendo
palavras únicas e questões de pontuação.
No contexto de PLN, um corpus é um conjunto de documentos (ou de frases)
geralmente anotados e utilizados para: 
Aprendizado (análise) 
Validação (verificação)
Em www.corpusdoportugues.org é possível verificar um  Corpus do Português que
tem duas partes distintas: um corpus (original e menor) que permite ver as
mudanças históricas assim como variações de género. um corpus (novo e muito
maior) que permite verificar as variações dialéticas (e tem 50 vezes mais dados
do português moderno)
Foram apresentadas aplicações do conteúdo visto na aula na parte pratica no
texto "A semana" de crônicas reunidas de Machado de Assis.
Algumas considerações não triviais foram abordadas tomando como exemplo textos
e frases que possuem dígitos numéricos, caracteres especiais e acentuação, como
por exemplo: 
Total de R$10,45
Para valores superiores a 45.455,67
www.ufabc.edu.br
UFABC
Livre-docente
Homem-Maquina
D'água
U.F.A.B.C.
C.M.C.C.
Ph.D.
Sant'Anna
L'ensemble
Lebensversicherungsgesellschaftsangestellter
???????????????????
O algoritmo Maximum Mathing que utiliza busca gulosa, pode ser usado para
separar palavras em textos que tenham sido elaborados sem a utilização do
espaço.
Uma stopword é uma palavra irrelevante para a análise do texto (ex: a ao aos
aquela aquelas aquele aqueles aqui aquilo...) .Uma stoplist é uma lista de
stopwords
O conceito de normalização das palavras foi iniciado com a introdução de duas
tecnicas importantes :
Stemming: O processo de stemming consiste em reduzir a palavra à sua raiz (sem
levar em conta a classe gramatical). Stemming geralmente refere-se a um
processo de heurística que corta as extremidades das palavras inclui
frequentemente a remoção de afixos derivacionais.  Ex.: amig : amigo, amiga,
amigão gat : gato, gata, gatos, gatas 
Lemmatization: O processo de lemmatisation consiste aplicar uma técnicas para
deflexionar as palavras (retira a conjugação verbal, caso seja um verbo, e
altera os substantivos e os adjetivos para o singular masculino, de maneira a
reduzir a palavra até sua forma de dicionário). Ex.: amigo : amigo, amiga,
amigão gato : gato, gata, gatos, gatas ter : tinha, tenho, tiver, tem


Como vimos na aula passada, podemos realizar pesquisas em texto usando
expressões regulares, entretanto, na língua portuguesa temos caracteres
acentuados em que sua marcação não é tão trivial. Se uma extração for feita
utilizando a RE \w+, teríamos a separação onde tivermos acentos. Para
corrigir isto, basta incluir os caracteres sequenciais da tabela ASCII no
intervalo da disjunção para acrescentá-los à extração: [-'a-zA-ZÀ-ÖØ-öø-ÿ]+.
Com isto separamos somente as palavras desejadas na nossa pesquisa,  incluindo
as acentuadas. Todavia, continuamos tendo alguns problemas em definir o que é
uma palavra; se a pontuação é um dos métodos de se separar o texto, T.A.R.S.
será separado em letras individuais, perdendo o sentido original. Outro
problema clássico é em relação a números com casas decimais ou então palavras
que incluem caracteres especiais em sua composição, como notação monetária.
Para estes casos é necessário técnicas mais avançadas para entender onde a
pontuação realmente termina uma palavra e quando ela faz parte dela. Levando em
conta que já temos todos os problemas de separar palavras válidas sob controle,
podemos chamar cada uma delas de token. Neste ponto, se calculada a
frequência de cada token no texto, vemos que, independente da entrada, teremos
sempre as mesmas classes de palavras como sendo as mais frequentes; artigos,
preposições e advérbios. De forma geral, estas palavras com alto grau de
repetição não carregam muita informação se analisadas de forma isolada, podendo
ser descartadas(dependendo do caso). Chamamos esses tokens descartáveis de
stopwords ou stoplist. Stopwords comuns em português podem ser o, a, então, é,
portanto, dentre outros. Depois de removidas todas as stopwords, ainda temos
várias palavras repetidas, mas em flexões diferentes. No fundo, as palavras
aluno, aluna, alunos, alunas são todas as mesmas, da mesma forma que todas as
conjugações do mesmo verbo trazem a mesma informação individual. O objetivo
agora é modificar, normatizar, cada uma das flexões para que todas estejam
escritas da mesma forma. Para isto existem dois métodos principais, o Stemming
e o Lemmatization. No stemming consideramos somente a parte que não muda na
palavra, ou seja, aluno, aluna, alunos, alunas seriam substituídos por
alun, que é a parte invariável. Já o método Lemmatization, reduz a palavra
para a sua forma não conjugada, ou no masculino e singular. Assim, aluno,
aluna, alunos, alunas seriam substituídos por aluno. É importante ressaltar
que as técnicas de remover as stopwords e depois normalizar o texto variam de
acordo com a utilização. Em estudos relacionados à análise de sentimento, não
podemos cortar certas stopwords, já que elas são cruciais para entender a real
intenção do escritor. Em contrapartida, alguns motores de busca efetuam estes
métodos para reduzir a quantidade de buscas no banco de dados, agilizando a
pesquisa do usuário. ou seja, cada aplicação deverá receber uma abordagem
diferente, corroborando com o objetivo final.


O uso de parênteses ((\w), por exemplo) indica um agrupamento, que serve como
um princípio de memória nas ER, pois é possível ser referenciado através de
\1. Porém, caso tenham parênteses dentro de parênteses, como em ((\d+)\.
(\w+)), é possível referênciar cada um dos agrupamentos, sendo lido da
esquerda para a direita. Então \2 indica o grupo (\d+), e assim por diante.
Para acentos ou caracteres especiais, consultar a tabela ASCII estendida, todos
os símbolos podem ser usados nas Ers.
É nececssário diferenciar token de vocabulário (ou tipo). O primeiro consiste
em qualquer palavra, ou cadeia de caracteres que possuam caracteres de
separação antes e depois (vírgulas, pontos e espaços). Então, quando falamos de
tokenizção, estamos separando as palavras de um texto, mesmo que repetidas. Já
um vocabulário/tipo, consiste apenas de palavras diferentes. Por exemplo, a
frase Fiz um bolo, comi um bolo possui 6 tokens e 4 vocabulários.
Corpus Textual é um conjunto de documentos de texto utilizados para Aprendizado
e validação. Por exemplo, a análise empírica que gerou a Lei de Herdan/Heaps,
que diz que o crescimento de um vocabulário em relação ao de tokens é da ordem
de: |V| = kN^b, onde 0 &lt; b &lt; 1. Isso é muito últil para analise de texto
criptografados ou com linguagens perdidas. No caso do manuscrito Voynich, não
sabiamos se era um texto com uma linguagem humana ou um texto com caracteres
aleatórios e sem sentido. Como o texto obedece à Lei empírica de Herdan/Heaps,
podemos considerá-lo um texto real.
Uma outra forma de segmentar um texto é usando o algoritmo Maximum Matching,
que usa modelos estatísticos (aprendizado supervisionado) e um dicionário.
Para uma análise melhor de um texto após a segmentação é necessário usar
Stopwords, que são palabras muito usadas na maioria dos textos (sempre ocupando
as primeiras posições num ranking de ocorrências) e, no contexto de
processamento e análise, não possuem significado semântico. Po?em, antes de
usar uma banco de stopwords, é necessário analisar a situalção e adicionar
exceções, pois em alguns contextos algumas das stopwords podem ter um
significado grande (por exemplo Internet das coisas viraria Iternet coisas,
perdendo grande parte do seu sentido).
É necessário, em alguns casos, normalizar palavras. Por exemplo, comeriam e
comerão podem ser consideradas uma palavra só (devido ao seu sentido) e
poderiam ser reduzidas a comer. Esse processo de simplificação pode ser feito
através de Stemming, que reduz uma palavra à sua raiz (amiga e amigos viram
amig). Isso é feito através de um corte do final da palavra, dado por uma
heurística (conjunto de regras). Ou ainda através de Lemmatisation, que usa um
dicionário de palavras (menos eficiência na memória) para reduzir diversas
variações de uma mesma palavra (amiga, amigas e amigos viram amigo, que é a
forma mais resumida).


Começamos a aula fazendo a definição de alguns termos. Tokens) Quantidade de
palavras existentes em um texto. Tipo/Vocábulo: Quantidade de palavras
diferentes no texto. Corpus) Coletânea sobre um determinado assunto geralmente
utilizado para aprendizado e validação.
Em geral existe uma relação de lei de potência entre o número de vocábulos |V|
e a quantidade de tokens N: |V| = kN^(B), onde 0&lt;B&lt;1. O interessante desta
relação é que através dela é possivel ter indícios se o texto em questão se
trata de fato de uma linguagem.
  Após esta introdução, o professor nos ofereceu um teste cotendo 3 questões:
1) Havia diversas palavras contendo caracteres e tínhamos que dizer quantas
palavras haviam cada linha. Existiam exemplos simples como a palavra "total" e
outros mais relativos como o termo www.ufabc.edu.br.
2) Explique o que é stopword. Inicialmente não poderíamos responder esta
pergunta uma vez que o conceito de stopword ainda não havia sido apresentado. 
Mais tarde, descobrimos que stopwords se tratam de palavras com pouco
significado semântico no texto e que, portanto, não contribuem para a
identificação de seu tema.
3)  A pergunta pedia para indicar uma palavra que melhor representa-se as
outras. 
Dado as palavras "outra", "outro", "outras" e "outros", encontre uma palavra
que melhor representa o conjunto. Este caso foi mais simples. A palavra "outr"
é uma boa candidata pois está contida em todas as outras palavras. No entanto,
o segundo caso era mais complicado. O conjunto de palavras era "tinha",
"tenho", "tem", "ter". Este caso é mais complexo que o anterior pois as
palavras não tem uma estrutura base. O professor disse que veríamos técnicas
para estes casos mais complexos posteriormente.
Após o teste nos aprofundamos no conceito de stopwords. Vimos, que em geral
elas são as palavras mais frequentes de um texto. Também foi nos apresentado um
exemplo de um texto onde todas as palavras foram removidas, exceto as
stopwords. Era impossível dizer sobre o que se tratava o texto, contribuindo
para a ideia de que stopwords não carregam grande significado semântico. 
Embora em muitos casos as stopwrods sejam desnecessárias, devemos tomar cuidado
com alguns casos específicos. A palavra "A" é uma das palavras mais frequentes
do idioma português e é uma stopword, no entanto, na expressão "Vitamina A",
ela representa um papel semântico muito imporatante. Portanto, é imprescindível
analisar quais serão as stopwords que removidas do texto.
Finalmente, vimos o conceito de normalização de palavras: O idioma português
possui muitas palavras flexionadas em gênero, número ou grau. A normalização
tem como objetivo reduzir ou simplificar estas palavras. Vimos dois métodos de
normalização: 1) Stemming: consiste em reduzir a palavra em sua raiz. 2)
Lemmatisation: consiste em deflexionar a palavra para sua forma no dicionário.


Aula 3. Grupos refere-se ao uso de parênteses em uma expressão regular. A
numeração dos grupos é feita da esquerda para a direita. Grupo 0 é quando não
há parênteses causando casamento inteiro da RE. Grupo 1 é o casamento da RE
dentro do primeiro parênteses. De modo geral, Grupo N é o casamento da RE
dentro do N-ésimo parênteses. Token é uma palavra ou um outro elemento atômico
capaz de ser analisado, isto é, não foi ignorada pela RE. Vocabulário é o
número de tokens diferentes no texto. Corpus, em NLP, é um conjunto de
documentos utilizados para aprendizado (análise) e validação (verificação). A
lei de Herdan, também conhecida como lei de Heaps, é uma lei determinada
empiricamente que descreve o tamanho do vocabulário em um documento ou um
conjunto de documentos em função do número de tokens no documento. É descrita
por |V| = kN^beta, onde V é o vocabulário, N é  o número de tokens, k e beta
são parâmetros determinados empiricamente sendo 0&lt;beta&lt;1. Problemas com RE. Em
línguas de alfabeto latina, mas com modificadores, o shortcut \w ignora tais
caracteres. A definição do que é um token pode depender da aplicação, mas de
modo geral está atrelado ao significado semântico. Por exemplo, 'R$10,45' deve
ser entendido como um token apenas. Outro exemplo, 'L'ensemble' também deve ser
entendido como um token apenas. Deste modo, observamos que o mecanismo de RE
pode funcionar relativamente bem para o inglês. No entanto, RE não funcionam
tão bem dependendo do quanto mais distante a língua for do inglês. Tal problema
agrava-se quando notamos que RE depende de espaços entre os tokens para
funcionar bem. No entanto em línguas como japonês, chinês e outras, não há
espaço entre palavras. Sendo necessário o uso de outros métodos, como o
algoritmo guloso Maximum Matching, requerendo um dicionário. Outros métodos
mais sofisticados usam modelos estatísticos (aprendizado de máquina). Stopwords
são palavras que podem ser consideradas irrelevantes para a análise, como por
exemplo preposições. No entanto, cada aplicação deve tomar cuidado na escolha
de suas stopwords. Ao analisar uma obra literária, provavelmente poderíamos
incluir o artigo 'a' na lista de stopwords. Mas se usarmos a mesma lista de
stopwords para analisar um livro de biologia, teríamos o problema de ignorar o
token 'vitamina A', que poderia ser recorrente. O artigo "When stopword lists
make the difference", dos autores Ljiljana Dolamic & Jacques Savoy sugere uma
lista de stopwords mínima, de aproximadamente 9 tokens. Normalização de
palavras é a simplificação de palavras de modo que palavras de mesmo valor
semântico, mas que possuem inflexões, sejam analisadas da mesma forma. Existem
duas técnicas principais para normalizar palavras, Stemming e Lemmatization.
Stemming ignora a classe gramatical reduzindo a palavra à sua raiz.
Lemmatization aplica regras para transforma a palavra em sua forma de
dicionário. Ambos métodos dependem da aplicação. Por exemplo, Stemming é mais
útil em buscas enquanto Lemmatization é mais útil em traduções.


Ainda dentro do conceito de expressões regulares, foi abordado o tema de grupos
ou agrupamentos de caracteres. Várias sub-expressões dentro de um mesmo padrão
podem ser reconhecidas e numeradas, sendo que a numeração parte dos
agrupamentos mais amplos para os mais restritos e segue da esquerda para a
direita. Além disso, para as expressões regulares em língua portuguesa, é
necessário incluir outros caracteres presentes na tabela ASCII estendida, para
incluir hífen, apóstrofo e  acentuações diversas.
No estudo da linguagem natural, existe o conceito de tokenização, em que cada
palavra pode ser considerada como um elemento distinto (token) no texto. A
padronização do que se considera token depende da necessidade do usuário,
podendo ser especificada no algoritmo. O conjunto de palavras diferentes
existentes no texto é definido como vocabulário, sendo este sempre menor do que
o número de tokens. Verificou-se a existência de um padrão entre a relação do
número de palavras em um texto e seu vocabulário, presente em qualquer texto em
linguagem natural. Outro conceito é o de corpus textual, que se refere a um
conjunto de textos, dentro de uma língua, utilizados como base de dados.
Na análise de textos, alguns algoritmos podem ser implementados buscando a
compreensão textual. Um deles é o algoritmo de casamento máximo de palavras,
que é mais eficiente em algumas línguas e não em outras. Neste caso, a partir
de um dicionário pré-estabelecido busca-se identificar palavras (as maiores
possíveis - greedy) a partir de um texto corrido, sem espaços. Outro conceito é
o de stopwords, que são palavras irrelevantes para a análise  textual,
incluindo artigos, preposições e outras palavras muito utilizadas que pouco
contribuem para o significado do texto. A exclusão das stopwords é útil, porém
deve ser feita com cuidados específicos, que dependem de cada tipo de texto.
Também utiliza-se a normalização de palavras, que é a busca de reduzir várias
palavras com o mesmo significado semântico para um único token. Na técnica de
stemming, busca-se reduzir as palavras à sua raiz comum (gat representaria
gato, gata, gatos, etc.). Já na técnica de lemmatization, busca-se transformar
todas as palavras do grupo para uma palavra genérica, por exemplo, substantivos
mudam para a versão no singular e masculino e verbos mudam para o
infinitivo. Na prática da criação de algoritmos, Python é uma das
linguagens que melhor se adequa ao processamento de linguagem natural, pois já
tem funções específicas para esse tipo de trabalho.
 
 
 
 


No início da aula, revimos o conceito de "memória" das expressões regulares, em
que podemos guardar o que foi buscado dentro dos parênteses da sentença. E
ainda, se tivermos um ou mais parênteses dentro de um outro, também podemos
usar a mesma ferramenta para armazenar os dados. Por exemplo, no caso "((\w+)\.
(\w+))@\1", a ferramenta "\1" repete o primeiro grupo, ou seja, tudo que está
dentro do parênteses de fora. Um exemplo desse caso seria
"alguma.coisa@alguma.coisa". Já usando "((\w+)\.(\w+))@\2", o que seria
repetido seria apenas o primeiro parênteses de dentro, chamado de segundo
grupo. por exemplo: "alguma.coisa@coisa". E assim por diante.
O segundo desafio da aula foi determinar um método para buscar palavras que
possuam hífen ou apostrofe ou acento em alguma letra. Por exemplo "homem-
máquina", "d'água" ou "família". Como sabemos, usando a expressão regular
"\w+", essas palavras seriam separadas e contadas como duas palavras
diferentes. Para o hífen e apostrofe, a estratégia mais simples é englobá-los
na busca. Por exemplo: [-'a-zA-Z]+. Contudo, para palavras com acento, seria
preciso consular o alfabeto ASCII, para englobar todos os caracteres possíveis.
Assim, a expressão melhor seria "[-'a-zA-ZÀ-ÖØ-öø-ÿ]".
Em seguida, vimos os conceitos de "token" e "vocábulo", onde token é o número
de palavras totais no texto, contando com pontuação, e vocábulos (ou tipos) são
o número de tokens diferentes dentro de um texto. Vimos também que a relação
entre token e vocábulo dentro de um corpus (coletânea) segue uma função do tipo
raíz quadrada. Esse, inclusive, é um argumento para evidenciar se um
determinado texto é ou não válido. Ou seja, é de fato um texto e não apenas um
conjunto de palavras aleatórias.
Por fim, acompanhamos um programa para fazer a contagem de palavras totais e
contar as palavras diferentes. Concluímos que muitas das palavras mais
frequentes de um texto não são significativas em relação ao assunto do texto.
Por exemplo, palavras como "que, a, de, do, aquele, isso, no, na" etc. Essas
palavras recebem o nome de STOPWORDS. Acompanhamos então o mesmo programa de
contagem sendo rodado, porém desta vez retirando as palavras STOPWORDS.
Contudo, percebemos que nem sempre as stopwords são insignificantes, e que, em
alguns casos, são palavras importantes numa busca. Exemplo: Rio de Janeiro vs
Rio Janeiro.
No final da aula, fomos introduzidos aos conceitos de "Stemming" e
"Lematization", em que o primeiro é uma ferramenta para reduzir uma palavra em
sua raíz, e último consiste em transformar uma palavra em seu variante singular
e masculino (como é encontrado no dicionário). Ambas tecnicas são usadas para
normalizar um texto. Essa normalização é importante pois, numa análise, podemos
encarar as palavras "bonito, bonitos, bonita, bonitos". Pelo Stemming, a
palavra que representa melhor esse grupo de palavras é "bonit" (palavra é
cortada); mas no Lematizarion, transformaríamos esse conjunto de palavras em
"bonito" (singular, masculino).


Algumas definições:
Corpus é uma coleção de texto ou fala legível por computador.
Tokens é a quantidade total de palavras em um corpus, enquanto tipos ou
vocabulário é a quantidade de palavras distintas de um corpus.
Em algumas línguas não possuem delimitadores entre palavras, sendo necessário
processar o texto e encontrar os limites de palavras, para isso, pode ser usado
o algoritmo MaxMatch, que é uma versão de busca gulosa, e precisa de um
dicionário (lista de palavras da língua), o algoritmo consiste em apontar para
o começo de uma string e escolhe a maior palavra no dicionário que é igual a
entrada na posição atual, e o ponteiro procede para o final desta palavra na
string, caso não encontre nenhuma palavra o ponteiro então avança um caracter.
Este processo é então aplicado iterativamente a partir da nova posição do
ponteiro. O algoritmo é usado para encontrar o limite de palavras em línguas
que não usam espaços entre palavras.
Em diversas línguas muitas palavras são derivações de uma outra palavra, com
objetivo de expressar diferentes categorias gramaticais, como tempo, numero,
grau, etc. Para algumas tarefas de processamento de linguagem natural é
interessante durante o pré-processamento reduzir ou simplificar essas
derivações, duas formas são:
Processo de stemming:
Remover de forma heurística os afixos de uma palavra para obter o seu tronco
(raiz).
Processo de lemmatization:
análise morfológica de uma palavra para encontrar seu lema, que é uma forma
normalizada de um conjunto de formas morfologicamente relacionadas, escolhidas
por convenção para representar esse conjunto. Esta é a forma também é conhecida
como forma de dicionário.


Na terceira aula de PLN o professor falou sobre normalização de texto. Como vem
acontecendo, os slides são baseados no livro Speech and language processing.
Revimos o exercício 6 da aula passada, em que deveríamos encontrar um padrão no
texto em que palavras se repetissem, como em "bola bola", "casa casa". A ER
para isso é \b(\w+)\W+\1. \b é uma ancora que indica se queremos encontrar
palavras que contenham o padrao (apenas para letras) no inicio ou fim da
palavra, dependendo de onde esta o \b. \1 indica que procuramos um padrão
(subgrupo) definido por ( ) que se repete. É importante reparar que vogais com
acento geralmente nao sao capturadas por expressoes regulares por serem
considerados simbolos distintos pela tabela ASC II. Entao, antes de processar o
texto é importante realizar um tratamento retirando acentos e deixar o texto em
um mesmo formato de letras (maiusculas ou minuscuilas). 
Nesse contexto, tokens remete ao número de palavras existentes em um documento
enquanto vocabulario é o conjunto de palavras distintas de determinado
documento. Um conjunto de documentos é chamado de corpus, geralmente utilizamos
eles para fazer analise ou validação. A Lei de Herdan tenta descrever
empiricamente a relação entre o número de palavras distintas de um documento em
função do seu comprimento. Ela é descrita como |V| = k(n^B).
Na parte prática, trabalhamos com o pacote sys (fornece informações sobre
constantes, funções e métodos do interpretador python) e o pacote re(modulo que
fornece expressoes regulares no python). Analisamos um corpus formado por
cronicas do machado de assis. Stopwords são palavras consideradas irrelevantes
em um contexto, muitas vezes retiramos ela do corpus para nao atrapalhar a
analise, porém, o tamanho de uma lista de stopwords pode reduzir a efetividade
de uma busca. Normalizar uma palavra é o mesmo que reduzila ao seu radical,
existem duas tecnicas para fazer isso: Steamming (reduzir a palavra ao seu
radical; corta as extremidades da palavra; pode ser representado como um
conjunto de regras que depende da linguagem; sample text, lovin steammers,
porter steamers, paice steamers) e Lemmatization (consiste em aplicar técnicas
para desflexionar as palavras. Retira conjugação verbal, altera substantivos e
adjetivos para o singular masculino; reduz a palavra até sua forma de
dicionario; heuristica é mais sofisticada, utiliza um dicionario de palavras).
utilizamos streaming quando queremos realizar busca e lemmatization para
tarefas de tradução.


                  
A aula começou retomando alguns conceitos da segunda aula, como a idéia de
expressão regular, a memória existente por parênteses, após isso foi definido a
representação de palavra, já que /w+ não atinge todas as palavras do português,
assim usaremos [-a-zA-ZÀØ-Ö-öø-ÿ]+ para selecionar todas as palavras em
português, vale ressaltar que as vezes um ponto no meio da palavra não
representa duas palavras, mas sempre vamos consideram que representa.
 
Tokens e tipo/vocabulário
Tokens representam a quantidade de palavras totais ( com repetição ) enquanto
tipo ou vocabulário a quantidade de palavras sem repetição ,ou seja, Tokens tem
uma idéia parecida com o número de elementos de uma lista, enquanto tipo é na
ideia de número de elementos de um conjunto.
No parágrafo acima o tokens é igual a 47 enquanto o vocabulário é de 30, por
análise empírica existe uma relação entre esses dois elementos ( tipos = k*
(tokens^B, tal que k e B são números reais e B maior que 0 e menor que 1 ), é
perceptível que o vocabulário aumenta com o número total de palavras, porém não
de forma linear.
Foi mostrado onde vamos tirar os dados para analisarmos, no caso vamos usar
algumas corporas ( em uma língua corpus representa um coletivo de registros
horários e/ou textos ),
o nome de alguns bancos de dados são: Shakespeare, Brown corpus, Switchboard
telephone conversations, COCA, Google N-grams e alguns em português disponíveis
emhttps://www.corpusdoportugues.org/x.asp ehttps://www.linguateca.pt/
CETEMPublico/ ( contem corpuras com mais de 1 milhão de Tokens ).
 
Stop words
As Stop words são palavras que contêm uma grande repetição no objeto em estudo
e que não representa muito para a análise do texto, assim nos a excluimos essas
da análise. Essa ferramenta tem que ser utiliza de maneira inteligente, pois
seus uso em excesso nem sempre é benéfica.
 
Stemming e Lemmatization
Streaming e Lemmatization são métodos que tentam minimizar o problema de
palavras com raiz igual, mas que estão flexionadas de forma diferente, como por
exemplo: homem e homens. O stemming tenta reduzir o problema cortando trechos
da palavra, já a Lemmatization busca a palavra em um dicionário e encontra sua
raiz, na visão computacional o stemming é mais barato computacionalmente e mais
simples de ser implementado.
 
Python
Foram mostrado 3 algoritmos em python, o primeiro mostrava todas as palavras, o
segundo as 20 mais utilizadas e o terceiro a mais utilizadas com uma lista com
algumas stop words.
 


A aula iniciou-se com a revisão de utilizão da memória através de regex. Para
que o resultado de um match seja armazenado na memória, coloca-se o trecho da
expressão regular responsável por encontrá-lo entre parênteses. Após isto, para
fazer referência ao conteúdo do primeiro espaço ocupado na memória pela regex
(ou seja, o primeiro match que a expressão entre parênteses encontrou),
utiliza-se a instrução \1. Analogamente, utiliza-se \2, \3, \4 e assim por
diante para acessar os conteúdos seguintes encontrados e armazenados pela regex
através do uso de parênteses, sempre em ordem. O \0 é uma referência ao
conteúdo completo armazenado na memória pela regex.
Após isso, estratégias para tratar acentuação em textos através de conhecimento
prévio da tabela ASCII foram introduzidas, e o conceito de tokenização (ou
seja, divisão do texto em palavas individuais) foi apresentado, assim como os
desafios que esta tarefa traz consigo, por exemplo: sinais de pontuação podem
ser considerados palavras? Deve-se contar palavras repetidas? A resposta
costuma variar de acordo com a aplicação.
O conceito de corpus foi mostrado. Corpus é um conjunto de textos ou frases,
geralmente anotados, que são usados como insumo de algoritmos de aprendizado e
validação dos resultados apresentados por estes algoritmos. 
A análise empírica de textos nos fez chegar a conclusão que o vocabulário
presente em um documento é diretamente proporcional (mensurado através da Lei
de Herdan, em 1960, e da Lei de Heaps, em 1978) ao número de palavras nele
contidas. Vocabulário é definido como o número de palavras diferentes no
documento, enquanto tokens são todas as palavras presentes (incluindo as
repetidas).
Para efetuar tokenização em textos compostos por caracteres não presentes na
tabela ASCII (como textos baseados em kanjis, por exemplo), existe um algoritmo
chamado Maximum Mathing, mas ele é caro, utiliza busca gulosa, requer um
dicionário e não funciona para textos em língua inglesa ou similares. Para tal,
usa-se algoritmos mais complexos baseados em modelos estatísticos e técnicas de
aprendizado supervisionado. 
Stopwords - ou stoplist - foram definidas como quaisquer palavras que não sejam
significativas para o contexto da análise a ser efetuada. Normalmente, tratam-
se de conjunções, artigos, conectivos, preposições etc.
Por fim, a noção de normalização de palavras foi mencionada. Ela consiste em
encontrar representantes para grupos de palavras flexionadas em gênero, número
e/ou grau, para que estas sejam selecionadas em conjunto ou contabilizadas
apenas uma vez, por exemplo. Duas técnicas populares para normalização de
palavras são Stemming e Lemmatization. A primeira consiste em reduzir palavras
as suas raizes: a raiz de "outros", "outro", "outras", "outras", "outrem" pode
ser "outr", por exemplo. Neste caso, raizes são prefixos em comum entre todas
as formas de flexão de uma palavra, mas existem diversas heurísticas que podem
ser utilizadas (assim dando origem a outros algoritmos). É preferível de ser
aplicada em buscas. A segunda gera um dicionário de palavras e possui
heurística mais sofisticada. Normalmente é utilizada em traduções.
 


Para usar uma regex adequadamente na língua portuguesa, é necessário adaptar o
reconhecimento de caracteres acentuados pois [a-zA-Z] não os contempla, devemos
olhar na tabela ASCII estendida. A regex com caracteres acentuados é [-'a-zA-
ZÀ-ÖØ-öø-ÿ]. Pontuações podem ser consideradas como palavras ou não. O
documento também deve ter os caracteres normalizados para somente letras
minúsculas para não haver diferenciação entre a mesma palavra em minúsculas e
maiúsculas. Normalizar as letras acentuadas para suas versões sem acento pode
ser necessário se a fonte de dados não tiver gramática e ortografia confiáveis,
por exemplo de uma rede social. Entretanto, se a fonte de dados for gramatica e
ortograficamente confiável, podemos considerar esses caracteres, exemplos:
médico, medico, bebe, bebê, sábia, sabia.
Corpus é um conjunto de documentos anotados utilizados para análise
(aprendizado) e verificação (validação).
Crescimento do vocabulário: Lei de Heaps ou Herdan
V = kN?, 0 &lt; ? &lt; 1 (radiciação)
O tamanho do vocabulário cresce conforme a quantidade de tokens cresce (mas não
proporcionalmente).
Stopword é uma palavra sem, ou com pouco, valor semântico, como artigos e
preposições, exemplos: a, em, do.
Normalizar as palavras do texto pode ser feito de duas maneiras. Stemming é a
técnica que corta os sufixos das palavras mesmo que o resultado não seja uma
palavra, exemplo: gatos, gata, gato são representados por gat. Lemmatization é
a técnica que remove o sufixo ou o modifica, de modo a eliminar a conjugação de
verbos, flexão de número e gênero de substantivos e adjetivos, etc. Para essa
técnica é necessário um dicionário do idioma.


Primeiramente, vimos sobre a última atividade onde usamos \1 para referenciar a
expressão entre parênteses, onde temos algo parecido com uma memória, o número
após a barra invertida é o número do parênteses, de 1 a n parênteses. Cada
subexpressão dentro de um parênteses nós chamamos de grupo, onde uma expressão
pode ter de 1 a n grupos de subexpressão, sendo a ordem contada da esquerda
para a direita. O grupo 0 é reservado para a expressão inteira,
independentemente de parênteses.
Um problema que temos com PLN quando analisamos o português, é o fato de na
tabela ASCII padrão não existir um código hexadecimal para as acentuações, com
isso expressões como \w+ não funcionarão com acentos, identificando apenas
parte da palavra. Para a solução deste problema, podemos usar a tabela ASCII
estendida, que, diferente da ASCII padrão que abrange de 0 a 127(7F
hexadecimal), ela abrange de 0 a 255(FF hexadecimal)[http://
www.theasciicode.com.ar/].
Tivemos também a definição de tokenização e vocabulário, onde tokenização se
refere a todas as palavras de um texto e vocabulário a todas as palavras
diferentes em um texto. Vimos também, por um exemplo de uma obra de Machado de
Assis, que as palavras de maior frequência no texto, normalmente não são
palavras chaves, e pouco se pode entender do texto pela sua análise. Algo que
pode nos ajudar a encontrar as palavras chaves de um texto, é uma lista de
stopword/stoplist, que ignora palavras de pouco sentido semântico. Esta lista
deve ser cuidadosamente criada, normalmente, deve-se criar uma para cada
análise desejada de um texto para não correr o risco de perder palavras chaves,
como Rio de Janeiro, que poderia ser filtrada como Rio Janeiro, perdendo assim,
a informação.
Ao final da aula, vimos as técnicas de Stemming e Lemmatisation que consistem
em técnicas para reduzir as palavras e encontrar palavras flexionadas.


Normalização de texto: Palavras (tokens) e stopwords
Os tokens são os itens presentes em um texto ou palavras. Conforme a forma de
que se deseja trabalhar com o texto sinais de pontuação por exemplo podem ser
considerados tokens.
Uma coleção textual pré-determinada para ser utilizado em treinamento de
máquina, validação são chamados de Corpus (plural de corpora). Apresentam
assuntos diversificados, geralmente em formato eletrônica podendo ter conteúdo
de linguagem escrita ou falada. Uma coleção textual em português brasileiro que
podemos citar são as obras escritas por Machado de Assis disponível
gratuitamente pela CAPES.
Vocabulário: consiste na quantidade de palavras presentes em um texto,
desconsiderando suas repetições.
 A lei de Herdan (Herdan, 1960) ou Lei de Heaps (Heaps, 1978), encontraram uma
relação empírica entre os tokens e vocabulários em uma corpora, descrita pela
equação:
|V|=kN^B
V é o tamanho do vocabulário;
N é o tamanho de token;
k e B são parâmetros positivos, B depende do tamanho do corpus e do gênero.
Stopwords ou stoplists são palavras com pouco sentido semântico, geralmente são
artigos, preposições, etc. Muitas vezes os stopwords não são utilizados pois
podem atrapalhar no resultado da analise desejada, deixando as palavras sem ou
perdendo contexto, perda de semântica. O estudo de stopwords remonta desde
1960.
Normalização é a redução ou simplificação das palavras, de modo a agrupar em
classes ou agrupamentos que os representem. Duas técnicas importantes são:
Stemming e Lemmatization
Antes do processo de normalização é feito a segmentação da palavra ou
Tokenização. Em algumas línguas como português as palavras já estão
naturalmente segmentadas. Já em línguas como como chinês e japonês a
segmentação é feito pelo máximo casamento ou MaxMatch, onde a palavra é
comparada com um dicionário da língua caso seja encontrado essa palavra ela é
agrupada caso contrário é retirado um caractere e feito novamente a verificação
até ocorrer o casamento.
O Stemming é uma heurística que realiza o truncamento da palavra até sua raiz,
como por exemplo eliminar prefixos e sufixos, remover letras indicadoras de
gênero, grau e numero. Exemplo: outro, outras, outros, outra poderia ser
simplificado para "outr"
Lemmanization ele classifica palavras em um mesmo grupo morfológico, no
português seria a deflexão dos verbos, em outras como alemão ou russo ocorre
para substantivos, adjetivos, pronomes. Exemplo: tinha, tenho, tem, ter podem
ser representados somente pela palavra "ser" como simplificação.
O Stemming geralmente é usado para buscas e o Lemmanization para traduções.


A abordagem sobre normalização de textos tratou de dois aspectos: palavras e
stopwords.
Para extração e/ou filtragem de termos em textos podemos usar expressões
regulares, porém é necessário se atentar a expressão que está sendo feita e
quais os resultados ela proporciona.
Quando usada para agrupar palavras, as ERs precisam conter as características
que o idioma a ser tratado contém, como comparativo podemos traçar um paralelo
entre um conteúdo em inglês e outro em português, o vocabulário em português
possui em certos casos acentos, caracteres especiais como traços e aspas
simples, diferente do inglês.  Como regex se baseia no vocabulário da língua
inglesa, a captura desses casos não é feita sem um tratamento prévio da
expressão.
Portanto, uma mesma expressão coletaria cadeias de caracteres diferentes em
ambos os idiomas caso nada fosse acrescentado para capturar as particularidades
de cada língua.
Suponha o uso do shortcut \w+ para o agrupamento de palavras em um fragmento
que possui o verbete guarda-chuva. Serão encontradas duas palavras: guarda
e chuva, o que contraria o sentido semântico do termo, que dá nome à um
objeto. Para abranger casos como esse é preciso acrescentar à expressão o
intervalo que possui caracteres especiais presentes no idioma.
Quando tratamos de normalização podemos encontramos como uma das tarefas
principais a radicalização palavras, que pode ser feita usando técnicas de
stemming e lemmatization. Stemming seria a função encarregada por remover/
truncar sufixos e manter prefixos comuns. Lemmatization é o processo de
agrupamento das formas inflexíveis de uma palavra para que possam ser
analisadas como um único item.
 
 
 


Descrevemos a aplicação da tecnologia de processamento de linguagem natural
(PNL) para a análise de linguagem subjetiva. Em particular, nos concentramos no
problema da classificação de opinião de material textual extraído de fontes de
dados relacionadas a negócios. Usamos recursos baseados em palavras e
sentimentos para induzir um classificador com base no uso de Máquinas de Vetor,
obtendo resultados de última geração. A partir disso é também possível que
experiências preliminares onde o uso de resumos antes da classificação de
opinião oferece vantagem competitiva sobre o uso de documentos completos quando
os documentos são longos e contêm material tanto subjetivo como não subjetivo.
O conceito de Token também é muito útil pois dependendo de como é empregado uma
frase pode ter mais ou menos e assim mudar a forma como a mineração do texto é
feita. A pesquisa nesta área desencadeou graças a iniciativas de avaliação das
Conferências de Recuperação de Informação (TREC) (Ounis et al., 2008) com a
tarefa de classificação de opiniões, a Conferência de Análise de Texto (TAC) de
geração de Resumos do Texto2 de opiniões e do programa de "Defi Fouille de
Textes" interessado na classificação de opinião (Grouin et al., 2009). O
conceito de StopWord também foi explicado onde algumas palavras extremamente
comuns que parecem ter pouco valor para ajudar a selecionar documentos que
correspondem a uma necessidade do usuário são excluídas do vocabulário
inteiramente. Essas palavras são chamadas de StopWords. A estratégia geral para
determinar uma lista de parada é classificar os termos pela frequência de
coleta (o número total de vezes que cada termo aparece na coleção de
documentos) e, em seguida, tomar os termos mais frequentes, muitas vezes
filtrados à mão para o seu conteúdo semântico em relação ao o domínio dos
documentos sendo indexados, como uma lista de parada, cujos membros são então
descartados durante a indexação. Cabe notar que tivemos revisão da aula passada
acerca de Expressões Regulares contendo a última questão do teste anterior
aplicada e novamente um novo teste foi empregado ao final da aula para fixação
do conhecimento da classe.


Seguindo do Teste de Avaliação em Aula da aula anterior, o exercicío 6,
apresenta um recurso em expressões regulares que é um tipo de memória, seguindo
de lá, existem alguns detalhes que não foram esclarecidos, como qual é a ordem
que os números da memória seguem em relação aos grupos, que é a ordem da
esquerda para a direita em suas aparições (siga o parenteses que abre "("), ou
seja, se há grupos menores dento de um grupo maior, a "memória" acessará
primeiro o grupo maior e depois os parênteses da esquerda para a direita. E há
o \0, que acessa toda a expressão como seu "match", ou um casamento inteiro.
Sobre o problema com acentuações presentes no português, existe uma tabela
ASCII estendida, ela abrange uma maior variedade de símbolos presentes nas
líguas que usam o nosso alfabeto, e qualquer expressão regular (deveria)
reconhecer estes, "[-'a-zA-ZÀ-ÖØ-öø-ÿ]" é a expressão utilizada para reconhecer
qualquer caracter presente em uma palavra em português, por exemplo.
Tokenização e Vocabulário
Seria a expressão para diferenciar o número total de palavras e o número total
de palavras diferentes em um texto, respectivamente. Normalmente textos tendem
a possuir um padrão entre a relação entre os tokens e o vocabulário, seguindo
uma curva de uma função de raiz quadrada, logo um texto que apresente um grande
número de tokens e um vocabulário muito pobre ou excessivamente rico, pode ser
um possível texto falso (criado automaticamente para o famoso "encher
linguíça", por exemplo).
No site da disciplina há alguns exemplos de código para contagem de palavras
( palavra.py), incluindo vocabulário e quantas vezes cada palavra aparece
(palavra2.py). 
todos os códigos podem ser utilizados com o seguinte comando no terminal:
python3 codigo.py texto.txt
Teste de avaliação em aula + Considerações não triviais + Stopword +
normalização
Quais as condições ideais para se considerar onde uma palavra termina? até um
ponto? até um espaço em branco? Essa não é uma pergunta fácil, e nem tem uma
resposta concreta, essa é uma consideração ao se fazer um sistema de contagem
de palavras, se for considerar outras línguas que usam ideogramas como chinês
que não usam espaço, é necessário outro tipo de regra para definir uma palavra,
nesses casos usasse um sistema chamado maximum matching que usando busca gulosa
e um dicionário, ele defini o final de uma palava pela maior que ele conseguir
identificar.
Stopwords, diferentes de seu nome, na verdade são palavras "ignoráveis", como
"a", "em", "para" em português, num sistema de relevância de palavras presentes
em um texto (normalmente são padrões para certa língua, mas podem ser
personalizados de acordo com seu objetivo), requerem um certo aprimoramento
para certos casos e nunca/raramente são usados para ferramentas de pesquisa
como google, bing, yahoo.
Em certos casos uma palavra com outro gênero e/ou flexão, não é interessante,
para certos objetivos, que sejam considerados como palavras diferentes, então
para normalizá-las, usasse: 
Stemming para o menor radical possível de uma palavra utilizando regras
implementadas
Lemmatization para a forma mais genérica da palavra (usa dicionário)


A terceira aula de PLN abordou a normalização de texto. Normalizar um texto
consiste em alterar as formas das palavras para uma que seja padrão, com o
intuito de simplificá-las e facilitar a busca por palavras parecidas. Sendo
que, é importante saber que palavra é um conjunto sequencial de caracteres, é a
unidade da língua escrita. Podemos ou não, considerar pontos e vírgulas como
palavras dependendo da finalidade da busca que queremos fazer. Para isso,
chamaremos as palavras que aparecem em um texto de tokens, assim o total de
tokens da seguinte frase é 9.
- O rato roeu a roupa do rei de Roma
E chamaremos de tipo o total de palavras diferentes em um texto. No exemplo a
seguir temos o total de 8 tipos, dado que "o", "que" e "não", são contatados
apenas uma vez.
-  "Que não sinta o que não tenho
    Não lamente o que podia ter"
Um conjunto de palavras, que forma um texto escrito, chama-se corpus e, uma
coleção de textos ou discursos sobre um determinado assunto, chama-se corpora e
eles são utilizados para o aprendizado e a validação. Podemos simplificar ainda
mais nossa busca e aumentar sua efetividade se desconsiderarmos as palavras que
não possuem significado relevante, como artigos e preposições. Podemos eliminar
essas palavras, chamadas stopwords, antes ou depois da leitura de texto.
Por fim, foram introduzidas as técnicas de normalização de textos, Stemming e
Lemmatisation. Stemming, significa reduzir a palavra a sua raiz. Por exemplo,
as palavras menino e menina seriam reduzidas para "menin". Essa técnica visa
cortar uma extremidade da palavra, de acordo com regras gramaticais da
linguagem. Já a técnica de Lemmatisation tem como objetivo, retira a conjugação
verbal, e altera substantivos e adjetivos para o singular masculino. Exemplo,
amigo, amigão, amiga são todas transformadas em amigo.
Existem algoritmos diferentes para aplicar essas duas técnicas, sendo que é
importante saber qual se adequa mais à uma determinada linguagem.


 Normalização de texto: Palavras e stopwords.
Bibliografia: Speech and Language Processing: An introduction to natural
language processing, computational linguistics, and speech recognition., por
Daniel Jurafsky e James H. Martin, Pearson/Prentice Hall.
Parte contida no capítulo dois da bibliografia.
Da aula anterior vimos como recuperar todas as palavras que aparecem em duplas,
ou seja, repetidas duas vezes consecutivamente, usando o seguinte comando de
expressões regulares: \b(\w+) \W\1
Em expressões regulares cada sub-expressão entre parenteses é um grupo. Os
grupos são numerados da esquerda para direita.
Ex: ((\w+)\.(\w_+))@\1 ? a expressão irá buscar uma sequencia de pelo menos uma
palavra, seguida de um ponto, seguida de outra sequencia de pelo menos uma
palavra, então um arroba seguido do resultado de \1, que significa o grupo da
busca anterior de número 1, ou seja tudo que está dentro dos parênteses mais
externos, (\w+)\.(\w_+) . Se fosse \2, buscariamos o conteudo encontrado pelo
grupo do primeiro parênteses interno, (\w+).
Palavras (tokenização)
O comando \w+ para buscar palavras de ER sginifica [a-zA-Z]+ portanto não irá
encontrar palavras que possuam acentuação. Para isso devemos usar a expressão
[-'a-zA-ZÀ-ÖØ-öø-ÿ].
token: é um conjunto de caracteres (de um alfabeto, por exemplo) com um
significado coletivo.
O processo de tokenização recebe um texto e retorna sua decomposição em uma
lista ligada de tokens.
Na frase Muito longo para os que lamentam, muito curto para os que
festejamtemos 12 tokens, mas somente 8 palavras diferentes(em termos de tipo/
vocabulário).
Em PLN um Corpus textualé um conjunto de documentos ou de frases, geralmente
anotados, utilizados para aprendizado(análise) ou validação(verificação).
* Foram considerados alguns exemplos práticos de programas em Python.
Stopwords sãopalavras com pouco sentido semântico, em muitos contextos pode ser
considerada irrelevante para análise (artigo, preposição). Podem ser eliminadas
antes ou depois do processamento, mas deve ser avaliada sua importância para
cada aplicação.
Normalização de palavras
Em textos em português temos diferentes palavras flexionadas em gênero, número,
grau, intensidade etc., além de inúmeros tempos verbais distintos. Normalização
de palavras pode ser entendido como o processo de simplificação de palavras.
Existem duas ténicas principais:
Stemming, que consiste em reduzir as palavras à sua raiz, sem considerar sua
classe gramatical. Geralmente refere-se a um processo de heurística que corta
as extremidades das palavras inclui
frequentemente a remoção de afixos derivacionais, pode ser representado por um
conjunto de regras que dependem da linguagem.
Ex: amig ? amigo, amiga, amigão, amigos
Lemmatisation,consiste aplicar uma técnica para deflexionar as palavras (retira
a conjugação verbal, caso seja um verbo, e altera os substantivos e os
adjetivos para o singular masculino, de maneira a reduzir a palavra até sua
forma de dicionário). Geralmente usa um dicionário de palavras (a heurística é
mais sofisticada).
As operações de stemming e lemmatization são operações comuns e iniciais
utilizadas em sistemas de recuperação de informação.
 


Num sistema de recuperação de informação, é necessário que se encontre
documentos e páginas que tenham os termos pesquisados em qualquer uma de suas
formas; por exemplo, se procurarmos por /amar/, devem ser encontrados
documentos com /amor/, /amado/ e /amante/, por exemplo. É possível transformar
as palavras flexionadas em uma forma geral, e esse processo é chamado de 
normalização. Em português, as palavras podem ser flexionadas nos tempos
verbais, em gênero, número e grau. A normalização de palavras consiste na
simplificação ou radicalização da palavra. Esses processos são elaborados
através de duas técnicas: o stemming e a lemmatization. O processo de stemming
consiste na redução da palavra para sua raiz, sem considerar a classe
gramatical a qual ela pertence. Em geral, ela consiste em cortar a palavra de
modo a retirar os sufixos, mantendo somente a parte da palavra que não varia
nas derivações. A lemmatization, por outro lado, visa retornar a palavra
derivda à sua forma no dicionário -- em portugues, na forma infinitiva, para
verbos, e na forma singula e masculina nas demais classes. Há muitos algoritmos
de stemming, principalmente na íngua inglesa. Eles consistem, de maneira geral,
de um conjunto de sufixos, de condições de remoção dos sufixos e de regras de
transformação, sem que seja necessário nenhum tipo de conhecimento prévio ou
consult.  Os algoritmos de normalização são uma etapa anterior ao
processamento, portanto, devem ser velozes e não consumir muitos recursos
computacionais; o stemming é muito eficiente nesse aspecto. O uso de um
dicionário para compara os resultados é uma opção para aumentar a acurácia, mas
estudos indicam que os ganhos não valem os recursos investidos. O stemming é
passível de erro, já que nem todas as palavras seguem as mesas regras. Podem
ocorrer erros por excesso, quando são removidos mais que o sufixo, ou por
falta, quando o sufixo não é removido totalmente. Além disso, a remoção do
sufixo pode acaretar na perda excessiva de informação. 


Assim como mencionado na aula anterior, a normalização de palavras pode ser
utilizada para facilitar a busca por documentos, sendo que as técnicas mais
relevantes são stemming e lemmatization. O primeiro possui o objetivo de obter
a raiz da palavra, sendo um processo heurístico de cortar as extremidades da
palavra e remover afixos derivacionais. Já o segundo é obtém os lemmas, o
significado básico da palavra.  Como cada língua possui diferentes
características, geralmente o desempenho de cada método varia, sendo mais ou
menos eficaz dependendo da situação.
Vale notar que a saída dos stemmers não é necessariamente uma palavra raiz e
eles podem cometer erros também, porém podem ser úteis para generalizar e
costumam ser eficientes.  
Algoritmo de Lovins: Um dos primeiros algoritmos de muitos steemers do inglês,
possui apenas duas etapas, onde busca pelo maior sufixo na palavra e em seguida
aplica as regras para transformar a palavra, removendo o final. Basicamente
realiza uma busca em uma tabela de 294 finais, sendo 29 condições e 35 regras
de transformação. Remove o sufixo mais longo e grava a nova palavra em uma
tabela diferente, que pode buscar por uma palavra raiz.
Algoritmo de Martin F. Porter: Voltado para o inglês, possui cinco passos,
remove sufixos e atualmente é o stemmer mais utilizado sendo mais completo que
o anterior.  Possui a ideia base de que a maior parte dos sufixos é composto de
uma combinação de sufixos menores e mais simples. As regras são aplicadas até
que uma delas passe nas condições, se uma regra é aceita o sufixo é removido.
Porém, é mais lento que o algoritmo de Lovins.
Algoritmo de Orengo: Um dos primeiros algoritmos para o português, foi
publicado em 2001 e possui oito passos, onde ocorre algumas reduções, como:
plural, feminino, tamanho, advérbio, pronome, verbo e remove acentos, sufixos e
se necessário, vogais.
NLTK: O python possui um toolkit de linguagem natural chamado NLTK e possui
ferramentas de stemming e lemmatization, além de bibliotecas para
classificação, conversão, análise semântica e outros. Há um demo disponível com
alguns algoritmos mencionados anteriormente em:http://text-processing.com/demo/
stem/
Também é possível remover os prefixos das palavras, mas pode ocorrer uma
remoção maior do que a desejada, ou remover menos do que se deveria. Essa é uma
forma de medir o desempenho do algoritmo.


Iniciamos a aula recapitulando os slides oriundos da aula 02, discutimos o
artigo publicado na revista Scientometrics para identificação de pessoas
importantes apenas com expressão regular. Para tanto, o autor do artigo de
Extract and quantifying eponyms in full-text articles utilizou como
ferramenta apenas expressões regulares em uma abordagem semi-supervisionada com
821 artigos da própria revista. O modelo desenvolvido apresenta quatro etapas
para identificação, sendo elas: parte da extração do autor, da referência, do
"adjectival eponyms" e do objeto. A simplicidade do código espanta, e sua
robustez também com um R² de 0,95, excelente pela simplicidade e pela área.
Começamos levantando a importância da recuperação de informação com quaisquer
formas escritas, e.g., uma busca no google: controladores digitais deve
retornar algo que contenha também controle digital. Isso indica uma
necessidade de um processo de normalização. Na língua portuguesa em específico
possuímos diferentes palavras flexionadas em gênero, número ou grau, além de
inúmeros inflexões temporais. Como vimos na aula passada, no processo de
normalização de palavras possuímos dois tipos técnicas, sendo elas: Stemming e
Lemmatization.
A diferença fundamental entre elas consiste em que uma realiza um corte, e a
outra uma redução considerando sua gramática, Stemming e Lemmatization. Um
ponto de diferenciação importante consiste do fato de que um método busca stems
(parte de uma palavra), enquanto outro tipo busca um lema (forma básica da
palavra).
No processo de Stemming a redução geralmente gerar palavras que não possuem
significado concreto e servem muito mais como um signo de um conjunto de
palavras. Um exemplo pode ser reduzir o conjunto {gatos, gatas, gato e gata}, o
resultado esperado possível seria gat.
Seguindo em frente, saindo categoria para os algoritmos temos o de Lovins, uma
das pioneiras da área. Essa linguista desenvolveu esse método durante o seu
doutorado nos anos 60. Basicamente, o método consiste de duas simples etapas
por palavra, 1ª etapa: procure pelo sufixo de maior tamanho na palavra que
atenda o condicional do sufixo, se atende, realize um corte, senão, reduza o
sufixo; repita até atender alguma condição ou chegar no comprimento 0. 2ª
etapa: aplique as regras que atendam o condicional para transformar a palavra;
faça isso em todas as regras possíveis. Essa lista de sufixo possui 294
situações chave e 29 condições para ser atendida. Já a lista de regras é de
comprimento 39. Após explicação do método, realizamos uma atividade prática em
papel para testarmos os nossos conhecimentos.
Outro algoritmo também conhecido e de funcionamento semelhante é o algoritmo de
Porter, esse método usa a estrutura de regras e operações de validação,
semelhante a busca gulosa presente no algoritmo de Lovins, aqui temos o
casamento do maior sufixo possível também. O método possui cinco etapas, a
primeira responsável pelos sufixos de inflexão, a segunda, terceira e quarta
etapas são responsáveis pelos stemming de sufixos e a última etapa a
recodificação. Detalhes e implementações encontram-se disponíveis nesse link
(https://tartarus.org/martin/PorterStemmer/), é importante ressaltar que o
próprio autor disponibiliza e centraliza as implementações.
Observamos que esses algoritmos para Stemming possuem uma forte dependência da
língua, em específico da sua estrutura gramatical. Um exemplo brasileiro
desenvolvido fora do país é algoritmo o Orengo de 2001, um stemming para língua
portuguesa. Esse código é o primeiro para língua portuguesa e possui 199 regras
e 8 passos.
Algumas considerações sobre o uso da técnica das devem ser feitas. Um ponto
também não explorado extensamente na literatura é a eliminação de prefixo, não
há nenhum ponto teórico que impeça esse tipo de recorte. Um exemplo que podemos
citar é o algoritmo de Affix Stripping Stemmer.
Uma forma de verificar o stemming é analisar através do dicionário, e um
cálculo relacionado à acurácia e outros métricas. Deve tomar cuidado para não
ocorrer situações como under stemming e over stemming. Essas situações análogas
aos dos algoritmos de aprendizado de máquina dizem respeito à generalização e
especialização do método para o corpus. No over stemming, vamos supor que temos
uma palavra X e essa palavra X foi reduzida à um stem que não corresponde ao
seu stem correto, quando isso ocorre temos um falso positivo. Do outro lado, no
under stemming há duas palavras X1 e X2 com stems que representam signos
diferentes, se ambos são classificados na mesma categoria, temos que um falso
negativo. Reconhecer quando os métodos erram e o tipo de erro é fundamental no
desenvolvimento de uma aplicação. Geralmente, os métodos de stemming tendem a
reduzir um tipo de erro e aumentar outro.
Dentro de sala, vimos métodos baseados exclusivamente dentro de regras, no
entanto, na categorização temos também métodos mais robustos baseado em
modelagem estatística. Nos algoritmos estatísticos recorremos à um grande
corpus para aprender a morfologia da língua.
O ganho do Stemming, comparando os métodos de Stemming e Lemmatization é o
tempo computacional, pois, por se tratar que número limitado de complexidade ao
analisarmos a complexidade temos um O(n) ou próximo, enquanto o Lemmatization
depende de mais busca. As principais desvantagens desses métodos, quando não
estatístico, é a forte dependência nas regras. Novas palavras tendem a não se
sair bem ao serem reduzidas, por exemplo, se tentar reduzir IPads e IPad, temos
um problema. Temos aqui uma limitação de generalização na idade das palavras
usadas no texto. Outro problema, mas dessa vez acadêmico, ocorre que na área de
processamento natural de linguagem nas etapas de normalização são dificilmente
bem descritas, levando à grandes problemas de reprodução (Fokkens et al.,
2013). Um link interessante com implementações a serem usadas em futuras aulas
é o recurso Python - NLTK ( http://text-processing.com/demo/stem/ )
Referência:
FOKKENS, Antske et al. Offspring from reproduction problems: What replication
failure teaches us. In: Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). 2013. p.
1691-1701.


Nesta aula vimos mais a fundo sobre a parte de normalização de texto stemming.
Relembrando, no idioma português diversas palavras são flexionadas de acordo
com gênero, número ou grau, além de diversos tempos verbais. A normalização de
palavras consiste em reduzir ou simplificar palavras, ou até mesmo retirar
sufixos ao ponto de deixar a palavra em seu radical.
O processo de stemming reduz a palavra até a sua raiz, por exemplo gat pode
representar gato, gata, gatos, gatas, referindo-se à um processo de heurística
que reduz as extremidades das palavras inclui a remoção de afixos
derivacionais, sendo representado por um conjunto de regras que dependem da
linguagem. Em suma, stemming é a ação de reduzir em stems e lemmatization é a
ação de reduzir em lemmas. Existem diversos algortimos de stemming,
principalmente a linguagem inglesa, como sample text, lovins stemmer e porter
stemmer. 
O algoritmo de lovins foi o pioneiro na área e influenciador de diversos
algoritmos posteriores, sendo composto por 294 sufixos, 29 condições e 34
regras de transformção, dividido em 2 etapas. Na primeira etapa é feito uma
busca pelo sufixo de maior tamanho na palavra e que satisfaça as condições do
algoritmo, e dependo do caso, é removido. É importante notar que o comprimento
mínimo do stem deve ser igual a 2, sendo, inclusive, estaa atividade realizada
em sala, em que tínhamos acesso às regras de sufixos e às condições com o
objetivo de reduzir palavras que eram pedidas. Tanto o algoritmo de lovins
quanto o algoritmo de porter eliminam e removem os sufixos das palavras, sendo
que para cadapalavra não é necessário conhecimento prévio para sua redução.
Para o caso do algoritmo de porter, mais utilizado atualmente, é mais completo
e simples do que o algoritmo de lovins.
Para o português, há o algoritmo orengo, pioneiro, constituído por 199 regras
distribuídas por 8 passos e considerando uma lista de exceções. Quando
analisamos a eficácia do stemming, é importante ressaltar que nem sempre ele
aumenta as chances de uma acurácia alta, podendo inclusive influenciar
negativamente nos resultados, a depende da sua aplicação.
Algumas considerações finais, não é eficaz o processo de stemming utilizando
uma base de dados como dicionário, pois demanda um tempo computacional elevado;
prefixos não são eliminados por não possuir motivo teórico em tal escolha; na
literatura há diversas referências sobre o assunto; há dois termos que podem
ocorrer devido ao stemming: overstemming, quando é removido uma parte do
radical junto com o sufixo e understemming, quando o sufixo não é removido ou é
apenas removido parcialmente. 


Normalização de palavras é um sistema de busca deve permitir que documentos
indexados com diferentes nomes sejam recuperados usando quaisquer das suas
formas de escrita.
Em textos da língua portuguesa temos diferentes palavras flexionadas em gênero,
número ou grau, além de inúmeros tempos verbais distintos. E. g. Trabalhador -
Trabalhadora, Degrau - Degraus, Amigo -Amigão.
O conceito de normalização das palavras foi iniciado com a introdução de duas
técnicas importantes :
Stemming: O processo de stemming consiste em reduzir a palavra à sua raiz (sem
levar em conta a classe gramatical). Stemming geralmente refere-se a um
processo de heurística que corta as extremidades das palavras inclui
frequentemente a remoção de afixos derivacionais.  amig : amigo, amiga, amigão
gat : gato, gata, gatos, gatas
Para ambas as técnicas existem diferentes algoritmos, principalmente para o
inglês.
O algoritmo de Lovins é composto por 294 sufixos, 29 condições e 34 regras de
transformação. O seu processamento é rápido com apenas duas etapas
O algoritmo Orengo de Stemming para Português foi desenvolvido por Orengo,
Viviane Moreira, & Huyck, Christian. (2001). A Primeira versão amplamente
divulgada de um algoritmo de radicalização para a língua portuguesa:
Constituído por 199 regras distribuídas por 8 passos e considera uma lista de
excepções
Em contra ponto com essas técnicas apresentadas, poderia ser utilizado uma se
dados (dicionário) para comparar as palavras reduzidas. Porém esse procedimento
requererá maior tempo de processamento computacional e mesmo consumindo maior
tempo, o esforço investido poderia não valer a pena.
Também é possível eliminar os prefixos em vez dos sufixos, não existe nenhum
motivo teórico para não considerar a eliminação de prefixos nos stemmers
O algoritmo de stemming não deveria permitir a perda de muita informação,
contudo, se as tecnica não for bem aplicada isso pode acontecer. É importante
utilizar medidas de desempenho para verificar o que está ocorrendo com a
informação e detectar casos de Overstemming ou Understemming


A normalização é um processo de radicalização de palavras (tokens) que visa diminuir
conjugações e flexões para uma raiz única, aglomerando o significado de cada uma em um único
termo. Existem diferentes métodos de se reduzir as palavras, como Stemming e Lemmatization,
sendo o stemming o mais utilizado. Dentro de cada método temos vários algoritmos que executam
a mesma tarefa, mas de forma particular, incluindo regras distintas e saídas ligeiramente
diferentes. O primeiro algoritmo de stemming é o Algoritmo de Lovins, criado por Julie Beth Lovins
na década de 60, composto por 294 sufixos, 29 condições e 34 regras de transformação. Este
algoritmo é dividido em duas partes; na primeira, o algoritmo identifica numa lista qual o maior
sufixo que a palavra possui e qual a condição está associado a ela. Caso este sufixo respeite as
condições, e.g., palavra final ter no mínimo 3 letras, ele será removido. A segunda parte é em
relação às regras de transformação, em que de acordo com o resultado depois da primeira parte,
a palavra sofre mais uma transformação para reduzi-la ainda mais. O token "Rubbing" perde o
"ing" pela primeira parte do algoritmo, resultando em "Rubb". Depois perde um dos "b" finais pelas
regras de transformação na segunda parte do algoritmo. Alguns anos depois do algoritmo da Julie,
Martin F. Porter propôs uma outra forma de radicalizar os tokens num texto. No algoritmo de
Porter, existem 5 passos consecutivos e bem definidos para cumprir a tarefa. O programa dele
ficou tão famoso que foi localizado em diversas outras línguas naturais e implementado em outras
linguagens artificiais. Também existem métodos de stemming voltados a língua portuguesa. O
principal deles se chama Orengo, desenvolvido por Viviane Moreira Orengo, possuindo 199 regras
distribuídas em 8 passos. As técnicas de redução das palavras vistas até o momento funcionam
de uma forma parecida, truncando os tokens para uma forma mais enxuta. Estes algoritmos
podem deixar as palavras um pouco confusas de serem lidas por um usuário humano, então
porque não colocar os tokens em sua forma de dicionário? Pois para isso seria necessário criar
um dicionário específico para reduzir as palavras. Por exemplo as conjugações do verbo comer no
presente do indicativo, "como, comes, come, comemos, comeis, comem" seriam atribuídas à
forma no infinitivo "comer". De forma prática precisaria ser feito toda uma busca num dicionário
para ver onde o token se encontra só para poder resumir a palavra de uma forma um pouco mais
natural. Este processo de se transformar as palavras na sua forma infinitiva é chamado de
Lemmatization. Entretanto essa necessidade não se mostra muito útil na maioria dos casos, já
que os tokens servem como base de dados para outros algoritmos que buscam relações no texto,
e estes algoritmos não se importam com a forma que a palavra está. O que pode prejudicar num
processo de Stemming são as reduções que resultam na mesma grafia; caso duas palavras
diferentes são processadas para uma forma em que elas ficam iguais, perdeu-se informações que
não serão mais acessadas, podendo gerar falsos positivos ou negativos na análise do texto. A
grande dificuldade do método de stemming é criar um algoritmo análogo a uma função injetora,
em que o domínio (corpus) de palavras resulte numa imagem distinta entre seus elementos. 


A normalização das palavras é muito importante em diversos contextos. Por
exemplo, os motores de buscas (google, bing) não apresentam apenas resultados
que contenham a palavra exata digitada. O sistema retorna palavras parecidas,
com um radical semelhante.
O processo de stemming é baseado em uma heurística que corta as extremidades
das palavras afim de reduzí-las à algo próximo de seu radical. Desta forma,
palavras que tem o mesmo significado e são escritas ligeiramente diferentes,
são agregadas e tomadas como a mesma (amig representa tanto amigos quanto
amigão). Isso é feito apartir de um conjunto de regras e dicionários de
sufixos, portanto, um algoritmo para lingla inglesa não funcionaria nas outras
linguagens.
O algoritmo de Lovins foi o pioneiro da língua inglesa. Apesar de não ser muito
utilizado hoje em dia, é de grande importância para a computação, pois a
maioria dos algoritmos de stemming foram baseados neste algoritmo. O algoritmo
segue uma tendência gulosa e consegue rodar em tempo linear. Dado uma palavra e
seu maior sufixo possível (aquele que ignora apenas as duas letras iniciais,
pois poderiam ocorrer um stemming onde a palavra vira uma letra só, o que não
faz sentido), o programa procura numa tabela de sufixos (acompanhados de uma
condição) com o mesmo tamanho. Se o sufixo da palavra estiver na tabela de
sufixos e supre a condição necessária, o sufixo atual é cortado da palavra.
Caso o sufixo não esteja na tabela, seu tamanho é reduzido e o processo se
repete. Isso ocorre até um sucesso, ou então a palavra não pode ser reduzida
mais. Após a remoção de um sufixo, a palavra passa por um tratamento, que
retira letras dobradas do final e uma série de outras correções.
O programa mais famoso para a língua inglesa é o algoritmo de Porter. Que
funciona de maneira similar ao de Lovins, porém mais simples e completo que seu
antecessor.
O primeiro algoritmo de stemming para a língua portuguesa a ser amplamente
divulgado foi o algoritmo de Orengo, funcciona de maneira parecida com o de
Lovins, porém faz o uso de uma lista de exceções.
Uma outra forma de reduzir palavras poderia ser com base em um banco de dados
contendo todas as palavras e seus radicais, porém isso pode não oferecer uma
vantagem tão grande e, do ponto de vista computacional, custa muito mais caro.


Stemmming - Nas linguagens naturais em geral, as palavras possuem diversas
flexões, ou seja, cada palavra pode ser apresentada de diferentes formas. O
processo de normalização consiste na simplificação/redução/radicalização destas
palavras. Um dos processos de normalização é chamado stemming.
O stemming consiste em remover as extremidades das palavras para converte-las
para sua forma simplificada/radicalizada.
Um dos algoritmos pioneiros de stemmimng é o algoritmo de Lovins. Seu processo
é simples e consiste em apenas duas etapas.
1) Procurar pelo sufixo de maior tamanho na palavra e que satisfaz as condições
-> remover
2) Aplicar as regras de transformação no final da palavra. Estas regras devem
ser aplicadas independente de ter havido remoção ou não.
Existe outro algoritmo popular para stemming da lingua portuguesa. Este é
chamado de algoritmo de Porter. É mais completo e mais simples que o algoritmo
de Lovins e portanto, é o mais usado atualmente.
Existem algoritmos de stemming para o portugues também. É o caso do algoritmo
ORengo. Consiste em 199 regras distribuídas em 8 passos, além disso, é capaz de
considerar exceções.
Existem algumas considerações finais a serem ditas a respeito do processo de
setemming
!) Uso de um banco de dados: seria possível fazer o processo de stemming
através de um banco de dados. Ou seja, um banco onde cada flexão da palavra
faria referência ao radical. No entanto, esse procedimento iria requerer muito
processamento e, talvez, não valesse a pena.
2) Por que não eliminar prefixos? Não há motivos teóricos para desconsiderar a
eliminação de prefixos.
3)O algoritmo de stemming nao deve permitir a perda de muita informação
4)É possível medir o desempenho de um processo de stemming através de dois
parâmentros. Oversteming: quando é removido não só o sufixo, mas também uma
parte do radical. Understemming: quando o sufixo não é removido completamente.


A aula retoma o tema da normalização das palavras, tema importante para o
tratamento e interpretação geral de textos, uma vez que palavras de mesmo
sentido podem tomar diferentes formas, como flexões de gênero, grau ou número.
Buscadores online já se aproveitam de normalização para apresentar ao usuário
resultados semelhantes de alta relevância.
A radicalização de palavras se dá por duas técnicas mais importantes:
 stemming e lemmatization.
O stemming consiste em reduzir a palavra ao seu radical morfológico,
como gat para [gato, gata, gatos, gatas], através de remoções de afixos
derivacionais que dependem de linguagem. Geralmente pode ser representado por
conjuntos de regras, sempre atrelados à linguagem.
Já o lemmatization consiste em reduzir à sua forma básica, mas ainda legível e
semanticamente completa, como produzir para [produção, produziram, produtivo].
Para o stemming existem dois algoritmos principais: o de Lovins e o de Porter.
Lovins, uma linguista computacional de Chicago, criou um stemmer  para o idioma
inglês baseado na remoção de 294 sufixos condicionados por 29 diretivas e 34
regras de transformação, divididos em duas etapas simples, num algoritmo
linear. Sua execução consiste em remover o maior sufixo que satisfaça sua
condição restritiva e, então, aplicar a regra de maior nível hierárquico que
satisfaça os sufixos definidos. Isso torna o algoritmo simples, pois cada
palavra pode ser analisada individualmente.
Porter foi um bibliotecário que criou um stemmer inicialmente projetado para
recuperar as informações da biblioteca onde trabalhava. É um algoritmo mais
completo e mais simples do que o de Lovins, sendo o mais utilizado atualmente,
quase 40 anos depois de sua criação, em 1980. Contém 5 passos, com regras e
operações de validações.
Existem, também, algoritmos de stemming para o português, como o Orengo, criado
por Viviane Moreira em 2001 como projeto de doutorado na Middlesex University.
É constituído de 199 regras distribuídas por 8 passos, considerando uma lista
de exceções.
O algoritmo de Porter já foi modificado para atender vários idiomas, como o
próprio português, por ser eficiente.
Entretanto, existem algumas considerações sobre os stemmers:
- Utilização de um dicionário aumentaria eficácia, mas reduziria eficiência.
Vale a pena? Não
- Prefixos: porque não são considerados?
- Muita perda de informação?
- Overstemming e Understemming: não alcançar os objetivos ou ir além deles
- Nomes próprios
O lemmatization geralmente usa dicionários de palavras, com uma heurística mais
sofisticada.


Aula 4. Normalização de texto. A motivação para normalizar textos é poder
reconhecer uma mesma palavra escrita de forma diferente, como quando com letras
maiúsculas. Ou palavras de mesmo valor semântico mas flexionadas em gênero,
número e grau, por exemplo. Existem diferentes técnicas para normalização de
texto, mas as mais conhecidas são Stemming e Lemmatization. Stemming é uma
heurística dependente da linguagem que reduz a palavra à sua raiz.  A raiz em
questão não é a definição linguística, mas sim uma parte da palavra que carrega
o seu significado. Existem diferentes algoritmos de stemming, mas os mais
conhecidos são Lovins stemmer e Porter stemmer. Ambos funcionam de forma
similar, apenas com pequenas diferenças quanto as suas regras. O algoritmo de
Lovins é composto por duas etapas. A primeira localiza e remove o maior sufixo
entre os 294 possíveis, se existir, e aplica uma das 29 condições relacionada
ao sufixo removido. A segunda tenta aplicar uma das 34 regras de transformação,
se possível. O algoritmo de Porter é mais completo e mais simples do que o de
Lovins, sendo o stemmer mais utilizado atualmente. Possui 5 passos compostos de
aplicar uma regra e fazer uma operação de validação. O algoritmo de Porter e o
algoritmo de Lovins foram criados originalmente para o inglês. Orengo é um
stemmer para o português, criado em 2001. Possui 8 passos com 199 regras mais
uma lista de exceções. O algoritmo de Porter foi amplamente adaptado para
várias outras línguas como português, russo, espanhol, italiano etc. O website
<http://text-processing.com/demo/stem/> pode ser utilizado para verificar o
processo de stemming e outros algoritmos de normalização. Considerações sobre
stemmers. Poderia-se utilizar um dicionário para comparar as palavras
reduzidas, mas necessita de maior tempo computacional, fazendo o esforço não
valer a pena. Stemmers, em geral, removem o final de palavra, mas poderiam ser
adaptados para removerem prefixos também. São métodos heurísticos, podem causar
perda de informação para algumas palavras Pode acontecer overstemming, quando
uma parte importante do radical é removida, e understemming, quando o sufixo é
removido parcialmente. 


A normalização de texto é a busca de reduzir várias palavras com o mesmo
significado semântico para um único tipo de palavra. Na língua portuguesa, por
exemplo, as palavras normalmente diferem em tempos verbais, gênero e número,
sendo que no processo de normalização, todo o grupo de palavras é transformado
em um único elemento. Muitas vezes, para esse processo, realiza-se redução,
simplificação ou radicalização das palavras, como na técnica de stemming. Nesta
técnica, busca-se reduzir as palavras à sua raiz comum (gat representaria gato,
gata, gatos, etc.). De modo geral, stemming é uma técnica de heurística, em que
as extremidades das palavras são removidas, e mais algumas regras são
implementadas dependendo da língua do texto. A técnica difere de lemmatisation,
pois nesta técnica as palavras são reduzidas à sua forma básica, sendo
necessário um dicionário de palavras como base para o algoritmo.
O primeiro algoritmo de stemming foi criado em língua inglesa por Julie
Lovings, em 1968. O algoritmo é simples e possui apenas duas etapas, que são
realizadas no texto a partir de uma base de dados. Não é necessário um
dicionário de palavras, a redução pode ser realizada somente com as técnicas
estruturadas. Outro algoritmo importante para a língua inglesa é o de Martin
Porter, que também é o mais utilizado atualmente. Este é considerado mais
simples que o de Lovings, mas ambos têm como principal carcaterística cortar o
maior sufixo possível pré-determinado. Na língua portuguesa, o primeiro
algoritmo de radicalização publicado é o de Viviane Orengo, mas posteriormente
foram criados outros que utilizam regras diferentes. Porém, a ideia principal
dos stemmers é comum em todos os casos.
Algumas considerações foram realizadas a respeito do stemming. O uso de
dicionários não é realizado, pois requer uma capacidade computacional
desnecessária considerando que o resultado pode ser atingido com algoritmos
mais simples. A eliminação de prefixos não é realizada normalmente, apesar de
não haver justificativas teóricas para evitá-la. O interesse na prática do
stemming é bastante recente na literatura, especialmente após a criação dos
primeiros algoritmos. Além disso, algoritmos de stemming trazem o risco de
perda de informação, seja por overstemming (quando parte do radical é removida
em conjunto com o sufixo)  ou understemming (quando o sufixo é removido de
forma incompleta).
 
 


Nessa aula, vamos nos familiarizar melhor com os conceitos de "stemming" e
conhecer alguns algoritmos frequentemente usados nessa área, tais como "Loving"
e "Porter".
Da aula passada, lembramos que stemming é uma técnica de normatização das
palavras, que consiste em reduzir e/ou simplificar as palavras e transformá-las
em seu radical ou raíz. Por exemplo, "casarão, casebre, casinha, casamento,
casa" podem ser reduzidos para a raiz "cas". Outro exemplo: o grupo de palavras
"reflexo, refletido, refletir, reflexão" podem ser substituídos por '"refle".
No idioma inglês, temos alguns algoritmos famosos, como lovins e porter. O
primeiro consiste em cortarmos os possíveis sufixos de uma palavra de acordo
com o algoritmo, sempre respeitando as regras para cada corte. Essa etapa é
feita do maior sufixo para o menor (quantidade de letras), em que a palavra
resultante deve ter no mínimo duas letras. Para cada sufixo, o algoritmo
apresenta regras condicionais para a retirada, e caso a palavra não cumpra tal
condição, é preciso tentar retirar uma quantidade de letras imediatamente
menor. Nesse caso, são 29 condições.  Exemplo: National -> Nat (ional). Nesse
caso, a palavra raíz é "nat". No segundo (Porter), substituimos a terminação da
palavra resultante da primeira etapa de acordo com uma tabela do algaritmo.
Exemplo: matrix -> matric. Nesse caso, são 34 regras.
É importante ressaltar que esses algaritmos foram feitos para o idioma inglês e
portanto, para a lingua portuguesa seria preciso criar nosso próprio algoritmo,
visto que na nossa lingua temos ainda mais variações de grau, gênero, número,
família etc.


Sabemos que no português existe um problema da flexão das palavras, onde a
mesma palavra pode ser apresentada de diversas forma, para tentar solucionar
isso aprendemos uma técnicas Steamming e foi apresentado o conceito de
Lemmatization.
 
Lemmatisation conceito
 
O Lemmatisation ele utiliza um dicionário como apoio para reduzir a palavra até
a sua raiz, assim as suas reduções são melhores, porém o seu custo operacional
é alto.
 
Steamming
 
A ideia desse algoritmo é fazer uma heurística na tentativa de conseguir a raiz
da palavra sem precisar consultar um dicionário ( radicalização ), ou seja,
precisamos que ele seja rápido, pois podemos analisar um texto muito grande de
maneira rápida e temos indícios que a utilização de um dicionário externo quase
nunca vai compensar pelo seu custo computacional, pois teremos uma resposta
muito parecida sem ou com o dicionário.
Infelizmente nem tudo é perfeito um Stemmer pode apresentar duas falhas um:
Overstemming ( removeu mais trechos do que devia, assim perdemos informação do
radical ) e Understemming ( quando removê mais trechos do que devia ).
Para isso existem três algoritmos mais conhecidos que fazem essa tarefa: Lovins
stemmer, Porter stemmer, sample text e Orengo, no caso os três primeiros foram
idealizados em inglês, já o último em português. O algoritmo de Lovins é
composto por 249 sufixos, 29 condições, 34 regras de transformação e duas
etapas ( procurar o sufixo da palavra na lista de sufixos e verificar se esse
está na coerente, após isso aplica a transformação ), o mesmo foi feito em
1969.
O algoritmo de Porter veio em 1980, sendo mais eficiente na radicalização das
palavras, assim tornou mais popular, o início do algoritmo é primeiramente uma
busca em uma lista de sufixos, quando é encontrado ele passa os seguintes
parâmetros Id da regra,  o sufixo, a substituição, o tamanho da palavra do
sufixo e da substituição menos um e uma função de validação.
Na stemmer da Orengo temos 199 regras distribuídas por 8 passos e uma lista de
excepções, os passos são: A redução do plural para o singular, depois a troca
da palavra do feminino para o masculino, as reduções argumentativa, adverbiais
e substantivas, no próximo passo ele verifica se algum sufixo foi retirado,
caso sim ele só retira os acentos, caso não ele aplica a redução verbais, nesse
passo ele verifica novamente se algum sufixo foi retirado, caso sim ele retira
os acentos, caso não ele aplica a redução de vogais e a remoção dos acentos.

 


A revisão da aula anterior consistiu em uma explicação do conceito de
normalização de palavras e das técnicas mais importantes para esta tarefa. São
elas: stemming e lemmatization. 
Stemming consiste em podar e substituir sufixos de palavras até que cheguemos
as suas chamadas raízes. As regras para que estas podas e substituições ocorram
variam de acordo com a língua do texto inicial e com o algoritmo de Stemming
escolhido. 
A terminologia associada as técnicas foi apresentada. No caso de Stemming,
"stem" é uma parte de uma palavra, e "stemmer" é o artefato, ou seja, algoritmo
que utilizado para efetuar a normalização das palavras de um texto.
Analogamente, para a Lemmatization, "lemma" é a forma básica de uma palavra (o
algoritmo consiste em reduzi-las a "lemmas"), e o artefato é chamado
"lemmatizer".
Existem diferentes tipos de algoritmos de stemming. Dois dos mais populares são
o algoritmo de Lovins e o de Porter. O mais antigo e pioneiro na área foi o de
Lovins. Publicado em 1968 por Julie Beth Lovins, ele é composto por 294
sufixos, 29 condições, 34 regras de transformação e consiste em apenas 2
etapas, portanto é rápido.
O algoritmo de Lovins consiste em procurar primeiro pelo sufixo de maior
tamanho que satisfaz sua respectiva condição (vide tabela) em uma palavra,
depois removê-lo e aplicar a regra de transformação associada aos últimos
caracteres da palavra com o sufixo removido (caso haja alguma). Fazendo isso
para todas as palavras no texto, todo o seu conteúdo será normalizado.
O algoritmo de Porter foi publicado em 1980 por Martin Porter. Ambos são muito
parecidos nos sentidos de que não requerem conhecimento prévio sobre o idioma
do texto e trabalham removendo sufixos das palavras nele contidas, mas o de
Porter é mais completo e simples. É o stemmer mais popular até a presente data.
Ele possui validações de casos específicos em que o algoritmo de Lovins
incorria na perda de informação útil das palavras.
Um algoritmo de stemming para o português foi apresentado. Ele se chama Orengo,
foi publicado em 2001 por Viviane Moreira Orengo e conta com 199 regras
distribuídas por 8 passos. Uma tabela com algoritmos de stemming para outras
línguas foi mostrada, mas sem entrar em detalhes.
Por fim, começou-se a explicar com mais profundidade o processo de de
lemmatization, que consiste na aplicação de técnicas para remover a flexão das
palavras flexionadas do texto, de forma a reduzi-la a sua forma no dicionário
disponível.


Para buscar um conjunto de palavras que representam um mesmo termo, exemplo:
controlador, controladores, controle; digital, digitais; é necessário
normalizar as palavras executando a radicalização, ou seja, eliminar flexão em
gênero, número ou grau, e conjugação.
A técnica do stemming reduz a palavra à sua raiz, efetuando uma poda no afixo
da palavra, e depende da linguagem. A palavra resultante (stem) pode não ser em
si uma palavra com significado.
A técnica do lemmatization reduz as palavras para uma forma básica mas que em
si é uma palavra com significado.
Existem vários algoritmos que efetuam stemming, principalmente para o idioma
inglês. O algoritmo de Julie Lovins é um algoritmo pioneiro. Nele são definidos
294 sufixos, 29 condições e 34 regras de transformação que são aplicadas em 2
etapas. O algoritmo de Martin Porter também elimina sufixos, porém é mais
completo e mais simples, e é o mais utilizado atualmente.
O algoritmo de Viviane Orengo foi o primeiro algoritmo de stemming para a
língua portuguesa. Possui 199 regras efetuadas em 8 passos.
A linguagem Python possui a biblioteca NLTK para stemming e lemmatization.
Uma forma de melhorar o resultado dos algoritmos seria comparar o texto
produzido com um dicionário do idioma, entretanto isto requereria mais tempo de
processamento. Usar o dicionário para corrigir as palavras podadas seria o
lemmatization. Os algoritmos apresentados eliminam sufixos mas poderiam (ou
deveriam) eliminar também prefixos. Podem ocorrer o overstemming ou
understemming, portanto os algoritmos de stemming, buscando evitar a perda de
informação, evitam principalmente o overstemming com as regras que determinam o
tamanho mínimo do stem (palavra podada) e também exigindo que permaneça pelo
menos uma vogal.


Um algoritmo de stemming é um procedimento computacional que reduz todas as
palavras com a mesma raiz ou a mesma baste, para uma forma comum, geralmente
removendo cada palavra de seus sufixos derivativos e inflexão. Pesquisadores em
muitas áreas de linguística computacional e recuperação de informações acham
isso um passo desejável, mas por razões variadas. Na análise morfológica
automatizada, a raiz de uma palavra pode ser de interesse menos imediato do que
seus sufixos, que podem ser utilizados como pistas para a estrutura gramatical.
No outro extremo, sufixos são encontrados podem ser subsidiários do problema de
removê-los consistentemente o suficiente para obter conjuntos de bastes
exatamente correspondentes. Embora a forma do algoritmo varie de acordo com sua
aplicação, certos problemas linguísticos são comuns a qualquer procedimento
derivado.
O Lovins stemmer possui 294 terminações, 29 condições e 35 regras de
transformação. Cada final está associado a uma das condições. No primeiro
passo, é encontrado o final mais longo que satisfaz suas condições associadas e
é removido. Na segunda etapa, as 35 regras são aplicadas para transformar o
final. O segundo passo é feito se um final é removido ou não no primeiro passo.
As regras no algoritmo de Porter são separadas em cinco fases distintas
numeradas de 1 a 5. São aplicadas às palavras no texto a partir da fase 1 e
passando para a fase 5. Além disso, elas são aplicadas sequencialmente uma após
a outra como comandos em um programa. Assim, no imediatamente seguinte,
especificamos o algoritmo Porter em uma linguagem pseudo-programação cujos
comandos assumem a forma de regras para a substituição do sufixo.
O algoritmo de Orengo está baseado em regras para a remoção de sufixos. No
total, apresenta 199 delas. Contudo, as regras apresentam exceções, permitindo
uma redução em termos de overstemming (quando parte do radical é removida pelo
algoritmo) de 5% (Orengo 2004: 49 e 51). Pode-se dizer, então, que o algoritmo
é um misto das duas alternativas apresentadas por Russell e Norvig.


No português por exemplo, as palavras podem ter flexão em número, grua ou
gênero, conjugações verbais entre outras variações.
A normalização das palavras é a redução ou radicalização da palavra. Existem
duas técnicas de normalização:
Stemming - Stem é parte de uma palavra, geralmente são heurísiticas que
eliminam as extremidades da palavra, geralmente os sufixos, regras. Não é
considera a classe gramatical da palavra reduzida.
Lemmanization - ação de reduzir a palavra em um lemma forma básica.
Existem muitos algoritmos de Stemming, geralmente são desenvolvidos por
linguistas, principalmente para a língua inglesa.
Um desses algoritmos é o Algoritmo de Lovins, pioneira no ramo. As heurísiticas
são compostos por 294 sufixos, 29 condições e 34 regras de transformação. O
processamento utiliza apenas 2 etapas. A primeira etapa consiste em remover o
maio sufixo possível com o radical tendo ao menos dois caracteres, esteja na
lista pré-estabelecida e cumpra a condição associada a ela. A segunda etapa
consiste em aplicar uma transformação no radical resultante da etapa 1, desde
que se enquadre em uma das regras pré-estabelecidas.
O algoritmo mais usado e famoso é o Algoritmo de Porter. É considerado um
algoritmo mais completo e mais simples que da Julie Lovins. Utiliza cinco
etapas, similar ao da Julie Lovins a etapa inicial consistem em remover o maior
sufixo desde que o radical possua 3 caracteres.
O primeiro algoritmo de radicalização para o português, amplamente divulgado,
foi desenvolvido por Viviane Moreira Orengo. Utiliza 199 regras distribuídas em
8 passos.
A não remoção de prefixos não possui nenhum embasamento teórico apesar de não
ser feito.
Caso a heurística não seja bem elaborada é comum ocorrer perda de informações
ou não ser otimizada ao realizar a radicalização. Existem duas métricas para
medir esse desempenho:
Overstemming quando parte do radical é removido;
Understemming quando o sufixo não é removido ou removido parcialmente.
Lemmatisation é o processo de deflexão das palavras, sua heurística costuma ser
mais sofisticada.


Em textos da língua portuguesa temos diferentes palavras flexionadas em gênero,
número ou grau, além de inúmeros tempos verbais distintos. A normalização de
palavras pode ser entendida como a redução, a simplificação ou a radicalização
de palavras. Para isso temos duas técnicas importantes: Stemming e
Lemmatization. O processo de stemming consiste em reduzir a palavra à sua raiz
(sem levar em conta a classe gramatical),  não é a raiz da palavra e a palavra
pode ficar sem significado. 
Stemming (a ação de reduzir em stems)                         X              
 Lemmatization (a ação de reduzir em Lemmas)
Stem: Parte de uma palavra                                            X       
        Lemma: Forma básica da palavra
Stemmer: O artefato (programa)                                     X           
    Lemmatizer: O artefato (programa)
Existem diferentes algoritmos como por exemplo: Lovins, Porter, Orengo.
O algorítimo de Lovins foi o primeiro e é composto por 294 sufixos, 29
condições e 34 regras de transformação com um processamento de apenas 2 etapas.
A primeira é procurar pelo sufixo de maior tamanho e removê-lo de acordo com as
condições de cada sufixo, a segunda é aplicar a regra de transformação de final
correspondente com o que sobrou da palavra. O algorítimo de Porters é um pouco
mais longo, mas é mais simples e é o stemmer mais utilizado atualmente. Todos
os 5 passos foram bem definidos: Usa-se regras e operação de validação. Cada
palavra é comparada sequencialmente com o maior sufixo possível e então removê-
lo.


Nessa aula foi exposto os conceitos de Stemming e Lemmanization que pertencem a
formas de normalização de texto. Por razões gramaticais, os documentos vão usar
diferentes formas de uma palavra, como organiza, organiza e organiza. Além
disso, existem famílias de palavras derivadas relacionadas com significados
semelhantes, como democracia, democracia e democratização. Em muitas situações,
parece que seria útil para uma busca por uma dessas palavras retornar
documentos que contenham outra palavra no conjunto.
O objetivo de ambos e de lema é reduzir formas inflexórias e, às vezes, formas
relacionadas a derivação de uma palavra para uma base comum. Por exemplo:

    Estou, são, é => ser
    carro, carros, carros, carros => carro

O resultado desse mapeamento de texto será algo como:

    Os carros do menino são cores diferentes => o carro do garoto é de cor
diferente

No entanto, as duas palavras diferem em sua primitiva. Âncora Stemming
geralmente se refere a um processo heurístico bruto que corta as extremidades
das palavras com a esperança de alcançar este objetivo corretamente a maior
parte do tempo, e muitas vezes inclui a remoção de afiamentos derivativos.
Âcora Lemmatização geralmente se refere a fazer as coisas corretamente com o
uso de um vocabulário e análise morfológica de palavras, normalmente com o
objetivo de remover apenas finais de inflexão e retornar a base ou a forma de
dicionário de uma palavra, que é conhecida como o lema Âncora. Se confrontado
com a serra de token, o retardo pode retornar apenas s, enquanto a lemmatização
tentaria retornar ver ou vir, dependendo se o uso do token era como um verbo ou
um substantivo. Os dois também podem diferir em que a maioria das vezes colapsa
palavras relacionadas derivadamente, enquanto que a lema geralmente apenas
colapsa as diferentes formas de inflexão de um lema. O processamento
lingüístico para derrubar ou lematizar é muitas vezes feito por um componente
de plug-in adicional ao processo de indexação, e existem vários desses
componentes, tanto comerciais como de código aberto. Ânnora O algoritmo mais
comum para frustrar o inglês, e um que repetidamente demonstrou ser
empiricamente muito efetivo, é o algoritmo de Âncora Porter (Porter, 1980).
Todo o algoritmo é muito longo e intrincado para apresentar aqui, mas
indicaremos sua natureza geral. O algoritmo de Porter consiste em 5 fases de
reduções de palavras, aplicadas sequencialmente.
No entanto, a forma talhada exata não importa, apenas as classes de
equivalência que ela forma.
Ao invés de usar um stemmer, você pode usar um lemmatizador, uma ferramenta de
PLN que faz análises morfológicas completas para identificar com precisão o
lema de cada palavra. Fazer análises morfológicas completas produz, no máximo,
benefícios muito modestos para recuperação. É difícil dizer mais, porque
qualquer uma das formas de normalização tende a não melhorar o desempenho de
recuperação de informações em inglês em conjunto - pelo menos não por muito.
Embora ajude muito para algumas consultas, também dói muito o desempenho para
os outros. O lançamento aumenta o recall ao mesmo tempo que prejudica a
precisão. Como um exemplo do que pode dar errado, observe que o porteter
stemmer deriva de todas as seguintes palavras:
"operar operacional opera operação operativa operacional operacional" para
operar. No entanto, uma vez que opera em suas várias formas é um verbo comum,
esperamos perder uma precisão considerável em consultas, como as seguintes com
o carregamento de Porter:

    operacional e de pesquisa
    operação e sistema
    operatório e odontológico

Para um caso como este, mover-se para usar um lemmatizador não resolveria
completamente o problema, porque formas particulares de inflexão são usadas em
colocações particulares: uma frase com as palavras operam e o sistema não é uma
boa combinação para o funcionamento e sistema da consulta. Obter melhor valor
da normalização do termo depende mais de questões pragmáticas de uso de
palavras do que em questões formais de morfologia lingüística.Cokmo os
algoritmos foram baseados no idioma inglês  foi necessário uma construção para
o Português e é ai que entra o Algoritmo de Orengo.


Complementando a aula passada, será um pouco mais aprofundado na parte de
normalização de palavras, por ter um potencial como em sistemas de buscas que
deveria ser capaz de encontrar documentos com diferentes nomes assim aumentando
o alcance de buscas e dando mais possibilidades de encontrar o que o usuário
busca.
Das técnicas vistas, iremos falar mais sobre o stemming, que se refere a um
sistema de heurística que corta palavras, normalmente sufixos e as altera para
uma forma mais básica da palavra, mas não necessariamente sua raiz.
Temos dois algoritmos mais famosos, para o idioma inglês, o de Lovins (1968) e
de Porter (1980), sendo o primeiro mais importante historicamente, pois foi o
pioneiro entre os stemmers (o código de stemming) e a tese de doutorado de sua
criadora, Julie Beth Lovins, composto de apenas 2 fases, na qual a primeira
corta o sufixo baseando-se em 294 sufixos contendo 29 condições (é uma busca
gulosa que procura pelo maior sufixo e o remove, se as condições forem
cumpridas), e depois 34 regras que podem, ou não alterar o fim da palavra para
gerar uma palavra mais "genérica". Agora o algoritmo de Porter foi criado a
para um relatório de recuperação de informação por um bibliotecário, Martin F.
Porter, sendo mais completo e mais "simples" que o de Lovins é também o mais
famoso e usado até mesmo hoje em dia, composto de 5 etapas, em resumo ele é bem
parecido com o de Lovins, casa o maior sufixo, remove, procura se é necessário
alguma alteração no final da palavra e checa se há alguma vogal restante na
palavra, o código de Porter (atualizado) e todas as suas implementações em
linguagens de programação estão disponíveis no site do próprio autor:
https://tartarus.org/martin/PorterStemmer
Há uma versão de stemming para o Português criado por Viviane Moreira Orengo,
composto por 199 regras e 8 passos, primeiro remove plural, remove modificação
para o feminino, depois remove todo e qualquer alteração de aumentativo,
advérbio, substantivo e verbo e no final, remove os acentos (essa parte é meio
controversa e pode ser considerada desnecessária/precisa ser alterada).
Considerações finais
Por que não usar um dicionário para o stemming?
Porque faz com que o algoritmo consuma muito mais tempo, e todo o esforço e
tempo a mais investido pode não ter um retorno que valha a pena.
Stemming para prefixos?
Não há nenhum motivo teórico para não fazê-lo, mas ninguém ainda o fez.
Stemming perde detalhe e informação?
Talvez, um algoritmo bem codificado não deveria permitir perda de informações,
mas alguns detalhes podem ser perdidos.
Stemming para nomes?
Possível Projeto Final?


A quarta aula da disciplina focou em um dos últimos tópicos abordados na aula
anterior, a técnica de normalização de textos chamada Stemming. Um stem é parte
de uma palavra e stemming é a ação de reduzir essa palavra. Utilizamos essa
técnica para encontrar palavras parecidas e isso é possível através da
radicalização delas. Nessa método, são cortados os sufixos, elementos que
adicionados à um radical formam uma nova palavra, a língua portuguesa possui
três tipos de sufixos: nominal, verbal e adverbial. Alguns exemplos de radical
e suas palavras derivadas são: folha - florescer, florear; amor - amar, amável.
O algoritmo deve ser capaz de identificar quais são os sufixos da palavra e
reduzi-la à ela. O algoritmo pioneiro para a língua inglesa foi o algoritmo de
Lovins, que foi uma linguista computacional, formada na Universidade Brown.
O Algoritmo de Lovins possui apenas 2 etapas, ele primeiro busca o sufixo de
maior tamanho e que satisfaça uma das 29 condições criadas e o remove caso sim.
Depois disso, se o sufixo é removido ou não, uma das 34 regras de transformação
são aplicadas para mudar o final. Apesar de possuir menos passos, o stemmer
mais utilizado para a língua inglesa atualmente é o do Martin Porter, por ser
mais simples mesmo possuindo mais passos.
Em 2001, foi divulgado um algoritmo de radicalização para a língua portuguesa.
Ele está baseado em 199 regras que têm como intuito a remoção de sufixos
através de 8 passos. Foi desenvolvido por Viviane Moreira Orengo e por isso
leva seu nome, sendo conhecido como Orengo. Esse tipo de algoritmo é
considerado uma heurística e portanto sua implementação e otimização dependem
das regras adotadas para cada língua.


Normalização de texto: Steeming
Um sistema de busca deve permitir que documentos indexados com diferentes nomes
sejam recuperados usando quaisquer das suas formas de escrita. A normalização
de palavras pode ser entendida como a redução ou a simplificação ou a
radicalização de palavras, o que pode facilitar essa busca.
Existem duas técnicas principais: Steeming e Lemmatization.
 
Steeming
    O processo de steeming consiste na redução das palavras à sua raiz, sem
levar em conta a classe gramatical. Ex: amig: amigo, amigos, amiga, amigas,
amigão, amiguinho
    Geralmente refere-se a um processo de heurística que corta as extremidades
das palavras, inclui frequentemente a remoção de afixos derivacionais e pode
ser representado por um conjunto de regras que depende da linguagem.
    O resultado não necessariamente é a raiz na forma linguística e pode não
ter significado.
 
Algoritmos de Steeming
    Steeming pode ser entendido como a redução das palavras em stems.
        Stem ? parte de uma palavra.
        Stemmer ? O artefato (programa)
       
    Existem diferentes algoritmos, principalmente para o inglês.
 
Algoritmo de Lovins
    Algoritmo pioneiro influenciador de muitos stemmers.
    Desenvolvido por Julie Beth Lovins em 1968 é composto por 294 sufixos, 29
condições e 34 regras. O processamento é rápido, feito em apenas duas etapas:
    I  Procurar pelo sufixo de maior tamanho na palavra e que satisfaz as
condições, então remover.
    II  As regras são aplicadas para transformar o final se um sufixo é
removido na primeira etapa.
 
Algoritmo de Porter
    Algoritmo mais completo e mais simples que o de Lovins.
    Desenvolvido por Martin F. Porter em 1980, é o algoritmo de stemming mais
usado atualmente. Tem 5 passos bem definidos usando regras de operação e
validação. É similar ao de Lovins, cada palavra é sequencialmente comparada com
o maior sufixo possível, se casou o sufixo então removê-lo da palavra.
   
Algoritmo de Orengo
    Publicado por Viviane Moreira Orengo em 2001 foi a primeira versão de um
algoritmo amplamente divulgado para a língua portuguesa. É constituido por 199
regras distribuídas por 8 passos e considera uma lista de exceções.


Introdução à linguagem Python; sintaxe e orientação a objetos. 
Implementação de um stemmer rudimentar, que reduz todas as palavras às suas
três primeiras letras; é simples, mas leva ao over-stemming que resulta na
perda de significado de algumas palavras, por remover parte de sua raiz além de
seu sufixo. Implementação do stemmer de Porter. Análise do resultado dos dois
Stemmers. 
Visualização de PLN utilizando grafos direcionados envolvendo duplas de
palavras adjacentes no texto. Visualização dos grafos gerados com e sem
stopwords. Maneiras pelas quais o uso de grafos pode auxiliar na análise de
textos: sumarização, uso de medidas relacionadas a topologia. 
Distância de edição refere-se a quantidade de operações em caracteres precisam
ser efetuadas numa palavra para transformá-la em outra. A distância de Hamming
diz respeito à quantidade de diferenças em strings de tamanho igual. A
distância de Levenshtein refere-se a maneira de converter uma palavra em outra
através de inserções, remoções e substituições de caracteres. 
Snowball é um framework para a escrita de algoritmos de stemming. 


Essa aula foi nossa primeira aula prática. Iniciamos com uma breve introdução
sobre python e as facilidades presentes na estrutura, tais como a fácil
manipulação de dados em uma lista. Considerações sobre a linguagem mais
utilizada na área foram feitas, em especial destacamos o sistema de
interpretação baseado em bytecodes (pyc) e posterior compilação por uma máquina
virtual em instruções de máquina. Assim como Java, temos aqui um grande caixa
que garante a fácil portabilidade de código em múltiplos sistemas. Quando
estamos lidando com sistemas, essa passeabilidade é fundamental no
desenvolvimento e reprodução dos algoritmos. Assim como Java, também podemos
ter funções com desempenho inferior ao esperado, pensando nisso, uma grande
vantagem da linguagem consiste em um conjunto de rotinas encapsuladas em C ou
C++. Geralmente, as rotinas mais fundamentais estão presentes em bibliotecas
nativas, no entanto, principalmente na área de aprendizado de máquina, temos
também grande repositórios guarda-chuva que democratizam e com massiva
validação de métodos da área, e.g., numpy, sklearning. O exemplo ilustrado para
introduzir python foi o quicksort, realizando uma ordenação com poucas linhas
de códigos. O sistema de estrutura de dados também foi apresentado, mostrando
como construir uma classe e um objeto na linguagem.
Recapitulando os conceitos da última aula, introduzimos de maneira mais ingênua
um método radicalização de palavras (Stemmer). O primeiro Stemmer escolhido foi
a redução de toda palavra somente aos três caracteres iniciais, aparentemente,
há um resultado razoável, mas olhando com mais cuidado temos um alto over-
stemming associado. Em outras palavras, o nosso método simplifica
demasiadamente e associa palavras que não possuem a mesma radicação. O método
fool-stemmer serve de um bom exemplo para exemplificar a fácil manipulação de
dados. Seguindo a literatura, a forma de avaliar métodos de radicalização,
entre os conhecidos podemos citar quatro medidas: Overstemming, Understeming,
Stemming Weight e Error Rate Relative to Truncation. Essa quatro medidas são
bons parâmetros para indicar a eficiência do nosso método. Seguindo, temos o
método de Porter (1980), brevemente estudado na aula anterior, esse método mais
simples se comparado com método de Julie, mas amplamente utilizado na
literatura.
Respondendo à pergunta desafio: Snowball é uma pequena linguagem de manipulação
de dados. Seu nome é um tributo a linguagem de programação SNOBOL. A
dedicatória ocorre pois a Snowball possui o mesmo conceito de padrões de string
entregando sinais para as instruções do código. O tipo básico de data nessa
linguagem são string de caracteres, inteiros e booleanos.
Conforme comentado já na aula anterior, tanto o algoritmo de Porters e o nosso
algoritmo tolo possui uma abordagem baseada em truncamento, regras para
sumarização. Há também métodos com mesmo objetivo que são baseados em uma
abordagem estatística ou misturada. Podemos citar o método de N-Grams dentre os
mais conhecidos.
Um abordagem explorada em sala consiste em definir um grafo para analisar os
padrões de formação presentes no texto analisado. Observamos aqui a necessidade
de eliminarmos stopwords, pois alguns tipos de palavras se repetem com uma
frequência acima das demais. Como estamos lidando com uma abordagem
estatística, repetições interferem de forma direta no resultado esperado.
Ainda na abordagem de grafos temos que uma rede de co-ocorrência possibilita a
realização da estrutura topológica nos grafos ao comparar os padrões de
formações presentes. Passando para o próximo assunto, assim como temos formas
de normalizar palavras, temos também uma abordagem para verificar a distância
entre elas. De forma grosseira, definimos como distância o quão similar uma
palavra está da outra, uma aplicação cotidiana que usa isso é o corretor
ortográfico. Definimos como palavras próximas aquelas que possuem uma distância
mínima entre si, e definimos distância como o número de operações necessárias
para uma palavra se transformar na outra. Esse conceito é amplamente empregado
na comparação de uma sequência de aminoácidos, pois estamos interessados em
saber se uma sequência está próxima uma da outra, se há uma lacuna e onde está
diferente.
Há uma variedade de tipos de operação à serem realizadas em duas palavras,
inicialmente nós nos atentamos somente a três tipos: Inserção, Deletar e
Substituição. Vamos supor que estamos tentando verificar o quão distante está a
palavra INTENTION e EXECUTION, podemos perceber que ao realizar as seguintes
operações: deletar primeira letra de Intention, substituir E e Execution por
N, substituir o S por X, o U por N e inserimos C entre INTE*NTION,
chegamos em um casamento de string. Se considerarmos o peso para cada operação
como 1, temos então que as palavras estão à uma distância 5, se levarmos em
conta a substituição à custo 2 (operação de maior custo computacional) temos 8
como distância (Distância de Levenshtein). Esse tipo de aplicação é muito usada
em Machine Translation para avaliar o quão bom o método está traduzindo uma
sentença.  


A aula fora prática em laboratório, aplicando conhecimentos adquiridos sobre
stemmer, grafos de palavras e distância de edição, na linguagem python. Antes
de partirmos para a parte de processamento de linguagens naturais (PLN) em si,
testamos uma primeira função recursiva para ambientalizarmos com a python e
testamos o poder das classes e do construtor. Em linhas gerais python é uma
linguagem interpretada orientada a objeto e que é compilada em uma máquina
virtual em intruções de linguagem de máquina, sendo que a utilização do mesmo é
fácil e eficiente para sistemas experts de PLN, além de conter diversas
bibliotecas de código aberto na área de PLN que facilita o desenvolvimento
nesta área (mais informações podem ser acessadas no site https://
www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/1/
ch01lvl1sec10/advantages-of-togetherness---nlp-and-python e com o livro Python
Natural Language Processing, que cobre desde a introdução de PLN, indo até deep
learning e ferramentas avançadas). 
Indo para a parte prática começamos a mexer com o stemmer. Relembrando a
definição do processo de stemmer, em suma, o stemmer possui como objetivo
reduzir as palavras ao seu radical, retirando seus sufixos de acordo com um
conjunto de regras aplicadas. O primeiro programa testado fora um programa não
eficiente de stemmer e limitado, sendo testado em textos disponibilizados pelo
professor. O problema deste programa é que a sua redução não leva em conta o
contexto e nem muitas regras, reduzindo palavras a um ponto que até mesmo
palavras que são totalmente diferentes e possuem significados diferentes, ao
serem reduzidas por esse stemmer, ficaram com a mesma redução, isto é, houve o
procedimento de over-stemming (acontece quando não é removido apenas o sufixo,
mas também uma parte importante do radical, diminuindo a precisão do
algoritmo). O próximo algoritmo interessante que estudamos fora o altoritmo de
Porter, mais completo e mais simples do que o algoritmo de Lovins, levando em
conta regras e evitando overstemming (além de poder ser aplicado em diversas
linguagens, como o Holandês, apresentado no artigo de "Porters stemming
algorithm for Dutch" (Kraaij and Pohlmann, 1994)).
O termo Snowball está associado a ser uma linguagem para algoritmos de stemming
(Porter, 2001). Sua criação foi necessária para definir exatamente stemmers, e
por mais algumas razões, como a falta de algoritmos de stemming prontamente
disponíveis para outras línguas que não o inglês. Em Snowball as regras de
algoritmos de stemming podem ser expressas de forma natural. Snowball é
bastante pequeno e pode ser aprendido por um programador experiente em
aproximadamente uma hora. Para visualizar algoritmos e realizar downloads, além
de aprender procedimentos, o Snowball encontra-se no site http://
snowballstem.org/. Em suma, retirado da própria descrição do site, Snowball é
uma pequena linguagem de processamento de string projetada para criar
algoritmos de processamento para uso em Recuperação de Informações, e no site
apresentado há vários stemmers úteis que foram implementados usando Snowball.
Há scripts disponíveis em ISO C, Java, Python, Rust e Go, além de, existir
pacotes em outras linguagens, por exemplo, na linguagem R possui o pacote
Snowball para utilizar suas tarefas (sua documentação está disponível em https:
//cran.r-project.org/web/packages/SnowballC/SnowballC.pdf).
Há diversas abordagens para stemming, e uma aplicação são redes (grafo) de
palavras. Testando essa aplicação, vimos um algoritmo que criava um grafo
direcionado com a frequência dos pares de palavras. Para visualizarmos o grafo
formado tivemos acesso ao site https://dreampuf.github.io/GraphvizOnline/,
possuindo diversas formas de visualização e montagem do grafo e diversos
formatos para salvar o grafo formado. Para testarmos conhecimentos adquiridos
anteriormente, foi pedido para modificar o programa de forma que ele criasse um
grafo de palavras sem as stopwords, considerando que anteriomente ele utilizava
as stopwords para essa contagem de palavras e consequentemente para a montagem
do grafo, mas não p?ecisando fazer muitas alterações para que essa tarefa fosse
completada. A utilidade de aplicação de redes de palavras com grafos é, por
exemplo, visualizar a co-ocorrencia de palavras, nos auxiliando a identificar
relações entre elas, podemos utilizar as propriedades topológicas para
descoberta de conhecimentos do texto, entre outras diversas aplicações (alguns
trabalhos interessantes que ilustram essas aplicações são "KeyGraph: Automatic
indexing by co-occurrence graph based on building construction metaphor"
(Ohsawa, Benson and Yachida, 1998) e "Text authorship identified using the
dynamics of word co-occurrence networks" (Akimushkin, Amancio e Oliveira,
2017)).
Podemos utilizar essas técnicas também para casamentos entre strings, sendo
aplicável em diversas áreas, como em correções ortográficas e biologia
computacional (sequências de aminoácidos). O próximo assunto que vimos a
aplicação fora a distância de Hamming, em que é utilizado para detecção de
erros em transmissões binárias de comp?imento fixo, isto é, ele é a quantidade
de bits utilizados de uma transmissão para a recepção. Testamos sua parte
prática para ver, por exemplo, quantas letras são diferentes em duas palavras
quando comparadas. Outra distância bastante utilizada é a distância de
Levenshtein, na qual mede a quantidade de diferenças entre duas strings, com
operaçõe sde inserção, exclusão e substituição, definindo o número mínimo de
edições para transformar uma string em outra string, sendo testado também em
aula.


Nessa aula de laboratório foram apresentados  os conceitos do Python além de
exemplos nessa linguagem de: Stemmer, grafos de palavras e distância de edição.
 
O Python possui um sistema de tipo dinâmico e gerenciamento automático de
memória. Ele suporta múltiplos paradigmas de programação, incluindo orientado a
objetos, imperativo, funcional e processual, além de possuir uma ampla e
abrangente biblioteca padrão. Python é primeiro interpretado em bytecodes (pyc)
e depois é compilado por uma máquina virtual em instruções de linguagem máquina
(geralmente c). Esse é um dos motivos dele ser um pouco lento, mas podem ser
utilizadas funções implementadas em C/C++ para melhorar o desempenho
computacional.
 
O primeiro exemplo mostrado na linguagem foi o da função someGreatFunction,
essa função recebe um vetor e ordena os elementos de forma não decrescente.
Vale notar que essa função não utiliza algum algoritmo de ordenação classico
como o mergesort ou insertionsort de forma direta, apenas faz comparações entre
o começo, o meio e o fim do vetor.
 
Em seguida, foi apresentado o algoritmo fool-stemmer.py. Esse algoritmo é um
stemmer que pode ser aplicado em arquivos de texto no formato "txt". Além do
stemmer das palavras ele aplica a padronização do texto em minúsculas e
percebe-se que pode ocorrer over-stemming, ou seja, uma parte do radical é
removida, não só o sufixo
 
O algoritmo porter-stemmer.py é uma versão melhorada do algoritmo de Martin F.
Porter, que utiliza uma função stem. Foi comparado a saida dos algoritmos fool-
stemmer.py e porter-stemmer.py
 
No contexto de algoritmos de stemming, Snowball é um pequeno processamento de
string projetado para criar algoritmos de stemming usados em recuperação de
informações.
 
Na parte prática foram utilizadas redes de palavras (grafos), mostrando que
duas palavras estão ligadas se ambas estão adjacentes em uma frase. Foi
solicitado a alteração do algoritmo para a criação de um grafo de palavras sem
as stopwords.
 
Foi explicado sobre as redes de co-ocorrencia de palavras (também com a
utilização de grafos), que permitem identificar, de forma visual, relações
potenciais entre palavras. Abordou-se também o conceito de string matching.
 
Por fim, foi abordado a questão de distância entre palavras, comparando e
utilizando os conceitos de Distância de Hamming (quantidade de bits) e a 
Distância de Levenshtein (a quantidade de diferenças ou mudanças entre as
palavras)


Na primeira aula de laboratório de processamento de linguagem natural, tivemos
como objetivo explorar os conceitos vistos em teoria de uma forma mais
dinâmica. Começamos com um algoritmo de stemming que funciona truncando todas
as palavras do texto para um formato só com 3 letras, reduzindo drasticamente o
tamanho do texto. O problema desse tipo de algoritmo é que as palavras são
reduzidas ao extremo, perdendo muita informação, já que palavras distintas
podem ter as 3 primeiras letras iguais, mas todas as outras diferentes. Este
tipo de erro é chamado de over-stemming. Após o uso deste algoritmo de stemming
que funciona de uma forma não muito inteligente, passamos a utilizar uma versão
do algoritmo de Porter. Neste algoritmo é feita uma análise das palavras para
podermos trunca-la de um modo mais inteligente. Assim, reduzimos problemas como
over-stemming e under-stemming. Caso seja necessário avaliar casos diferentes
do algoritmo criado pelo Porter, é possível usar um framework que ele criou
chamada Snowball. O objetivo desta framework é criar algoritmos de stemming
diferenciados, voltado a textos muito específicos ou outras linguagens, em que
é necessário criar ou apagar regras. Outro ponto importante da aula, foi
conseguir perceber de uma forma mais visual a frequência e a ligação de
palavras vizinhas. Para isso foi criado um algoritmo que calcula a frequência
de pares de palavras num texto. Com isto conseguimos interpretar os dados em um
grafo; as palavras sendo os nós, um par de palavras como as arestas e a
frequência como o peso das arestas. Montando o grafo de um texto literário
grande o suficiente, conseguimos perceber uma grande relação de conjunções,
artigos e preposições com quase todas as outras palavras, formando um grafo bem
conectado e com grandes clusters. Como vimos nas aulas passadas, estas palavras
são chamadas de stopwords e não precisamos considerá-las num texto como esse. E
foi justamente o que fizemos, rodamos um algoritmo com uma lista de stopwords e
retiramos todas as que combinavam. O resultado foi um grafo bem mais desconexo,
com nenhum cluster, mas que carrega mais informação sobre o texto. A aula
finalizou mostrando dois métodos de verificar qual a diferença entre duas
strings. O primeiro deles foi a Distância de Hamming, que é um número
identificando a quantidade de caracteres que 2 strings de mesmo tamanho diferem
entre si. O segundo método é a Distância de Levenshtein em que é feita uma
verificação de quantos caracteres de uma string a precisam ser mudados para
ficar igual à uma string b. Neste caso, as strings podem ter tamanhos
diferentes, inclusive tamanho zero.


A linguagem escolhida para desenvolver a parte prática da disciplina foi
Python, principalmente por sua simplicidade e suas bibliotecas. O código
SomeGreatFunction.py consegue, em 15 linhas, reproduzir o funcionamento do
algoritmo QuickSort. As linhas 7, 8 e 9 usam um recurso muito útil da linguagem
chamado list comprehension, que permite reproduzir um loop for completo em uma
linha. Python é considerada uma linguagem de alto nível (bem alto), paois
nestes casos o código se assemelha à linguagem humana. Python é uma linguagem
que primeiro é interpretada e depois compilada por uma máquina virtual.
Classes em python funcionam de maneira semelhante à outras linguagens, uma
diferença é o nome do método construtor, que ao invés de levar o nome da
classe, chama __init__.
O primeiro stemmer implementado é o Fool-Stemmer, cuja lógica de remoção de
sufixo é simples, bastando manter as três primeiras letras da palavra. Porém,
fica claro que não é eficiente e gera muitos casos de over-stemming (quando
ocorre a remoção de partes do radical da palavra). Temos também o algortimo de
Porter implementado, vale notar a grande quantidade de condições (if eles) para
dar conta de todos os casos.
Snowball, um projeto realizado pelo próprio Porter e outros pesquisadores, é
uma linguagem para definição de stemmers e então stemmers rápidos podem ser
gerados.
Uma forma de representar um texto é através da rede (ou grafo) de palavras.
Onde os vétices são as palavras e as arestas indicam que uma palavra precede a
outra, formando um grafo direcionado (digrafo). Pesos são atribuidos às
arestas, cada ocorrência de uma mesma aresta adiciona em um o peso da mesma.
Desta forma, podemos estudar as relações entre palavras com o objetivo de
entender o significado de um texto, algo que será explorado em aulas futuras.
Uma forma de avaliar a semelhança entre duas cadeias de caractéres é através de
algoritmos de string matching. Por exemplo a distâcia de hamming, que mede a
quantidade de bits usados na mudança de cadeia pra outra. Outro algoritmo é a
dado pela distância de Levenshtein, onde cada operação (inserção, exclusão ou
substituição) acrescenta um no total da distância. Então, é calculado a forma
mais barata de utilizar as operações para transformar uma cadeia na outra.
 
 


A aula foi realizada no laboratório. O objetivo foi fazer uma pequena
introdução à linguagem python e apresentar os seguintes tópicos: algoritmos de
stemming, grafo de palavras e medidas de distância entre palavras
 
1)Python
Vimos que o python é uma linguagem híbrida, interpretada e compilada.
Inicialmene o código em python é interpretado em bytecods (pyc) e depois é
compilado por uma máquina virtual para instruções em linguagem de máquina. Este
é um dos motivos de python ser uma linguagem relativamente mais lenta.
 
2) Stemmer
2.1)Fool-Stemmer>
Este algorítmo é um dos mais simples possíveis para o processo de stemmer, por
isso seu nome peculiar "fool". Basicamente este algoritmo considera que os
stems de todas as plavras são suas n primeiras letras (caso a palavra tenha
menos de n letras, todas a palavra é considerada). No entanto, este
procedimento gera o chamado "overstemming", ou seja, este algoritimo "corta"
tanto as palavras que em alguns casos é impossível diferenciar uma palavra da
outra.
 
2.2) Algoritmo de Porter
O algorítmo de porter, remove o maior sufixo presente na palavra à ser tratada
que pode ser encontrada na tabela de condições do algoritmo. É possível o
functionamento do algoritmo de porter em detalhes em [1].
Nós utilizamos este algoritmo em um texto no idioma inglês. Ficou claro a
superioridade desta implementação. 
 
3) Grafo de palavras
Uma das coisas mais importantes da analise de dados é a visualização. Seria
interessante, portanto, uma maneira de representar um texto visualmente, de
forma que  com apenas uma imagem, fosse possível identificar pontos
importantes.  Em laboratório, nós fizemos esta representação visual através de
grafos utilizando a ferramenta GraphViz. O processo foi feito em x etapas.
1) Inicialmente, tratamos o texto removendo as stopwords. 2) Após isso,
identificamos todos os pares de palavras sequencialmente ordenados e contamos
quantas vezes cada par aparece. 3) Filtramos todos os pares com uma frequência
maior que n. 4) Geramos uma string com estas informações utilizando uma 
sintaxe própria do GraphViz. 5) Exportamos esta string para o GraphViz.
O resultado foi um grafo que mostra a relação entre as palavras e quantas vezes
esta relação aconteceu. Este grafo se mostrou ser uma ferramenta visual
excelente para entender pontos importantes sobre o texto.
 
4) Casamento entre strings
Em alguns casos, é interessante saber quão parecida duas palavras são, para
isso, existem medidas de distância. Falarei aqui sobre duas: Hamming e
Levenshtein
 
4.1) Hamming
Sua implementação é extremamente trivial. Basicamente calcula a distância entre
duas palavras utilizando o número de letras que diferem entre uma palavra e a
outra. O problema desta implementação é que as duas palavras devem ter
exatamente a mesma quantidade de letras.
 
4.2) Levenshtein
Neste método, a distância é calculada com base no número mínimo de ediçóes para
transformar uma palavra na outra. No entanto, foi visto em aula, que este
algorítmo é extremamente inviável. Uma string com aproximadamente 20 letras
levou um tempo absurdamente alto
 
5) Referencias
[1] Porter, M.F. An algorithm for suffix stripping. Computer Laboratory, Corn
Exchange Street. Cambrdige, 1980


A aula começa com uma rápida demonstração do poder semântico do Python,
apresentando o arquivo SomeGreatFunction.py, que é, na verdade, o quickSort().
O professor então apresenta alguns aspectos do interpretador do Python, como
sua máquina virtual. O arquivo classes.py é exibido para exemplificar a
orientação a objetos em Python.
Stemmers
Um primeiro exemplo de stemmer é exibido em aula. O programa carrega um arquivo
de texto, separa os tokens construtivamente com chars e corta as palavras em
"radicais" de três ou menos letras, caso a palavra não contenha três dígitos. É
um caso claro de over-stemming, pois se remove também partes dos radicais
morfológicos das palavras.
É exibido então o algoritmo do bibliotecário Porter e suas variações para
outras linguagens. O código do Porter para inglês é examinado, mas somente no
main(). A função de stemming é vista por cima, com suas centenas de
condicionais.
Snowball é uma linguagem desenvolvida para desenvolver stemmers que gera o
código de um stemmer em ANSI C ou Java.
Existem outras abordagens para stemming, como as estatísticas (N-Grams, HMM) ou
mixas (inflexional e derivacional, ou sensível a contexto).
O código para gerar o grafo de palavras é exibido, assim como o GraphVizOnline,
que interpreta a saída do código python e constrói grafos. Nós então analisamos
a saída para a Ilíada de Homero e colocamos na ferramenta, gerando grafos
imensos. Nós então passamos a brincar com os parâmetros para reduzir o número
de vértices, assim como implementamos a filtragem das stop words. Algumas
outras dicas do GraphViz foram apresentadas.
Algumas vantagens da examinação de grafos foram apresentadas, como: permitir
identificar relações entre palavras e elementos, identificar propriedades
topológicas para extração de informação, e o estudo da organização das
palavras.


Aula 5, laboratório. Revisão de conceitos básicos da linguagem python. Python é
uma linguagem interpretada, roda sobre uma máquina virtual, geralmente
compilado em C. Stemmer. Overstemming ocorre quando o uma parte do radical é
removida. Algoritmo de Porter é o stemmer mais utilizado atualmente. Snowball é
uma linguagem de programação que processa pequenas strings. É utilizado para
criar algoritmos stemming que são usados no campo de recuperação de
informação. Existem três principais abordagens de métodos stemming:
truncamento, estatístico e misto. Dentro os métodos de truncamento estão os
algoritmos de Lovins, Porter, Paice/Husk e Dawson. Para Stemmers estatísticos
temos N-Grams e HMM. Os mistos são Krovetz, Corpus Based e Context Sensitive.
Grafos de co-ocorrência de palavras. Permitem visualização de forma amigável de
relações entre as palavras. Pode-se aplicar propriedades, algoritmos e métodos
de grafos. Existem alguns métodos para medir o quão diferente duas strings são
entre si. Distância de Hamming, entre duas strings de mesmo tamanho, é o número
de substituições necessárias para transformar uma string em outra. Distância de
Levenshtein, é o número mínimo de edições (inserções, remoções ou
substituições) necessárias para mudar uma string em outra. Atividades. Foram
feitas comparações entre o Porter stemmer e um fool stemmer, que tomava como
radical apenas os três primeiros caracteres de uma palavra. Construiu-se
grafos, sendo as arestas palavras subsequentes em um texto. Utilizou-se o site 
<http://dreampuf.github.io/GraphvizOnline/> para visualização do grafo.
Refizemos o grafo considerando stopwords. Por fim observamos que o melhor
momento para remover as stopwords é durante o pré-processamento, pois como
indicado no slide 24, pode acontecer de palavras importantes não serem
consideradas. 


Nesta aula prática, foram apresentados alguns algoritmos relacionados com os
conteúdos já vistos previamente na teoria. A linguagem de programação em que
foram implementados os códigos é Python, que tem características específicas
que são preferíveis para o processamento de linguagem natural, em uma primeira
análise. Uma das características da linguagem são uma maior similaridade com a
língua e a lógica humana. Primeiramente, python é interpretado em bytecodes e
na sequência compilado em instruções de linguagem de máquina, sendo que essa
tradução pode causar lentidão nos processos.
Dois modelos de stemmers foram apresentados, sendo um deles o fool-stemmer e o
outro o algoritmo de Porter para língua inglesa. O primeiro é um algoritmo
muito simples, que somente corta os sufixos das palavras de forma grosseira,
sendo que o sentido das palavras não se mantêm. O algoritmo de Porter é mais
robusto e apresenta um resultado comparativamente mais refinado, porém o código
acaba sendo mais complexo e apresenta vários passos. Outro algoritmo citado em
aula é o stemmer snowball, que é um string curto de processamento da linguagem,
desenvolvido especialmente para a busca de recuperação da informação. O
compilador do snowball permite traduzir um script para outro programa, como
JAVA ou ANSI C.
Além disso, foi apresentado o modelo de criação de grafos de palavras a partir 
de um texto previamente dado. A função do grafo é apresentar a frequência de
palavras e de relações entre palavras. A utilização de grafos pode auxiliar na
identificação de relações potenciais entre palavras, sendo uma ferramenta
interessante para estudos linguísticos.  Pode-se estudar padrões e recorrências
da organização das palavras em um texto, possibilitando novas descobertas e
conhecimentos.


Casamento aproximado de strings, considere o problema de de correção
ortográfica que se resume a dado uma palavra qual pode estar errada por um ou
mais caracteres queremos corrigir esta palavra, ou seja, encontrar em uma lista
de palavras que sabemos que são corretas a que mais se parece com a palavra
possivelmente errada que estamos examinando, para isto podemos percorrer toda a
lista de palavras corretas calculando a diferença entre a palavra examinada e a
da lista, substituímos a palavra examinada por aquela da lista que possui a
menor diferença. Com isso surge um problema, o que queremos dizer exatamente
com diferença? Quantificamos a diferença do tamanho das palavras? Ou a
quantidade de caracteres similares entre ambas? A posição importa? Dado duas ou
mais strings existem diversas formas de quantificar a diferença entre elas, já
que a ideia de diferente não só varia entre contextos mas como diferentes
ideias de diferença podem ter outras utilidades em um mesmo problema ou
contexto.
Distância de Edit:
É a distância que quantifica algumas das ideias que foram vagamente discutidas,
a distância de Edit é a quantidade de edições necessárias para transformar uma
string em outra podendo atribuir um peso a diferentes edições, mais
formalmente: considere as 3 possíveis edições, adição, inserção e substituição,
atribua um peso a cada uma dessas edições e dentre todas possíveis formas de
transformar uma string na outra a partir dessas edições com custo ponderado,
escolha a menor delas, esta será a distância.
Agora podemos definir a distância Levenshtein, que é simplesmente a distância
que acabamos de definir só que cada operação de edição tem o peso igual a 1.
Distância de Hamming:
Dado duas strings de mesmo comprimento a distância de Hamming quantifica a
quantidade de posições que diferem em ambas strings.


Na aula 05 vimos python, Stemmers, grafo de palavras e distancia de edição.
Como foi a primeira aula em laboratório foi o primeiro contato de muitos na
ferramenta, entao o professor alguns conceitos programação orientada a objetos
na aula e deu um breve contexto sobre como o python funciona e foi programado.
Vimos como funciona o algoritmo de stemmer na pratica utilizando um stemmer
mais simples que capturava apenas as 3 primeiras letras de qualquer palavra.
Essa metodologia acaba levando ao overstemming, que é o que acontece quando
removemos não apenas o sufixo de uma palavra mas tambem o seu radical. Vimos
tambem uma implementação do algoritmo stemmer de Porter, um dos mais utilizados
atualmente e que possui codigo aberto.
Snowball é uma liguagem de programação projetada para criar algoritmos de
stemming para o uso na recuperação de informação.
Outra ferramenta importante é a combinação de PLN com rede de grafos. Na aula
criamos uma rede direcionada das palavras do texto. Isso acrescido de um
dataprep para remoção das stopwords permite que tenhamos uma ideia das palavras
mais citadas no texto. Com a ajuda da ferramente graphviz criamos diversos
graficos interativos alterando cor de aresta e tipos de grafos. Um dos grafos
visto foi o de co-ocorrencia de palavras, que nos permitia identificar de forma
visual pontenciais relações entre palavras. É possivel obter diversas
informações a respeito de um texto ou outro tipo de dado analisando a sua
topografia grafica.
Falamos tambem de casamento: quando estamos procurando a palavra que mais se
parece com uma para fazer uma correção; e distancia de hamming: utilizada para
identificar erros nas transmissoes binarias de comprimento fixo. A distancia de
levenshtein calcula o numero de edições necessarias para transformar uma
palavra em outra


O primeiro laboratório teve como inicio a apresentação da estrutura de python
para os alunos, dando uma breve descrição da linguagem demonstrando uma maneira
diferente do comum de definir um vetor, onde é possível constituir um vetor com
um for ( h = [x for x in arr if x < pivot], onde h é só os elementos menores
que pivot ) e uma pequena demonstração de classes em python. Sobre o desempenho
da linguagem foi dito que ela é lenta, porém é possível usar trechos de códigos
em c com auxilio da biblioteca ctypes (https://docs.python.org/3/library/
ctypes.html ) quando é necessário que certo trechos sejam rápidos.
 
Algoritmos trabalhados
 
fool stemmer.py
É um radicalizador simples, cortando toda a palavra ficando somente com os três
primeiros caracteres, o nome do arquivo é passado antes da execução ( argv )
podendo ter mais de um arquivo. O processo do algoritmo é o seguinte ele entra
no laço e começa a formar as palavras, quando ele encontrar algo que não seja
um caractere ele inicia o processo de corte em word[0:3], ou seja o arquivo foi
reduzido a 3 caracteres. O problema desse stemmer é que a redução na maioria
dos casos gera overstemmer, por isso ele é funcional, mas não é eficiente na
radicalização do texto.
 
    ( string ).isalpha() -> verifica se é um caracter
    ( string ).lower()    -> todas as letras em minúsculo.
 
porter-stemmer.py
O porter Stemmer ele é mais eficiente na radicalização, tratando melhor os
casos gerando um melhor resultado, porém a versão deste algoritmo no
laboratório foi a original ( inglês ), ou seja, para trabalhar com a língua
portuguesa ela não serve ( necessita de adaptação ). O algoritmo é mais
complexo utilizando conceitos de programação orientado a objetos  ( mesmo sendo
uma tradução da versão feita na linguagem bcp ) e segue os 5 passos de porter.
 
Comparação entre fool stemmer.py  e porter-stemmer.py
 
Na contagens de quantidade palavras e linhas nada é perdido nos algoritmos
( 141912 e 12499 ), já  quantidade de tokens o fool stemmer.py ficou com
562325, porter-stemmer.py com 717681 e versão original com 775584, já no
vocabulário temos 5499, 10892 e 13149.
 
Transformando em texto em um grafo
 
graphy1.py
Precisa de duas informações o nome do arquivo em um parâmetro inteiro, onde ele
só gera um digrafo ( grafo direcionado ) com arestas que tenha conexão maior ou
igual a esse inteiro. No caso os vértices representam uma palavra, já as
arestas representam a relação de uma palavra com a sua seguinte. Utiliza
dicionário para realizar o programa. O formato do grafo é o seguinte (vértice
origem) - > (vértice destino) [label = ( peso da aresta ) ]
 
graphy2.py
O graphy2.py é uma adapitação do graphy1.py, mas tiramos algumas palavras
(stopwords) e esse tinha como intenção ser construído pelos alunos.
 
Visualização dos grafos
 
Para visulizar o grafo utilizamos o http://dreampuf.github.io/GraphvizOnline/
, onde colocamos a saida do programa na caixa de texto desse programa e ele
gera um grafo, ele contém 6 algoritmos diferentes de visualização e 6 formatos
de saída.
 
Outra métodos
 
Uma forma de identificar palavras iguais onde estas não estam na mesma flexão é
o casamento aproximado entre strings, uma forma de comparar é com o casamento
de Hamming onde esse conta a diferença entre duas strings de tamanhos iguais. A
outra maneira é através da distância de Levenshtein onde essa mede a quantidade
de alterações mínimas necessárias para uma string x virar a string y.
 
O que é snowball em stemming?
 
Snowball é uma linguagem de programação para pré tratamento de strings, ou
seja, é uma linguagem para construção de stemmers.


 


O início da aula consistiu em uma revisão sobre aspectos básicos da linguagem
Python. O professor disponibilizou uma série de códigos fonte para que os
alunos pudessem acompanhá-lo. Através dos códigos presentes nos arquivos
SomeGreatFunction.py e classes.py, os alunos revisaram declaração de funções,
funções básicas do Python (como imprimir texto na tela), recursão, criação de
listas, classes, herança etc.
Python foi definida como uma linguagem interpretada e por isso, mais lenta, mas
é possível otimizar sua performance através da utilização de funções escritas
em C/C++.
Após a revisão de Python, o primeiro stemmer foi apresentado (contido no
arquivo fool-stemmer.py). Ele remove todo o conteúdo das palavras após os três
primeiros caracteres (caso elas possuam mais de três caracteres), e assim
surgiu a ideia de Over-stemming. Over-stemming significa remover caracteres
além do sufixo das palavras, ocasionando perda de caracteres do radical e
possivelmente, redução de palavras semanticamente diferentes ao mesmo radical.
Para evitar isto, o stemmer de Porter passou a ser utilizado. Testes foram
feitos com o texto Ilíada, de Homero, e foi possível observar que o stemmer de
Porter utilizado (presente no arquivo porter-stemmer.py) é aplicável apenas a
idiomas em que os sufixos das palavras são derivados do Latim.
A seguir, foi pedido aos alunos que buscassem o significado do termo Snowball
(aplicado ao contexto de stemmers). Snowball é uma linguagem de processamento
de cadeias de texto cuja principal aplicação é a criação de algoritmos de
stemming para recuperação de informação.
O primeiro algoritmo de conversão de textos em digrafos foi mostrado. Sua
estratégia consiste em transformar cada palavra do texto em um nó do digrafo,
ao passo que cada sequência de duas palavras dá origem a uma aresta em que a
primeira palavra precede e aponta para a segunda. Cada vez que uma aresta se
repete no texto, seu peso é acrescido de 1. O script (contido em graph1.py),
após computar o grafo, imprime-o na tela no formato DOT. O visualizador de
grafos no formato DOT contido em https://dreampuf.github.io/GraphvizOnline/ foi
utilizado para analisar as arestas de peso maior ou igual a 200 do grafo gerado
pelo texto Ilíadas. Depois, o programa foi modificado para que as stopwords
(cujo dicionário veio do arquivo stopwords.txt) presentes fossem removidas
antes do início da transformação do texto em digrafo. 
Por fim, o tópico de casamento aproximado de cadeia textuais e alguns casos de
uso foram apresentados. Por exemplo, o reconhecimento de palavras digitadas de
forma parcialmente incorreta e de cadeias de DNA. Neste contexto, as distâncias
de Hamming e Levenshtein foram introduzidas como ferramentas para computar quão
próximas são duas strings. A primeira, aplicada em contextos de transmissão de
cadeias binárias (em que erros ocorrem com frequência e por diversas razões) de
comprimento fixo é definida como a quantidade de bits usados na mudança de uma
transmissão para a recepção, enquanto a segunda é o número mínimo de operações
de adição, alteração e/ou exclusão de caracteres que a primeira string deve
sofrer até tornar-se a segunda.


Snowball é uma linguagem de processamento criada por Dr. Martin Porter para
desenvolver algoritmos de stemming para tratamento de textos. Em seu website,
snowballstem.org, estão disponíveis scripts para download e também o código
fonte da linguagem Snowball. Os tipos de dados suportados pela Snowball são
strings, integers e booleans. O nome Snowball é uma homenagem à série de
linguagens de programação orientadas a string SNOBOL.
Podemos usar a linguagem de programação Python para escrever algoritmos de
stemming eficientemente.Um exemplo é o fool-stemmer.py que simplesmente reduz
todas as palavras do texto para suas 3 primeiras letras, causando o problema do
overstemmming pois palavras diferentes acabam sendo reduzidas ao mesmo radical.
Também é implemetado em python o algoritmo de Porter (porter-stemmer.py), que é
um algoritmo mais robusto e utilizado até hoje.
Utilizando o stemming podemos gerar um grafo de palavras (graph1.py) com as
palavras do texto nos vértices e as arestas representam a adjacência dessas
palavras no texto, assim, podemos visualizar quais pares de palavras ocorrem
mais frequentemente. Nesse caso, pode ser interessante remover as stopwords do
texto. Os grafos de palavras permitem o estudo matemático de um texto e a
visualização por humanos também.
A distância de Hamming é utilizada para detectar erros comparando duas strings,
contando a quantidade de bits diferentes. A distância de Levenshtein também
mede a quantidade de diferenças entre duas strings porém é mais detalhada que a
distância de Hamming pois usa operações de inserção, exclusão e substituição
para definir o número mínimo de edições para transformar uma string em outra.


Vimos que python é interpretado em bytecodes (pyc) e depois é compilado por uma
máquina virtual em instruções de linguagem máquina (geralmente C). Esse é um
dos motivos dele ser lento comparado a linguagens como o C, podem ser
utilizadas funções implementadas em C/C++ para melhorar o desempenho
computacional.
Vimos no código fool-stemmer.py que usando um radical de três letras,
facilmente confundiremos palavras, e com isso, a análise não será efetiva da
semântica do texto.
A análise pode ser melhorada usando o Porter stemmer. O Porter stemmer é um
processo para remover as terminações morfológicas e inflexão mais comuns de
palavras em inglês. Seu principal uso é como parte de um processo de
normalização de termos que normalmente é feito ao configurar os sistemas de
Recuperação de Informação. Existe uma linguagem para análise de texto com
stemmer,  o Snowball. nesta linguagem os padrões podem ser exatamente
definidos, e dos quais programas em ANSI C ou Java podem ser gerados. Uma gama
de stemmers é apresentada em forma algorítmica e Snowball, incluindo o original
Porter stemmer para inglês.
Vimos também a geração de um grafo de palavras, e, foi implementado um grafo
onde pode ser escolhidos nós com um peso de um valor x ou maior. Também é
possível com uma lista de stopwords tornar nosso grafo com vértices mais
relevantes ao sentido do texto. Após montarmos o grafo, todas as análises com
grafos são passíveis de serem usadas.
No final, vimos o problema da correspondência aproximada de Strings, que é
importante em muitas áreas diferentes, como biologia computacional,
processamento de texto e reconhecimento de padrões. Algoritmos eficientes são
projetados que abordam várias variantes do problema, incluindo a comparação de
duas cadeias de caracteres, identificação de padrão aproximada em uma string ou
o cálculo da subseqüência comum mais longa que compartilham duas strings.


Aula prática em laboratório.
Foram testados alguns dos algoritmos vistos nas aulas de teoria a algumas
características do Python. 
Foi explicado o uso básico de orientação a objetos no Python. Algumas
características interessantes a respeito do Python notada nessa aula, muitas
implementações que necessitam ser feitas em outras linguagens  já vem nativa no
Python como por exemplo ferramentas para manipulação de vetores, uso de
sintaxes próxima da linguagem humana, isso torna o código muito mais enxuto e
de melhor entendimento, como por exemplo o algoritmo do quicksort. Esses
recursos serão fundamentais para a implementação dos algoritmos em sala.
O primeiro código testado foi o um stemmer bem simples. Esse programa realiza o
truncamento das palavras de um texto deixando no máximo os três primeiro
caracteres das palavras ocasionando o over-stemmer na maioria dos casos. Possui
um caráter meramente didática.
O segundo código testado foi o stemmer de Porter, esse script apresentou um
melhor resultado, a redução aos radicais foram mais representativos, porém a
versão desse script foi feita para ser utilizado em textos da língua inglesa.
Quando aplicado em um texto em português ocorreu o under-stemming na maior
parte das palavras. Tentativas de implementar o algoritmo de Porter para outras
línguas ocorreram diversas vezes porém com erros, por isso, o autor lançou
oficialmente uma framework que auxilia a portabilidade de seu algoritmo para
outras línguas como o português esse projeto foi chamado de Snowball.
O terceiro algoritmo faz a contagem da frequência de dois tokens contíguos em
todo o texto, sendo exibido aqueles com frequência maior ou igual ao
predeterminado pelo usuário. O resultado pode ser melhor observado por meio de
digrafos (grafos direcionados) simplesmente copiando e colando na página https:
//dreampuf.github.io/GraphvizOnline/ . Essa forma de visualização é muito
interessante para realizar análises de propriedades topológicas, predição de
conjunto de termos mais frequentes.
O último código realiza a verificação do casamento dos caracteres entre
duas frases, devolvendo um valor de quanto distante está o casamento dessas
frases. Esse tipo de comparação é comumente feito para verificar erros de
transmissão digital bit a bit, usando a distância de Hamming por exemplo. Foi
testado a Distância de Levenshtein e foi comprovado a ineficiência do código
devido a sua demora para comparar frases longas em que só houve alterações de
dois caracteres.
 
 


Inicialmente foi falado sobre algumas particularidades de Python. Criada por
Guido van Rossum em 1991, Python é uma linguagem de programação de alto nível,
interpretada e multi paradigma, suporta os paradigmas: orientado a objetos,
imperativo, funcional e procedural. Como é interpretada seus códigos fontes são
transformados em uma linguagem intermediária, que será interpretada pela
máquina virtual da linguagem quando o programa for executado.
O processo de interpretação da linguagem intermediária durante a execução do
programa, consiste na tradução dos comandos da linguagem intermediária para
linguagem de máquina. Sendo assim, em tempo de execução, o código intermediário
pode ser encarado como um código fonte que será compilado dinamicamente pelo
interpretador da linguagem em código de máquina.
Ter este processo de compilação embutido na execução do programa tem um custo
mais alto. Nos últimos anos foi investido para otimizar este processo,
resultando em técnicas que permitem que as linguagens interpretadas alcancem
boas performances.
Alguns temas estudados em aulas anteriores tiveram códigos implementados em
Python. Como exemplo de stemmer vimos uma versão pouco criteriosa e analitica,
o truncamento das palavras foi feito de forma que se reduzia qualquer termo à
suas três primeiras letras. Uma versão extremamente simples mas que pode
oferecer algumas complicações como prefixos sem significado ou palavras de
significados e origens diferentes com a mesma raiz/termo reduzido.
Os stemmer mais conhecidos e são o desenvolvido por Lovins e o feito por
Porter, a implementação do algoritmo de Porter se mostra eficiente e simples,
porém não se adequa ao português.
Várias versões do algoritmo de Porter foram reproduzidas (e em alguns  casos
modificadas) em outras linguagens, porém com essas variações pode ocorrer perda
de precisão e outros problemas, além da possibilidade de não funcionar de forma
similar em outro idioma.
Para abordar todos esses problemas, foi desenvolvido um sistema rigoroso para
definir algoritmos de derivação. Uma linguagem, Snowball, foi inventada, em que
as regras de algoritmos de derivação podem ser expressas de forma natural.[...]
Um compilador Snowball traduz cada definição Snowball em um programa
equivalente em ANSI C ou Java. Finalmente vocabulários padrões de palavras e
seus equivalentes de raiz são fornecidos para cada stemmer. A combinação de de
várias etapas pode ser usada para definir a definição de um stemmer exatamente,
e espera-se que Snowball em si seja um recurso útil na criação de stemmers no
futuro.  - tradução livre de um trecho do artigo introdutório sobre Snowball
encontrado em: http://snowball.tartarus.org/texts/introduction.html.
 


Na aula fomos ao Laboratório onde testamos algumas funções em sala. As funções
foram: SomeGreatFunction.py, classes.py, fool-stemmer.py, porter-stemmer.py
e graph1.py. Todas podem ser encontradas no site da disciplina. Foi explicado o
porque do Python ser mais lento do que C, por exemplo. Ao usar as funções de
Stemming, pudemos transformar os resultados em grafos e esses grafos podem ser
vistos no site http://dreampuf.github.io/GraphvizOnine/. Foi pedido para
explicar o que é Snowball com relação a Stemming.
Snowball é uma pequena string de processamento de linguagem feita para criar
algoritimos de Stemming para recuperar informações.
Perto do fim da aula, observamos mais superficialmente o que as funções Hamming
e Levenshtein podem fazer. Ambas servem para dizer a diferença entre duas
palavras, sendo que a distancia de Levenshtein funciona melhor para comparações
de palavras pequenas.


Ao longo da aula que desta vez e prática, o professor abordou os conceitos de
Normalização nos dando introdução breve sobre uma linguagem utilizada, O
Python. Descreveu que é uma linguagem interpretada e por isso é relativamente
mais lenta se comparada a other mas que é extremamente funcional para uma
finalidade de PLN. Embora tenha sido criado um programa de emprego em
aplicações de grande valia para testemunhar o funcionamento e melhoramento dos
mecanismos da Normalização de Texto. Empregamos o algoritmo de Porter para
normalização e uma aplicação para Grafos (que eu particularmente gosto muito)
também foi exibido no que nos mostra como muitas são como possibilidades de uso
para o PLN. Uma das aplicações exibidas para calcular a quantidade de mudanças
em Cordas, método de Levenshtein, Na teoria da informação, linguística e
ciência da computação, a distância de Levenshtein é uma métrica de strings para
medir a diferença entre duas sequências. Informalmente, a distância Levenshtein
entre duas palavras é o número mínimo de edições de um único caracter
(inserções, eliminações ou substituições) necessárias para mudar uma palavra
para a outra. Advém Vladimir Levenshtein, que considerou essa distância em
1965. 
A distância de Levenshtein também pode ser referida como distância de edição,
embora esse termo também possa indicar uma família maior de métricas de
distância: Está intimamente relacionado aos alinhamentos de strings pairwise.
?Por fim foi pedido que tentássemos dar uma definição para SnowBall no contexto
de PLN que na sua concepção existem dois motivos principais para criação do
Snowball. Um deles é a falta de algoritmos de derivação prontamente disponíveis
para outras línguas que não o inglês. O outro é a consciência de um certo
fracasso da minha parte na promoção de implementações exatas do algoritmo de
derivação descrito em (Porter, 1980), que passou a ser chamado de algoritmo de
derivação de Porter. O primeiro ponto precisa de alguma qualificação: uma
grande quantidade de trabalho foi feito em variedades em uma ampla gama de
linguagens naturais, tanto no seu desenvolvimento quanto na avaliação, (uma
bibliografia completa não pode ser tentada aqui). Mas é raro ver um stemmer
apresentado em uma forma algorítmica inequívoca a partir da qual as
codificações em C, Java, Perl etc. podem ser facilmente feitas. Quando as
descrições exatas são tentadas, é frequentemente com abordagens para frustrar
que são relativamente simples, como, por exemplo, o latino de Schinke (Shinke
1996), ou o stemmer esloveno de Popovic (Popovic, 1990). Um stemmer mais
complexo e, portanto, mais característico é o Kraanj-Pohlmann para o holandês
(Kraaij, 1994), que é apresentado como código aberto em ANSI C. Para extrair
uma descrição algorítmica de seu stemmer do código-fonte revela-se bastante
difícil . A disparidade entre a definição do stemmer de Porter e muitas das
suas supostas implementações é muito maior do que é geralmente realizada na
comunidade IR. Três problemas parecem ser compostos: um é um mal entendido
sobre o significado do algoritmo original, outro é um erro nas codificações e
um terceiro é o impulso quase irresistível dos programadores para adicionar
melhorias. Por exemplo, um script Perl anunciado na Web como uma implementação
do algoritmo Porter foi testado em outubro de 2001, e descobriu-se que 14 por
cento das palavras foram geradas de forma incorreta quando recebido um grande
vocabulário de exemplo. A maioria das palavras de inglês tem terminações muito
simples, então isso significa que ele estava efetivamente recebendo tudo
errado. Em certos pontos da Web estão as demonstrações do porteiro. Você
escreve algum inglês em uma caixa e as palavras com haste são exibidas. Estes
são frequentemente defeituosos.  Os pesquisadores frequentemente pegam versões
defeituosas do stemmer e relatam que aplicaram 'Porter stemming' , com o
resultado de que suas experiências não são bastante repetitivas. Os
pesquisadores que trabalham na remoção às vezes dão exemplos incorretos do
comportamento do Porter stemmer em seus trabalhos publicados.
Para abordar todos esses problemas, houve a tentativa de desenvolver um sistema
rigoroso para definir algoritmos de derivação. Uma linguagem, Snowball, foi
inventada, em que as regras de algoritmos de derivação podem ser expressas de
forma natural. Snowball é bastante pequeno, e pode ser aprendido por um
programador experiente em uma hora ou mais. Neste site, uma série de variedades
de língua estrangeira é apresentada (a) no Snowball, e (b) em uma descrição
menos formal em inglês. (b) pode ser pensado.


Nas aulas de laboratório usaremos python, por ser mais "literal" que linguagens
de mais baixo nível, como C/C++, e mesmo sendo mais lento por ter q ser
compilada em liguagem de máquina em uma máquina virtual.
No início da aula (após uma breve apresentação sobre python) é mostrado um
stemmer bem simples e ruim, nomeado de fool stemmer, ele apenas pega as 3
primeiro letras de cada palavra, gerando o que se denomina Over-stemming
(retira-se mais que apenas o sufixo da palavra).
Depois há o stemming criado por Porter, já comentado a aula anterior, porém em
sua versão em python.
Um pequeno detalhe sobre stemming, há o Snowball (em tributo à SNOBOL outra
linguagem com foco em strings), uma linguagem de programação para pequenas
strings, desenhado com o foco em criar algoritmos de stemming usado em
recuperação de informação. (site: http://snowballstem.org , github: https://
github.com/snowballstem )
Além de seu óbvio uso em truncamento de palavras, stemming podem ser usados em
outras áreas, como para gerar dados estatísticos em textos (ex: palavras mais
usadas) e radicais de palavras baseando-se na ocorrência das palavras e não em
um corpus específico, dentre outras.
Usando grafos em textos, é possível ver, por exemplo, o quão frequente são
certas palavras seguidas de outras (graph1.py gera um resultado textual, porém
esse texto no site https://dreampuf.github.io/GraphvizOnline/ mostra como é o
grafo representativo), porém como é fácil de perceber há muitas palavras são
seguidas de conectivos e artigos, é possível fazer um código que ignore essas
stopwords (graph2.py).
Outro ponto importante é o casamento aproximado de strings, sua funcionalidade
vai de correção ortográfica (qual é a palavra mais próxima da que o usuário
está digitando) até biologia computacional (qual é o melhor alinhamento entre
duas fitas de proteínas presentes em um DNA). Dessa forma, há dois algoritmos a
serem estudados, Distância de Hamming e Levenshtein, Sendo o primeiro um método
mais simples que compara duas cadeias de string de tamanho igual (ou ele
retorna um erro (-1)) e retorna o número de caracteres diferentes em cada
posição. Já Levenshtein considera o número mínimo de operações para transformar
uma string em outra (operação: inserção, remoção e substituição de um
caracter).
 


A quinta aula da disciplina foi no laboratório. Nós a começamos testando alguns
códigos que o professor disponibilizou para nós, afim de introduzir a linguagem
de programação Python. O primeiro foi o SomeGreatFunction.py e ao testá-lo
pudemos verificar que o código demora bastante para compilar devido ao fato da
linguagem primeiro ser interpretada em bytecodes para depois ser compilado por
uma máquina virtual, assim concluiu-se que podemos usar outras linguagens para
melhor aproveitamento computacional no caso desse algoritmo. Depois analisamos
um código de orientação a objetos, no qual a classe Animal, recebe como objeto
o animal e sua idade como atributo e a classe Dog que herda a classe Animal.
Após isso, voltamos para o tema da aula anterior que foi Stemming. O primeiro
código a ser testado foi o fool-stemmer.py tendo como entrada o texto capivara-
pt.txt utilizando o seguinte comando no terminal Linux: python3 fool-stemmer.py
capivara-pt.txt e pudemos observar o efeito do over-stemming, que é quando um
stem é reduzido demais e acaba perdendo seu significado, isso pode ocorrer
devido ao uso de stemmers que não são próprios para uma determinada língua. O
algoritmo seguinte a ser testado foi o porter-stemmer.py e com o seguinte
comando: python3 porter-stemmer.py capivara-pt.txt podemos notar que o
resultado obtido não foi apropriado devido ao idioma para qual esse algoritmo
foi desenvolvido. Para corrigir esses tipos de problema ao utilizarmos
algoritmos de stemming podemos usar o Snowball, que é uma linguagem que produz
stemmers adaptados de acordo com regras descritas na forma natural.
Depois disso, aprendemos como gerar grafos das palavras de um determinado texto
utilizando o código graph1.py, disponibilizado pelo professor. Esse programa
conta a frequência de pares de palavras e devolve um grafo que pode ser
visualizado no link https://dreampuf.github.io/GraphvizOnline/.
Por fim, foram introduzidos dois novos conceitos utilizados no string-matching.
O primeiro foi a distancia de Hamming, utilizado para a detecção de erros  nas
transmissões binárias e o segundo foi a distância de Levenshtein que é
utilizado para medir a quantidade de diferenças entre duas strings.
